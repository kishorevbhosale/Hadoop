2015-10-05 11:44:13,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 11:44:13,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 11:44:13,872 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 11:44:13,938 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 11:44:13,938 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 11:44:13,943 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 11:44:13,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 11:44:13,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 11:44:14,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 11:44:14,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 11:44:14,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 11:44:14,095 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 11:44:14,103 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 11:44:14,108 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 11:44:14,113 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 11:44:14,115 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 11:44:14,115 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 11:44:14,115 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 11:44:14,126 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 46621
2015-10-05 11:44:14,126 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 11:44:14,294 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46621
2015-10-05 11:44:14,454 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 11:44:14,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 11:44:14,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 11:44:14,573 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 11:44:14,592 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 11:44:14,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 11:44:14,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 11:44:14,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 11:44:14,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 11:44:14,989 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 11:44:14,990 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 11:44:16,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:17,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:18,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:19,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:20,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:21,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:22,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:23,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:24,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:25,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:25,203 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:44:31,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:32,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:33,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:34,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:35,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:36,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:37,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:38,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:39,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:40,210 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:40,211 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:44:46,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:47,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:48,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:49,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:50,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:51,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:52,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:53,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:54,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:55,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:44:55,218 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:45:01,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:02,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:03,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:04,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:05,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:06,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:07,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:08,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:09,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:10,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:10,225 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:45:16,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:17,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:18,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:19,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:20,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:21,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:22,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:23,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:24,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:25,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:25,233 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:45:31,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:32,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:33,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:34,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:35,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:36,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:37,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:38,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:39,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:40,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:40,240 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:45:46,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:47,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:48,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:49,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:50,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:51,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:52,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:53,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:54,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:55,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:45:55,247 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:46:01,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:02,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:03,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:04,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:05,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:06,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:07,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:08,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:09,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:10,254 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:10,255 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:46:16,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:17,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:18,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:19,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:20,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:21,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:22,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:23,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:24,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:25,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:25,262 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:46:31,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:32,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:33,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:34,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:35,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:36,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:37,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:38,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:39,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:40,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:40,269 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:46:46,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:47,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:48,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:49,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:50,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:51,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:52,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:53,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:54,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:55,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:46:55,276 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:47:01,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:02,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:03,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:04,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:05,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:06,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:07,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:08,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:09,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:10,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:10,283 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:47:16,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:17,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:18,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:19,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:20,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:21,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:22,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:23,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:24,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:25,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:25,291 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:47:31,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:32,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:33,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:34,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:35,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:36,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:37,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:38,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:39,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:40,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:40,298 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:47:46,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:47,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:48,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:49,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:50,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:51,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:52,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:53,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:54,305 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:55,305 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:47:55,306 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:48:01,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:02,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:03,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:04,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:05,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:06,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:07,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:08,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:09,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:10,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:10,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:48:16,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:17,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:18,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:19,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:20,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:21,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:22,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:23,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:24,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:25,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:25,322 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:48:31,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:32,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:33,324 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:34,324 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:35,325 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:36,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:37,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:38,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:39,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:40,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 11:48:40,329 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 11:48:45,788 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 11:48:45,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 12:01:42,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 12:01:42,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 12:01:43,149 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 12:01:43,212 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 12:01:43,212 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 12:01:43,217 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 12:01:43,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 12:01:43,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 12:01:43,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 12:01:43,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 12:01:43,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 12:01:43,335 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 12:01:43,343 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 12:01:43,348 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 12:01:43,353 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 12:01:43,355 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 12:01:43,355 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 12:01:43,355 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 12:01:43,365 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42529
2015-10-05 12:01:43,365 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 12:01:43,514 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42529
2015-10-05 12:01:43,601 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 12:01:43,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 12:01:43,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 12:01:43,640 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 12:01:43,651 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 12:01:43,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 12:01:43,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 12:01:43,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 12:01:43,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 12:01:43,730 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 12:01:43,730 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 12:01:44,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:01:45,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:01:46,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:01:47,806 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:01:48,806 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:01:49,807 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:01:50,807 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:01:51,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:01:52,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:01:53,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:01:53,810 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:01:59,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:00,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:01,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:02,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:03,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:04,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:05,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:06,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:07,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:08,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:08,818 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:02:14,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:15,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:16,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:17,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:18,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:19,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:20,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:21,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:22,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:23,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:23,825 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:02:29,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:30,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:31,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:32,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:33,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:34,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:35,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:36,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:37,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:38,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:38,832 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:02:44,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:45,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:46,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:47,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:48,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:49,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:50,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:51,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:52,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:53,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:02:53,839 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:02:59,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:00,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:01,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:02,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:03,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:04,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:05,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:06,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:07,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:08,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:08,846 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:03:14,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:15,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:16,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:17,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:18,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:19,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:20,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:21,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:22,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:23,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:23,853 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:03:29,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:30,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:31,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:32,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:33,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:34,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:35,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:36,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:37,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:38,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:38,860 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:03:44,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:45,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:46,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:47,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:48,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:49,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:50,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:51,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:52,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:53,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:03:53,868 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:03:59,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:00,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:01,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:02,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:03,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:04,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:05,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:06,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:07,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:08,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:08,875 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:04:14,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:15,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:16,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:17,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:18,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:19,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:20,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:21,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:22,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:23,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:23,882 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:04:29,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:30,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:31,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:32,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:33,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:34,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:35,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:36,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:37,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:38,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:38,889 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:04:44,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:45,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:46,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:47,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:48,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:49,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:50,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:51,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:52,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:53,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:04:53,896 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:04:59,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:00,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:01,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:02,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:03,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:04,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:05,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:06,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:07,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:08,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:08,904 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:05:14,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:15,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:16,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:17,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:18,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:19,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:20,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:21,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:22,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:23,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:23,911 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:05:29,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:30,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:31,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:32,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:33,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:34,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:35,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:36,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:37,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:38,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:38,919 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:05:44,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:45,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:46,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:47,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:48,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:49,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:50,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:51,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:52,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:53,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:05:53,927 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:05:59,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:00,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:01,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:02,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:03,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:04,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:05,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:06,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:07,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:08,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:08,934 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:06:14,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:15,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:16,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:17,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:18,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:19,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:20,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:21,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:22,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:23,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:23,941 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:06:29,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:30,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:31,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:32,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:33,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:34,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:35,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:36,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:37,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:38,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:38,949 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:06:44,950 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:45,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:46,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:47,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:48,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:49,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:50,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:51,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:52,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:53,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:06:53,957 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:06:59,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:00,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:01,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:02,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:03,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:04,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:05,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:06,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:07,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:08,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:08,964 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:07:14,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:15,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:16,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:17,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:18,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:19,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:20,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:21,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:22,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:23,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:23,971 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:07:29,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:30,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:31,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:32,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:33,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:34,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:35,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:36,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:37,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:38,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:38,978 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:07:44,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:45,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:46,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:47,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:48,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:49,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:50,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:51,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:52,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:53,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:07:53,985 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:07:59,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:00,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:01,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:02,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:03,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:04,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:05,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:06,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:07,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:08,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:08,992 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:08:14,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:15,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:16,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:17,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:18,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:19,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:20,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:21,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:22,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:23,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:23,999 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:08:30,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:31,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:32,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:33,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:34,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:35,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:36,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:37,005 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:38,005 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:39,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:39,007 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:08:45,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:46,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:47,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:48,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:49,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:50,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:51,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:52,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:53,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:54,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:08:54,014 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:09:00,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:01,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:02,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:03,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:04,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:05,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:06,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:07,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:08,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:09,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:09,021 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:09:15,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:16,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:17,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:18,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:19,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:20,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:21,025 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:22,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:23,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:24,027 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:24,027 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:09:30,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:31,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:32,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:33,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:34,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:35,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:36,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:37,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:38,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:39,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:39,034 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:09:45,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:46,036 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:47,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:48,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:49,038 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:50,038 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:51,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:52,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:53,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:54,041 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:09:54,041 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:10:00,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:01,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:02,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:03,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:04,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:05,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:06,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:07,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:08,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:09,048 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:09,049 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:10:15,049 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:16,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:17,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:18,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:19,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:20,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:21,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:22,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:23,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:24,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:24,056 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:10:30,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:31,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:32,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:33,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:34,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:35,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:36,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:37,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:38,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:39,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:39,063 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:10:45,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:46,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:47,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:48,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:49,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:50,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:51,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:52,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:53,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:54,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:10:54,070 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:11:00,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:01,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:02,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:03,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:04,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:05,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:06,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:07,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:08,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:09,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:09,077 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:11:15,078 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:16,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:17,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:18,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:19,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:20,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:21,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:22,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:23,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:24,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:24,085 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:11:30,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:31,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:32,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:33,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:34,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:35,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:36,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:37,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:38,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:39,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:39,092 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:11:45,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:46,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:47,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:48,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:49,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:50,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:51,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:52,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:53,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:54,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:11:54,099 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:12:00,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:01,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:02,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:03,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:04,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:05,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:06,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:07,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:08,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:09,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:09,107 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:12:15,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:16,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:17,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:18,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:19,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:20,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:21,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:22,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:23,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:24,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:24,115 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:12:30,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:31,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:32,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:33,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:34,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:35,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:36,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:37,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:38,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:39,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:39,122 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:12:45,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:46,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:47,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:48,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:49,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:50,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:51,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:52,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:53,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:54,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:12:54,129 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:13:00,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:01,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:02,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:03,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:04,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:05,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:06,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:07,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:08,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:09,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:09,136 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:13:15,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:16,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:17,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:18,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:19,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:20,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:21,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:22,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:23,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:24,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:24,144 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:13:30,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:31,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:32,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:33,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:34,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:35,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:36,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:37,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:38,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:39,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:39,151 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:13:45,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:46,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:47,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:48,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:49,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:50,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:51,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:52,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:53,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:54,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:13:54,158 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:14:00,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:01,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:02,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:03,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:04,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:05,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:06,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:07,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:08,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:09,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:09,165 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:14:15,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:16,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:17,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:18,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:19,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:20,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:21,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:22,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:23,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:24,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:24,173 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:14:30,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:31,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:32,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:33,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:34,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:35,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:36,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:37,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:38,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:39,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:39,180 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:14:45,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:46,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:47,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:48,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:49,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:50,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:51,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:52,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:53,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:54,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:14:54,187 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:15:00,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:01,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:02,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:03,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:04,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:05,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:06,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:07,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:08,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:09,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:09,195 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:15:15,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:16,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:17,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:18,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:19,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:20,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:21,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:22,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:23,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:24,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:24,202 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:15:30,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:31,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:32,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:33,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:34,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:35,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:36,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:37,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:38,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:39,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:39,210 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 12:15:45,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:46,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:47,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:48,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:49,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:50,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 12:15:50,261 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 12:15:50,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 12:16:59,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 12:16:59,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 12:17:00,095 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 12:17:00,158 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 12:17:00,158 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 12:17:00,164 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 12:17:00,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 12:17:00,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 12:17:00,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 12:17:00,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 12:17:00,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 12:17:00,285 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 12:17:00,292 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 12:17:00,298 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 12:17:00,303 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 12:17:00,305 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 12:17:00,305 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 12:17:00,305 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 12:17:00,315 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45279
2015-10-05 12:17:00,315 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 12:17:00,462 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45279
2015-10-05 12:17:00,551 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 12:17:00,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 12:17:00,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 12:17:00,590 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 12:17:00,601 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 12:17:00,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 12:17:00,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 12:17:00,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 12:17:00,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 12:17:00,680 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 12:17:00,681 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 12:17:01,070 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 8733@rushikesh1
2015-10-05 12:17:01,071 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /usr/local/hadoop/hdfs is not formatted for BP-1289193924-192.168.6.248-1444027603222
2015-10-05 12:17:01,071 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 12:17:01,232 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1289193924-192.168.6.248-1444027603222
2015-10-05 12:17:01,232 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /usr/local/hadoop/hdfs/current/BP-1289193924-192.168.6.248-1444027603222
2015-10-05 12:17:01,233 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /usr/local/hadoop/hdfs/current/BP-1289193924-192.168.6.248-1444027603222 is not formatted for BP-1289193924-192.168.6.248-1444027603222
2015-10-05 12:17:01,233 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 12:17:01,233 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1289193924-192.168.6.248-1444027603222 directory /usr/local/hadoop/hdfs/current/BP-1289193924-192.168.6.248-1444027603222/current
2015-10-05 12:17:01,271 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-05 12:17:01,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1731338917;bpid=BP-1289193924-192.168.6.248-1444027603222;lv=-56;nsInfo=lv=-63;cid=CID-835494d7-181a-47ee-a6fd-c158f23855e2;nsid=1731338917;c=0;bpid=BP-1289193924-192.168.6.248-1444027603222;dnuuid=null
2015-10-05 12:17:01,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID b90e4639-208c-4d8e-99f0-a1c9be3a3519
2015-10-05 12:17:01,433 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-54642fa3-8aa4-4e91-9cd5-5e282428b689
2015-10-05 12:17:01,433 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /usr/local/hadoop/hdfs/current, StorageType: DISK
2015-10-05 12:17:01,438 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-05 12:17:01,438 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1289193924-192.168.6.248-1444027603222
2015-10-05 12:17:01,439 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1289193924-192.168.6.248-1444027603222 on volume /usr/local/hadoop/hdfs/current...
2015-10-05 12:17:01,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1289193924-192.168.6.248-1444027603222 on /usr/local/hadoop/hdfs/current: 27ms
2015-10-05 12:17:01,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1289193924-192.168.6.248-1444027603222: 28ms
2015-10-05 12:17:01,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1289193924-192.168.6.248-1444027603222 on volume /usr/local/hadoop/hdfs/current...
2015-10-05 12:17:01,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1289193924-192.168.6.248-1444027603222 on volume /usr/local/hadoop/hdfs/current: 0ms
2015-10-05 12:17:01,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-10-05 12:17:01,649 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1289193924-192.168.6.248-1444027603222 on volume /usr/local/hadoop/hdfs
2015-10-05 12:17:01,651 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444035698651 with interval 21600000
2015-10-05 12:17:01,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1289193924-192.168.6.248-1444027603222 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-05 12:17:01,674 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop/hdfs, DS-54642fa3-8aa4-4e91-9cd5-5e282428b689): finished scanning block pool BP-1289193924-192.168.6.248-1444027603222
2015-10-05 12:17:01,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1289193924-192.168.6.248-1444027603222 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-05 12:17:01,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-05 12:17:01,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1289193924-192.168.6.248-1444027603222 (Datanode Uuid b90e4639-208c-4d8e-99f0-a1c9be3a3519) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1
2015-10-05 12:17:01,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1289193924-192.168.6.248-1444027603222 (Datanode Uuid b90e4639-208c-4d8e-99f0-a1c9be3a3519) service to rushikesh1/192.168.6.248:54310
2015-10-05 12:17:01,766 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop/hdfs, DS-54642fa3-8aa4-4e91-9cd5-5e282428b689): no suitable block pools found to scan.  Waiting 1814399883 ms.
2015-10-05 12:17:01,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2166b417bc4,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 57 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 12:17:01,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1289193924-192.168.6.248-1444027603222
2015-10-05 12:30:53,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1289193924-192.168.6.248-1444027603222:blk_1073741825_1001 src: /192.168.6.248:42172 dest: /192.168.6.248:50010
2015-10-05 12:30:59,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:42172, dest: /192.168.6.248:50010, bytes: 68143668, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-563106838_1, offset: 0, srvID: b90e4639-208c-4d8e-99f0-a1c9be3a3519, blockid: BP-1289193924-192.168.6.248-1444027603222:blk_1073741825_1001, duration: 6089836543
2015-10-05 12:30:59,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1289193924-192.168.6.248-1444027603222:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-05 12:39:08,635 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop/hdfs, DS-54642fa3-8aa4-4e91-9cd5-5e282428b689): Scheduling suspect block BP-1289193924-192.168.6.248-1444027603222:blk_1073741825_1001 for rescanning.
2015-10-05 12:39:08,637 WARN io.netty.handler.stream.ChunkedWriteHandler: ChunkedInput.isEndOfInput() failed
java.io.IOException: Stream closed
	at java.io.PushbackInputStream.ensureOpen(PushbackInputStream.java:74)
	at java.io.PushbackInputStream.read(PushbackInputStream.java:135)
	at io.netty.handler.stream.ChunkedStream.isEndOfInput(ChunkedStream.java:82)
	at io.netty.handler.stream.ChunkedWriteHandler.discard(ChunkedWriteHandler.java:177)
	at io.netty.handler.stream.ChunkedWriteHandler.doFlush(ChunkedWriteHandler.java:203)
	at io.netty.handler.stream.ChunkedWriteHandler.channelInactive(ChunkedWriteHandler.java:147)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
	at io.netty.handler.codec.ReplayingDecoder.channelInactive(ReplayingDecoder.java:352)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:769)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$5.run(AbstractChannel.java:567)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 12:40:14,137 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop/hdfs, DS-54642fa3-8aa4-4e91-9cd5-5e282428b689): no suitable block pools found to scan.  Waiting 1813007512 ms.
2015-10-05 13:22:01,628 WARN io.netty.handler.stream.ChunkedWriteHandler: ChunkedInput.isEndOfInput() failed
java.io.IOException: Stream closed
	at java.io.PushbackInputStream.ensureOpen(PushbackInputStream.java:74)
	at java.io.PushbackInputStream.read(PushbackInputStream.java:135)
	at io.netty.handler.stream.ChunkedStream.isEndOfInput(ChunkedStream.java:82)
	at io.netty.handler.stream.ChunkedWriteHandler.discard(ChunkedWriteHandler.java:177)
	at io.netty.handler.stream.ChunkedWriteHandler.doFlush(ChunkedWriteHandler.java:203)
	at io.netty.handler.stream.ChunkedWriteHandler.channelInactive(ChunkedWriteHandler.java:147)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
	at io.netty.handler.codec.ReplayingDecoder.channelInactive(ReplayingDecoder.java:352)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:769)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$5.run(AbstractChannel.java:567)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 13:22:01,630 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop/hdfs, DS-54642fa3-8aa4-4e91-9cd5-5e282428b689): Scheduling suspect block BP-1289193924-192.168.6.248-1444027603222:blk_1073741825_1001 for rescanning.
2015-10-05 13:23:06,633 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/usr/local/hadoop/hdfs, DS-54642fa3-8aa4-4e91-9cd5-5e282428b689): no suitable block pools found to scan.  Waiting 1810435016 ms.
2015-10-05 13:33:47,950 WARN io.netty.handler.stream.ChunkedWriteHandler: ChunkedInput.isEndOfInput() failed
java.io.IOException: Stream closed
	at java.io.PushbackInputStream.ensureOpen(PushbackInputStream.java:74)
	at java.io.PushbackInputStream.read(PushbackInputStream.java:135)
	at io.netty.handler.stream.ChunkedStream.isEndOfInput(ChunkedStream.java:82)
	at io.netty.handler.stream.ChunkedWriteHandler.discard(ChunkedWriteHandler.java:177)
	at io.netty.handler.stream.ChunkedWriteHandler.doFlush(ChunkedWriteHandler.java:203)
	at io.netty.handler.stream.ChunkedWriteHandler.channelInactive(ChunkedWriteHandler.java:147)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
	at io.netty.handler.codec.ReplayingDecoder.channelInactive(ReplayingDecoder.java:352)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:769)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$5.run(AbstractChannel.java:567)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 13:56:33,674 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-05 13:56:36,817 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 13:56:36,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 13:57:32,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 13:57:32,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 13:57:32,953 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 13:57:33,016 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 13:57:33,016 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 13:57:33,021 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 13:57:33,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 13:57:33,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 13:57:33,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 13:57:33,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 13:57:33,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 13:57:33,141 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 13:57:33,149 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 13:57:33,154 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 13:57:33,159 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 13:57:33,161 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 13:57:33,162 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 13:57:33,162 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 13:57:33,172 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41609
2015-10-05 13:57:33,172 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 13:57:33,320 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41609
2015-10-05 13:57:33,407 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 13:57:33,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 13:57:33,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 13:57:33,447 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 13:57:33,458 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 13:57:33,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 13:57:33,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 13:57:33,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 13:57:33,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 13:57:33,536 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 13:57:33,537 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 13:57:33,896 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 16050@rushikesh1
2015-10-05 13:57:33,897 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop/hdfs: namenode clusterID = CID-0263b19d-6040-4c0e-a744-ce12d3c03f5c; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 13:57:33,898 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 13:57:33,899 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 13:57:34,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 13:57:36,000 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 13:57:36,002 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 13:57:36,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:00:52,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:00:52,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:00:53,342 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:00:53,405 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:00:53,405 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:00:53,410 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:00:53,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:00:53,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:00:53,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:00:53,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:00:53,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:00:53,530 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:00:53,538 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:00:53,543 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:00:53,548 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:00:53,550 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:00:53,550 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:00:53,550 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:00:53,560 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 58605
2015-10-05 14:00:53,560 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:00:53,713 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:58605
2015-10-05 14:00:53,796 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:00:53,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:00:53,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:00:53,838 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:00:53,849 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:00:53,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:00:53,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:00:53,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:00:53,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:00:53,930 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:00:53,930 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:00:54,273 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 18091@rushikesh1
2015-10-05 14:00:54,275 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop/hdfs: namenode clusterID = CID-e69bfabe-c44c-4976-9ee1-582158f3e7c9; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 14:00:54,276 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 14:00:54,277 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:00:54,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 14:00:56,378 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 14:00:56,380 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 14:00:56,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:08:45,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:08:45,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:08:46,347 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:08:46,410 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:08:46,410 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:08:46,416 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:08:46,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:08:46,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:08:46,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:08:46,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:08:46,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:08:46,534 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:08:46,542 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:08:46,547 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:08:46,552 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:08:46,554 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:08:46,554 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:08:46,554 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:08:46,564 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 59209
2015-10-05 14:08:46,564 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:08:46,727 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:59209
2015-10-05 14:08:46,814 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:08:46,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:08:46,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:08:46,854 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:08:46,865 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:08:46,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:08:46,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:08:46,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:08:46,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:08:46,946 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:08:46,946 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:08:47,165 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 19666@rushikesh1
2015-10-05 14:08:47,167 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop/hdfs: namenode clusterID = CID-e69bfabe-c44c-4976-9ee1-582158f3e7c9; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 14:08:47,168 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 14:08:47,169 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:08:47,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 14:08:49,270 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 14:08:49,272 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 14:08:49,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:09:48,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:09:48,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:09:48,696 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:09:48,760 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:09:48,760 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:09:48,765 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:09:48,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:09:48,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:09:48,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:09:48,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:09:48,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:09:48,883 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:09:48,891 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:09:48,896 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:09:48,901 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:09:48,904 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:09:48,904 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:09:48,904 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:09:48,914 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 58670
2015-10-05 14:09:48,914 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:09:49,062 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:58670
2015-10-05 14:09:49,149 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:09:49,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:09:49,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:09:49,189 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:09:49,200 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:09:49,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:09:49,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:09:49,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:09:49,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:09:49,280 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:09:49,280 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:09:50,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:09:51,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:09:52,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:09:53,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:09:54,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:09:55,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:09:56,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:09:57,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:09:58,358 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:09:59,358 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:09:59,359 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:10:05,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:06,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:07,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:08,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:09,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:10,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:11,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:12,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:13,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:14,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:14,367 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:10:20,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:21,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:22,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:23,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:24,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:25,371 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:26,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:27,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:10:27,834 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 14:10:27,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:11:00,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:11:00,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:11:00,983 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:11:01,046 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:11:01,046 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:11:01,051 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:11:01,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:11:01,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:11:01,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:11:01,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:11:01,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:11:01,173 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:11:01,181 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:11:01,186 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:11:01,191 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:11:01,193 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:11:01,193 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:11:01,193 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:11:01,203 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37981
2015-10-05 14:11:01,203 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:11:01,354 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37981
2015-10-05 14:11:01,444 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:11:01,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:11:01,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:11:01,484 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:11:01,496 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:11:01,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:11:01,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:11:01,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:11:01,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:11:01,588 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:11:01,589 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:11:02,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:03,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:04,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:05,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:06,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:07,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:08,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:09,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:10,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:11,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:11,668 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:11:17,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:18,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:19,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:20,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:21,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:22,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:23,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:24,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:25,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:11:26,107 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 14:11:26,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:12:07,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:12:07,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:12:08,321 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:12:08,385 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:12:08,385 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:12:08,390 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:12:08,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:12:08,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:12:08,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:12:08,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:12:08,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:12:08,513 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:12:08,521 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:12:08,526 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:12:08,531 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:12:08,533 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:12:08,533 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:12:08,534 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:12:08,544 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38301
2015-10-05 14:12:08,544 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:12:08,696 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38301
2015-10-05 14:12:08,780 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:12:08,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:12:08,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:12:08,820 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:12:08,831 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:12:08,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:12:08,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:12:08,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:12:08,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:12:08,911 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:12:08,911 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:12:09,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:10,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:11,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:12,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:13,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:14,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:15,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:16,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:17,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:18,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:18,992 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:12:24,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:25,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:26,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:27,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:28,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:29,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:30,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:31,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:32,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:33,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:34,000 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:12:40,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:41,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:42,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:43,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:44,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:45,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:46,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:47,005 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:48,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:49,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:49,007 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:12:55,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:56,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:57,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:58,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:12:59,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:00,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:01,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:02,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:03,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:04,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:04,015 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:13:10,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:11,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:12,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:13,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:14,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:15,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:16,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:17,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:18,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:19,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:19,022 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:13:25,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:26,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:27,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:28,025 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:29,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:30,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:31,027 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:32,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:33,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:34,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:34,030 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:13:40,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:41,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:42,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:43,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:44,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:45,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:46,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:47,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:48,036 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:49,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:13:49,037 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:13:51,348 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 14:13:51,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:14:55,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:14:55,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:14:55,980 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:14:56,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:14:56,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:14:56,048 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:14:56,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:14:56,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:14:56,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:14:56,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:14:56,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:14:56,167 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:14:56,175 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:14:56,180 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:14:56,185 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:14:56,187 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:14:56,187 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:14:56,187 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:14:56,197 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 55929
2015-10-05 14:14:56,197 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:14:56,351 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:55929
2015-10-05 14:14:56,432 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:14:56,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:14:56,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:14:56,472 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:14:56,483 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:14:56,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:14:56,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:14:56,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:14:56,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:14:56,563 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:14:56,563 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:14:57,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:14:58,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:14:59,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:00,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:01,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:02,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:03,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:04,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:05,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:06,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:06,642 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:15:12,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:13,644 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:14,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:15,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:16,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:17,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:18,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:19,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:20,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:21,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:21,650 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:15:27,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:28,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:29,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:30,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:31,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:32,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:33,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:15:34,138 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 14:15:34,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:18:00,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:18:00,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:18:01,573 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:18:01,635 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:18:01,636 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:18:01,641 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:18:01,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:18:01,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:18:01,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:18:01,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:18:01,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:18:01,759 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:18:01,767 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:18:01,772 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:18:01,777 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:18:01,780 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:18:01,780 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:18:01,780 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:18:01,793 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42716
2015-10-05 14:18:01,793 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:18:01,943 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42716
2015-10-05 14:18:02,030 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:18:02,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:18:02,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:18:02,070 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:18:02,081 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:18:02,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:18:02,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:18:02,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:18:02,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:18:02,160 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:18:02,160 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:18:03,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:04,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:05,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:06,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:07,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:08,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:09,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:10,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:11,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:12,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:12,240 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:18:18,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:19,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:20,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:21,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:22,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:23,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:24,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:25,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:26,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:27,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:27,247 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:18:33,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:34,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:35,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:36,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:37,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:38,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:39,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:40,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:41,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:42,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:42,254 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:18:48,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:49,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:50,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:51,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:52,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:53,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:54,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:55,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:56,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:57,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:18:57,262 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:19:03,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:04,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:05,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:06,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:07,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:08,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:09,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:10,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:11,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:12,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:12,269 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:19:18,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:19,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:20,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:21,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:22,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:23,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:24,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:25,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:26,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:27,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:27,277 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:19:33,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:34,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:35,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:36,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:37,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:38,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:39,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:40,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:41,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:42,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:42,284 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:19:48,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:49,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:50,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:51,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:52,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:53,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:54,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:55,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:56,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:57,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:19:57,291 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:20:03,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:04,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:05,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:06,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:07,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:08,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:09,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:10,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:11,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:12,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:12,299 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:20:18,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:19,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:20,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:21,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:22,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:23,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:24,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:25,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:26,305 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:27,305 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:27,306 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:20:33,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:34,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:35,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:36,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:37,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:38,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:39,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:40,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:41,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:42,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:20:42,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:20:42,963 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 14:20:42,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:21:16,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:21:16,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:21:17,217 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:21:17,279 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:21:17,279 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:21:17,283 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:21:17,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:21:17,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:21:17,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:21:17,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:21:17,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:21:17,403 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:21:17,411 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:21:17,416 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:21:17,421 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:21:17,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:21:17,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:21:17,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:21:17,434 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 44393
2015-10-05 14:21:17,434 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:21:17,587 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44393
2015-10-05 14:21:17,670 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:21:17,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:21:17,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:21:17,710 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:21:17,721 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:21:17,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:21:17,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:21:17,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:21:17,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:21:17,800 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:21:17,800 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:21:18,200 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 30416@rushikesh1
2015-10-05 14:21:18,202 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop/hdfs: namenode clusterID = CID-a364f9a4-e79a-44a6-81f9-9bb294355762; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 14:21:18,203 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 14:21:18,204 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:21:18,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 14:21:20,305 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 14:21:20,307 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 14:21:20,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:22:48,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:22:48,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:22:48,887 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:22:48,950 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:22:48,950 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:22:48,955 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:22:48,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:22:48,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:22:48,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:22:48,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:22:48,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:22:49,072 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:22:49,080 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:22:49,085 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:22:49,090 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:22:49,092 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:22:49,093 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:22:49,093 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:22:49,102 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 59429
2015-10-05 14:22:49,103 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:22:49,254 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:59429
2015-10-05 14:22:49,337 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:22:49,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:22:49,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:22:49,376 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:22:49,387 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:22:49,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:22:49,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:22:49,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:22:49,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:22:49,468 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:22:49,468 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:22:49,790 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 32284@rushikesh1
2015-10-05 14:22:49,791 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop/hdfs: namenode clusterID = CID-16e26b6a-c6d3-4305-9258-6ac2377f9361; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 14:22:49,792 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 14:22:49,793 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:22:49,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 14:22:51,894 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 14:22:51,896 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 14:22:51,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:24:34,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:24:34,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:24:34,831 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:24:34,894 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:24:34,894 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:24:34,899 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:24:34,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:24:34,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:24:34,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:24:34,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:24:34,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:24:35,019 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:24:35,027 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:24:35,032 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:24:35,037 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:24:35,039 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:24:35,039 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:24:35,039 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:24:35,049 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 58635
2015-10-05 14:24:35,049 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:24:35,203 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:58635
2015-10-05 14:24:35,286 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:24:35,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:24:35,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:24:35,326 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:24:35,337 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:24:35,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:24:35,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:24:35,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:24:35,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:24:35,416 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:24:35,416 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:24:35,749 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 1980@rushikesh1
2015-10-05 14:24:35,751 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop/hdfs: namenode clusterID = CID-b7b4773c-aa10-4221-b19a-bc503e2eb21d; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 14:24:35,752 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 14:24:35,753 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:24:35,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 14:24:37,854 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 14:24:37,856 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 14:24:37,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:27:47,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:27:47,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:27:48,507 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:27:48,570 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:27:48,570 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:27:48,575 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:27:48,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:27:48,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:27:48,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:27:48,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:27:48,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:27:48,696 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:27:48,704 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:27:48,709 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:27:48,714 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:27:48,716 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:27:48,716 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:27:48,716 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:27:48,726 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36905
2015-10-05 14:27:48,726 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:27:48,878 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36905
2015-10-05 14:27:48,960 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:27:48,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:27:48,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:27:49,000 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:27:49,012 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:27:49,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:27:49,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:27:49,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:27:49,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:27:49,091 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:27:49,091 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:27:49,461 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 4173@rushikesh1
2015-10-05 14:27:49,463 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop/hdfs: namenode clusterID = CID-3d97d519-575f-4203-ab3b-88fb90d7ad6e; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 14:27:49,463 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 14:27:49,465 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:27:49,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 14:27:51,566 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 14:27:51,568 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 14:27:51,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:29:48,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:29:48,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:29:48,780 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:29:48,843 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:29:48,843 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:29:48,848 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:29:48,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:29:48,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:29:48,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:29:48,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:29:48,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:29:48,969 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:29:48,977 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:29:48,982 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:29:48,987 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:29:48,989 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:29:48,989 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:29:48,989 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:29:48,999 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 52717
2015-10-05 14:29:48,999 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:29:49,154 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:52717
2015-10-05 14:29:49,237 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:29:49,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:29:49,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:29:49,276 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:29:49,287 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:29:49,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:29:49,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:29:49,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:29:49,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:29:49,367 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:29:49,367 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:29:50,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:29:51,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:29:52,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:29:53,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:29:54,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:29:55,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:29:56,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:29:57,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:29:58,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:29:59,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:29:59,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:30:05,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:06,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:07,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:08,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:09,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:10,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:11,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:12,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:13,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:14,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:14,455 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:30:20,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:21,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:22,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:23,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:24,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:25,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:30:25,826 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 14:30:25,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:31:44,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:31:44,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:31:45,602 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:31:45,664 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:31:45,664 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:31:45,669 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:31:45,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:31:45,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:31:45,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:31:45,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:31:45,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:31:45,789 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:31:45,797 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:31:45,802 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:31:45,807 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:31:45,809 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:31:45,809 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:31:45,809 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:31:45,819 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 49691
2015-10-05 14:31:45,819 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:31:45,966 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:49691
2015-10-05 14:31:46,050 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:31:46,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:31:46,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:31:46,090 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:31:46,101 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:31:46,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:31:46,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:31:46,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:31:46,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:31:46,180 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:31:46,180 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:31:46,511 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 7919@rushikesh1
2015-10-05 14:31:46,513 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop/hdfs: namenode clusterID = CID-0341059b-3255-4303-b873-ad811b4bbab8; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 14:31:46,513 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 14:31:46,514 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:31:46,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 14:31:48,615 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 14:31:48,617 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 14:31:48,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:33:48,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:33:49,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:33:49,608 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:33:49,670 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:33:49,670 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:33:49,676 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:33:49,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:33:49,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:33:49,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:33:49,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:33:49,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:33:49,794 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:33:49,801 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:33:49,807 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:33:49,812 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:33:49,814 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:33:49,814 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:33:49,814 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:33:49,824 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 33659
2015-10-05 14:33:49,824 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:33:49,976 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33659
2015-10-05 14:33:50,058 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:33:50,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:33:50,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:33:50,098 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:33:50,109 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:33:50,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:33:50,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:33:50,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:33:50,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:33:50,188 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:33:50,188 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:33:50,545 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 9817@rushikesh1
2015-10-05 14:33:50,547 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop/hdfs: namenode clusterID = CID-24649b29-13ea-4ce3-8cc6-d04d7aa1f2f6; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 14:33:50,548 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 14:33:50,549 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:33:50,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 14:33:52,650 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 14:33:52,652 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 14:33:52,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:35:52,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:35:52,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:35:53,009 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:35:53,072 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:35:53,072 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:35:53,078 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:35:53,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:35:53,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:35:53,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:35:53,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:35:53,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:35:53,199 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:35:53,207 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:35:53,212 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:35:53,217 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:35:53,220 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:35:53,220 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:35:53,220 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:35:53,230 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43314
2015-10-05 14:35:53,230 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:35:53,382 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43314
2015-10-05 14:35:53,466 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:35:53,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:35:53,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:35:53,506 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:35:53,517 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:35:53,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:35:53,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:35:53,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:35:53,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:35:53,597 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:35:53,597 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:35:54,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:35:55,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:35:56,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:35:57,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:35:58,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:35:59,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:00,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:01,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:02,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:03,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:03,677 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:36:09,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:10,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:11,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:12,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:13,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:14,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:15,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:16,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:17,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:18,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:18,685 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:36:24,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:25,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:26,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:27,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:28,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:29,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:30,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:31,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:32,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:33,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:33,693 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:36:39,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:40,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:41,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:42,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:43,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:36:44,031 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 14:36:44,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:41:56,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:41:56,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:41:57,522 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:41:57,585 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:41:57,585 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:41:57,590 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:41:57,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:41:57,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:41:57,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:41:57,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:41:57,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:41:57,710 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:41:57,718 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:41:57,723 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:41:57,728 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:41:57,731 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:41:57,731 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:41:57,731 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:41:57,741 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 33469
2015-10-05 14:41:57,741 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:41:57,889 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33469
2015-10-05 14:41:57,976 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:41:57,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:41:57,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:41:58,016 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:41:58,027 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:41:58,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:41:58,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:41:58,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:41:58,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:41:58,107 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:41:58,107 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:41:58,443 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 13769@rushikesh1
2015-10-05 14:41:58,445 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop/hdfs: namenode clusterID = CID-45e22ac5-343f-48fe-b7d6-c5047ca13ee5; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 14:41:58,445 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 14:41:58,446 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:41:58,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 14:42:00,547 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 14:42:00,549 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 14:42:00,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:43:46,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:43:46,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:43:47,489 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:43:47,552 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:43:47,552 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:43:47,557 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:43:47,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:43:47,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:43:47,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:43:47,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:43:47,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:43:47,677 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:43:47,685 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:43:47,690 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:43:47,695 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:43:47,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:43:47,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:43:47,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:43:47,708 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 46733
2015-10-05 14:43:47,708 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:43:47,863 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46733
2015-10-05 14:43:47,946 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:43:47,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:43:47,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:43:47,986 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:43:47,999 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:43:48,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:43:48,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:43:48,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:43:48,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:43:48,081 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:43:48,081 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:43:49,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:43:50,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:43:51,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:43:52,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:43:53,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:43:54,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:43:55,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:43:56,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:43:57,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:43:58,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:43:58,160 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:44:04,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:44:05,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:44:06,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:44:07,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:44:08,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:44:09,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:44:10,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:44:11,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:44:12,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:44:13,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:44:13,168 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:44:18,992 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 14:44:18,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:46:51,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:46:51,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:46:52,271 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:46:52,334 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:46:52,334 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:46:52,339 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:46:52,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:46:52,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:46:52,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:46:52,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:46:52,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:46:52,474 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:46:52,482 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:46:52,488 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:46:52,492 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:46:52,495 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:46:52,495 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:46:52,495 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:46:52,505 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 59933
2015-10-05 14:46:52,505 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:46:52,658 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:59933
2015-10-05 14:46:52,782 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:46:52,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:46:52,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:46:52,937 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:46:52,954 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:46:53,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:46:53,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:46:53,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:46:53,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:46:53,247 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:46:53,248 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:46:54,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:46:55,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:46:56,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:46:57,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:46:58,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:46:59,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:00,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:01,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:02,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:03,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:03,401 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:47:09,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:10,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:11,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:12,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:13,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:14,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:15,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:16,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:17,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:18,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:18,408 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:47:24,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:25,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:26,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:27,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:28,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:29,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:30,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:31,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:32,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:33,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:33,415 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:47:39,416 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:40,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:41,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:42,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:43,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:44,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:45,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:46,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:47,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:48,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:48,423 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:47:54,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:55,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:56,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:57,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:58,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:47:59,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:00,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:01,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:02,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:03,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:03,430 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:48:09,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:10,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:11,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:12,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:13,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:14,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:15,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:16,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:17,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:18,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:18,437 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:48:24,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:25,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:26,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:27,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:28,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:29,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:30,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:31,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:32,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:33,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:33,445 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:48:39,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:40,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:41,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:42,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:43,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:44,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:45,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:46,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:47,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:48,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:48,452 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:48:54,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:55,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:56,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:57,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:58,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:48:59,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:00,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:01,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:02,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:03,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:03,460 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:49:09,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:10,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:11,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:12,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:13,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:14,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:15,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:16,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:17,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:18,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:18,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:49:24,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:25,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:26,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:27,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:28,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:29,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:30,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:31,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:32,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:33,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:33,475 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:49:39,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:40,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:41,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:42,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:43,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:44,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:45,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:46,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:47,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:48,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:48,482 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:49:54,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:55,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:56,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:57,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:58,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:49:59,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:00,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:01,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:02,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:03,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:03,489 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:50:09,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:10,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:11,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:12,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:13,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:14,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:15,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:16,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:17,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:18,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:18,496 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:50:24,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:25,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:26,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:27,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:28,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:29,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:30,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:31,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:32,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:33,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:33,504 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:50:39,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:40,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:41,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:42,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:43,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:44,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:45,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:46,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:47,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:48,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:48,512 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:50:54,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:55,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:56,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:57,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:58,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:50:59,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:00,517 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:01,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:02,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:03,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:03,520 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:51:09,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:10,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:11,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:12,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:13,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:14,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:15,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:16,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:17,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:18,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:51:18,531 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 14:51:19,804 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 14:51:19,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:52:20,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:52:20,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:52:20,704 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:52:20,766 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:52:20,767 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:52:20,771 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:52:20,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:52:20,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:52:20,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:52:20,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:52:20,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:52:20,894 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:52:20,902 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:52:20,907 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:52:20,912 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:52:20,914 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:52:20,914 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:52:20,914 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:52:20,924 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 52123
2015-10-05 14:52:20,924 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:52:21,079 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:52123
2015-10-05 14:52:21,161 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:52:21,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:52:21,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:52:21,202 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:52:21,213 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:52:21,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:52:21,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:52:21,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:52:21,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:52:21,294 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:52:21,294 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:52:21,711 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hadoop/hdfs/in_use.lock acquired by nodename 4987@rushikesh1
2015-10-05 14:52:21,731 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /usr/local/hadoop/hdfs: namenode clusterID = CID-1c598070-6447-4935-b7b1-7797894dd1ab; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 14:52:21,732 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 14:52:21,733 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:52:21,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 14:52:23,834 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 14:52:23,836 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 14:52:23,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:56:27,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:56:27,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:56:28,480 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:56:28,545 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:56:28,545 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:56:28,551 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:56:28,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:56:28,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:56:28,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:56:28,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:56:28,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:56:28,662 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:56:28,670 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:56:28,675 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:56:28,680 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:56:28,682 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:56:28,682 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:56:28,682 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:56:28,692 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41947
2015-10-05 14:56:28,692 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:56:28,844 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41947
2015-10-05 14:56:28,926 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:56:28,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:56:28,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:56:28,966 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:56:28,977 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:56:29,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:56:29,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:56:29,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:56:29,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:56:29,056 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:56:29,057 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:56:29,408 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 6988@rushikesh1
2015-10-05 14:56:29,409 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /app/hadoop/tmp/dfs/data is not formatted for BP-1047307160-192.168.6.248-1444037177454
2015-10-05 14:56:29,409 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 14:56:29,533 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1047307160-192.168.6.248-1444037177454
2015-10-05 14:56:29,533 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1047307160-192.168.6.248-1444037177454
2015-10-05 14:56:29,534 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /app/hadoop/tmp/dfs/data/current/BP-1047307160-192.168.6.248-1444037177454 is not formatted for BP-1047307160-192.168.6.248-1444037177454
2015-10-05 14:56:29,534 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 14:56:29,534 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1047307160-192.168.6.248-1444037177454 directory /app/hadoop/tmp/dfs/data/current/BP-1047307160-192.168.6.248-1444037177454/current
2015-10-05 14:56:29,584 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-05 14:56:29,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1076933745;bpid=BP-1047307160-192.168.6.248-1444037177454;lv=-56;nsInfo=lv=-63;cid=CID-1905fb61-f005-431a-8d60-f7510caef36b;nsid=1076933745;c=0;bpid=BP-1047307160-192.168.6.248-1444037177454;dnuuid=null
2015-10-05 14:56:29,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID c93258dd-c611-4aa5-97b7-24650de092ef
2015-10-05 14:56:29,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ac877d8a-2ce2-4932-8a7a-8e922899d0b8
2015-10-05 14:56:29,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-05 14:56:29,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-05 14:56:29,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1047307160-192.168.6.248-1444037177454
2015-10-05 14:56:29,762 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1047307160-192.168.6.248-1444037177454 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 14:56:29,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1047307160-192.168.6.248-1444037177454 on /app/hadoop/tmp/dfs/data/current: 34ms
2015-10-05 14:56:29,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1047307160-192.168.6.248-1444037177454: 35ms
2015-10-05 14:56:29,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1047307160-192.168.6.248-1444037177454 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 14:56:29,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1047307160-192.168.6.248-1444037177454 on volume /app/hadoop/tmp/dfs/data/current: 0ms
2015-10-05 14:56:29,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-10-05 14:56:30,136 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1047307160-192.168.6.248-1444037177454 on volume /app/hadoop/tmp/dfs/data
2015-10-05 14:56:30,138 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444042010138 with interval 21600000
2015-10-05 14:56:30,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1047307160-192.168.6.248-1444037177454 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-05 14:56:30,144 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ac877d8a-2ce2-4932-8a7a-8e922899d0b8): finished scanning block pool BP-1047307160-192.168.6.248-1444037177454
2015-10-05 14:56:30,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1047307160-192.168.6.248-1444037177454 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-05 14:56:30,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-05 14:56:30,243 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ac877d8a-2ce2-4932-8a7a-8e922899d0b8): no suitable block pools found to scan.  Waiting 1814399893 ms.
2015-10-05 14:56:30,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1047307160-192.168.6.248-1444037177454 (Datanode Uuid c93258dd-c611-4aa5-97b7-24650de092ef) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1
2015-10-05 14:56:30,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1047307160-192.168.6.248-1444037177454 (Datanode Uuid c93258dd-c611-4aa5-97b7-24650de092ef) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:56:30,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa118c6ce75,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 64 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 14:56:30,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1047307160-192.168.6.248-1444037177454
2015-10-05 14:58:29,297 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-05 14:58:33,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:58:34,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 14:58:34,192 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 14:58:34,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 14:59:36,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 14:59:36,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 14:59:37,493 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 14:59:37,555 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 14:59:37,555 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 14:59:37,560 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 14:59:37,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 14:59:37,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 14:59:37,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 14:59:37,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 14:59:37,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 14:59:37,678 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 14:59:37,686 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 14:59:37,691 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 14:59:37,696 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 14:59:37,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 14:59:37,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 14:59:37,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 14:59:37,708 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 33904
2015-10-05 14:59:37,708 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 14:59:37,861 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33904
2015-10-05 14:59:37,943 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 14:59:37,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 14:59:37,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 14:59:37,982 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 14:59:37,993 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 14:59:38,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 14:59:38,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 14:59:38,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 14:59:38,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 14:59:38,071 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 14:59:38,071 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 14:59:38,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 8912@rushikesh1
2015-10-05 14:59:38,567 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1047307160-192.168.6.248-1444037177454
2015-10-05 14:59:38,567 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1047307160-192.168.6.248-1444037177454
2015-10-05 14:59:38,568 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-05 14:59:38,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1076933745;bpid=BP-1047307160-192.168.6.248-1444037177454;lv=-56;nsInfo=lv=-63;cid=CID-1905fb61-f005-431a-8d60-f7510caef36b;nsid=1076933745;c=0;bpid=BP-1047307160-192.168.6.248-1444037177454;dnuuid=c93258dd-c611-4aa5-97b7-24650de092ef
2015-10-05 14:59:38,643 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ac877d8a-2ce2-4932-8a7a-8e922899d0b8
2015-10-05 14:59:38,643 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-05 14:59:38,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-05 14:59:38,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1047307160-192.168.6.248-1444037177454
2015-10-05 14:59:38,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1047307160-192.168.6.248-1444037177454 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 14:59:38,682 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1047307160-192.168.6.248-1444037177454/current: 24576
2015-10-05 14:59:38,684 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1047307160-192.168.6.248-1444037177454 on /app/hadoop/tmp/dfs/data/current: 14ms
2015-10-05 14:59:38,684 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1047307160-192.168.6.248-1444037177454: 15ms
2015-10-05 14:59:38,684 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1047307160-192.168.6.248-1444037177454 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 14:59:38,685 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1047307160-192.168.6.248-1444037177454 on volume /app/hadoop/tmp/dfs/data/current: 0ms
2015-10-05 14:59:38,685 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-10-05 14:59:38,844 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ac877d8a-2ce2-4932-8a7a-8e922899d0b8): no suitable block pools found to scan.  Waiting 1814211292 ms.
2015-10-05 14:59:38,846 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444058961846 with interval 21600000
2015-10-05 14:59:38,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1047307160-192.168.6.248-1444037177454 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-05 14:59:38,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1047307160-192.168.6.248-1444037177454 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-05 14:59:38,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-05 14:59:38,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1047307160-192.168.6.248-1444037177454 (Datanode Uuid c93258dd-c611-4aa5-97b7-24650de092ef) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2
2015-10-05 14:59:38,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1047307160-192.168.6.248-1444037177454 (Datanode Uuid c93258dd-c611-4aa5-97b7-24650de092ef) service to rushikesh1/192.168.6.248:54310
2015-10-05 14:59:39,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xcd039f79e7,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 44 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 14:59:39,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1047307160-192.168.6.248-1444037177454
2015-10-05 14:59:56,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd0fdb9337d,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 5 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 14:59:56,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1047307160-192.168.6.248-1444037177454
2015-10-05 15:00:23,065 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-05 15:00:26,220 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 15:00:26,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 15:01:08,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 15:01:08,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 15:01:09,291 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 15:01:09,354 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 15:01:09,354 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 15:01:09,359 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 15:01:09,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 15:01:09,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 15:01:09,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 15:01:09,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 15:01:09,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 15:01:09,478 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 15:01:09,486 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 15:01:09,491 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 15:01:09,496 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 15:01:09,498 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 15:01:09,498 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 15:01:09,498 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 15:01:09,508 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 46761
2015-10-05 15:01:09,508 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 15:01:09,658 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46761
2015-10-05 15:01:09,741 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 15:01:09,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 15:01:09,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 15:01:09,781 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 15:01:09,792 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 15:01:09,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 15:01:09,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 15:01:09,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 15:01:09,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 15:01:09,872 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 15:01:09,872 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 15:01:10,255 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 10816@rushikesh1
2015-10-05 15:01:10,256 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-d6becb97-7542-4f0c-8e82-431112946387; datanode clusterID = CID-1905fb61-f005-431a-8d60-f7510caef36b
2015-10-05 15:01:10,257 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 15:01:10,258 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 15:01:10,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 15:01:12,359 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 15:01:12,361 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 15:01:12,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 15:02:56,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 15:02:56,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 15:02:57,443 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 15:02:57,509 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 15:02:57,509 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 15:02:57,514 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 15:02:57,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 15:02:57,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 15:02:57,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 15:02:57,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 15:02:57,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 15:02:57,626 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 15:02:57,633 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 15:02:57,639 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 15:02:57,643 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 15:02:57,645 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 15:02:57,646 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 15:02:57,646 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 15:02:57,656 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45330
2015-10-05 15:02:57,656 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 15:02:57,807 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45330
2015-10-05 15:02:57,889 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 15:02:57,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 15:02:57,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 15:02:57,929 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 15:02:57,940 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 15:02:57,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 15:02:57,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 15:02:58,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 15:02:58,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 15:02:58,019 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 15:02:58,019 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 15:02:58,368 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 12719@rushikesh1
2015-10-05 15:02:58,369 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /app/hadoop/tmp/dfs/data is not formatted for BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:02:58,369 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 15:02:58,485 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:02:58,485 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:02:58,485 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733 is not formatted for BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:02:58,485 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 15:02:58,486 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1750158012-192.168.6.248-1444037565733 directory /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current
2015-10-05 15:02:58,535 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-05 15:02:58,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=null
2015-10-05 15:02:58,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-05 15:02:58,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-05 15:02:58,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-05 15:02:58,686 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-05 15:02:58,687 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:02:58,688 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 15:02:58,700 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 12ms
2015-10-05 15:02:58,700 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 13ms
2015-10-05 15:02:58,700 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 15:02:58,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 0ms
2015-10-05 15:02:58,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-10-05 15:02:58,828 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data
2015-10-05 15:02:58,829 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444037689829 with interval 21600000
2015-10-05 15:02:58,830 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): finished scanning block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:02:58,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-05 15:02:58,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-05 15:02:58,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-05 15:02:58,869 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1814399959 ms.
2015-10-05 15:02:58,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1
2015-10-05 15:02:58,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-05 15:02:58,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xfb91299744,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 40 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 15:02:58,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:04:19,013 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-05 15:04:23,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 15:04:23,395 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 15:04:23,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 15:04:57,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 15:04:57,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 15:04:58,051 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 15:04:58,113 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 15:04:58,113 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 15:04:58,118 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 15:04:58,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 15:04:58,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 15:04:58,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 15:04:58,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 15:04:58,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 15:04:58,238 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 15:04:58,245 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 15:04:58,251 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 15:04:58,255 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 15:04:58,258 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 15:04:58,258 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 15:04:58,258 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 15:04:58,268 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 57023
2015-10-05 15:04:58,268 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 15:04:58,417 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57023
2015-10-05 15:04:58,499 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 15:04:58,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 15:04:58,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 15:04:58,539 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 15:04:58,550 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 15:04:58,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 15:04:58,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 15:04:58,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 15:04:58,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 15:04:58,628 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 15:04:58,628 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 15:04:58,980 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 14561@rushikesh1
2015-10-05 15:04:59,067 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:04:59,067 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:04:59,068 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-05 15:04:59,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-05 15:04:59,142 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-05 15:04:59,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-05 15:04:59,171 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-05 15:04:59,172 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:04:59,172 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 15:04:59,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 24576
2015-10-05 15:04:59,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-10-05 15:04:59,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-10-05 15:04:59,181 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 15:04:59,181 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 0ms
2015-10-05 15:04:59,181 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 0ms
2015-10-05 15:04:59,350 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1814279478 ms.
2015-10-05 15:04:59,352 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444049200352 with interval 21600000
2015-10-05 15:04:59,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-05 15:04:59,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-05 15:04:59,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-05 15:04:59,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2
2015-10-05 15:04:59,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-05 15:04:59,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x117a1337b48,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 43 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 15:04:59,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:05:40,622 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-05 15:05:44,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 15:05:45,248 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 15:05:45,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 15:06:52,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 15:06:52,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 15:06:52,875 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 15:06:52,937 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 15:06:52,937 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 15:06:52,942 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 15:06:52,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 15:06:52,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 15:06:52,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 15:06:52,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 15:06:52,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 15:06:53,061 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 15:06:53,069 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 15:06:53,074 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 15:06:53,079 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 15:06:53,081 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 15:06:53,081 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 15:06:53,081 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 15:06:53,091 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 59310
2015-10-05 15:06:53,091 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 15:06:53,238 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:59310
2015-10-05 15:06:53,324 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 15:06:53,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 15:06:53,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 15:06:53,376 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 15:06:53,389 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 15:06:53,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 15:06:53,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 15:06:53,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 15:06:53,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 15:06:53,470 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 15:06:53,470 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 15:06:53,882 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 16425@rushikesh1
2015-10-05 15:06:53,986 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:06:53,986 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:06:53,987 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-05 15:06:54,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-05 15:06:54,070 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-05 15:06:54,070 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-05 15:06:54,095 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-05 15:06:54,096 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:06:54,096 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 15:06:54,103 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 24576
2015-10-05 15:06:54,104 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 7ms
2015-10-05 15:06:54,104 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-10-05 15:06:54,104 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 15:06:54,104 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 1ms
2015-10-05 15:06:54,104 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-10-05 15:06:54,263 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1814164565 ms.
2015-10-05 15:06:54,264 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444043019264 with interval 21600000
2015-10-05 15:06:54,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-05 15:06:54,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-05 15:06:54,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-05 15:06:54,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3
2015-10-05 15:06:54,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-05 15:06:54,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1326249bb3e,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 47 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 15:06:54,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:15:38,464 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-05 15:15:41,142 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 15:15:41,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-05 15:16:24,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 15:16:24,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 15:16:24,699 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 15:16:24,762 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 15:16:24,762 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 15:16:24,767 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 15:16:24,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-05 15:16:24,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 15:16:24,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 15:16:24,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 15:16:24,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 15:16:24,884 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 15:16:24,892 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 15:16:24,897 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 15:16:24,902 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 15:16:24,904 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 15:16:24,904 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 15:16:24,904 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 15:16:24,914 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 47663
2015-10-05 15:16:24,914 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 15:16:25,062 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:47663
2015-10-05 15:16:25,148 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 15:16:25,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 15:16:25,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 15:16:25,188 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 15:16:25,200 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 15:16:25,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 15:16:25,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 15:16:25,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 15:16:25,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 15:16:25,278 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 15:16:25,279 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 15:16:25,643 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 18704@rushikesh1
2015-10-05 15:16:25,746 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:16:25,746 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:16:25,746 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-05 15:16:25,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-05 15:16:25,838 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-05 15:16:25,838 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-05 15:16:25,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-05 15:16:25,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:16:25,875 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 15:16:25,887 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 24576
2015-10-05 15:16:25,888 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 13ms
2015-10-05 15:16:25,888 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 15ms
2015-10-05 15:16:25,889 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 15:16:25,889 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 0ms
2015-10-05 15:16:25,889 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 0ms
2015-10-05 15:16:26,060 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1813592768 ms.
2015-10-05 15:16:26,062 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444045681062 with interval 21600000
2015-10-05 15:16:26,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-05 15:16:26,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-05 15:16:26,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-05 15:16:26,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=4
2015-10-05 15:16:26,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-05 15:16:26,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1b786826c1f,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 46 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 15:16:26,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 15:22:23,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741825_1001 src: /192.168.6.248:52766 dest: /192.168.6.248:50010
2015-10-05 15:22:29,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:52766, dest: /192.168.6.248:50010, bytes: 68143668, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1631392185_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741825_1001, duration: 6128174717
2015-10-05 15:22:29,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-05 15:26:34,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741826_1002 src: /192.168.6.248:52845 dest: /192.168.6.248:50010
2015-10-05 15:26:40,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:52845, dest: /192.168.6.248:50010, bytes: 68143668, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1076965213_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741826_1002, duration: 6091012808
2015-10-05 15:26:40,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-05 15:31:35,776 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073741826_1002 for rescanning.
2015-10-05 15:31:35,776 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-10-05 15:31:35,778 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting.
2015-10-05 15:44:13,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2015-10-05 15:44:13,276 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2015-10-05 15:44:13,291 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741825_1001 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741825
2015-10-05 15:44:13,305 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741826_1002 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741826
2015-10-05 15:47:55,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 src: /192.168.6.248:53158 dest: /192.168.6.248:50010
2015-10-05 15:48:07,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:53158, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1734310134_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003, duration: 11910456830
2015-10-05 15:48:07,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-05 15:48:07,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004 src: /192.168.6.248:53162 dest: /192.168.6.248:50010
2015-10-05 15:48:07,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:53162, dest: /192.168.6.248:50010, bytes: 4045946, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1734310134_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004, duration: 368546167
2015-10-05 15:48:07,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-05 15:48:44,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 src: /192.168.6.248:53182 dest: /192.168.6.248:50010
2015-10-05 15:48:56,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:53182, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-633820094_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005, duration: 11908878403
2015-10-05 15:48:56,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-05 15:48:56,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741830_1006 src: /192.168.6.248:53188 dest: /192.168.6.248:50010
2015-10-05 15:48:56,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:53188, dest: /192.168.6.248:50010, bytes: 4045946, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-633820094_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741830_1006, duration: 367990742
2015-10-05 15:48:56,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-05 15:50:56,457 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 for rescanning.
2015-10-05 16:19:52,272 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-05 16:19:55,460 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 16:19:55,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-06 12:27:06,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-06 12:27:06,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-06 12:27:06,793 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-06 12:27:06,856 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-06 12:27:06,856 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-06 12:27:06,861 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-06 12:27:06,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-06 12:27:06,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-06 12:27:06,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-06 12:27:06,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-06 12:27:06,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-06 12:27:07,015 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-06 12:27:07,023 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-06 12:27:07,028 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-06 12:27:07,033 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-06 12:27:07,036 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-06 12:27:07,036 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-06 12:27:07,036 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-06 12:27:07,046 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 51175
2015-10-06 12:27:07,046 INFO org.mortbay.log: jetty-6.1.26
2015-10-06 12:27:07,212 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:51175
2015-10-06 12:27:07,373 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-06 12:27:07,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-06 12:27:07,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-06 12:27:07,439 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-06 12:27:07,454 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-06 12:27:07,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-06 12:27:07,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-06 12:27:07,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-06 12:27:07,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-06 12:27:07,582 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-06 12:27:07,582 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-06 12:27:08,207 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3816@rushikesh1
2015-10-06 12:27:08,307 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-06 12:27:08,307 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-06 12:27:08,336 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-06 12:27:08,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-06 12:27:08,485 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-06 12:27:08,485 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-06 12:27:08,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-06 12:27:08,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 12:27:08,527 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 12:27:08,585 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 57ms
2015-10-06 12:27:08,585 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 58ms
2015-10-06 12:27:08,585 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 12:27:08,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 5ms
2015-10-06 12:27:08,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-10-06 12:27:08,895 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1737349933 ms.
2015-10-06 12:27:08,897 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444129154897 with interval 21600000
2015-10-06 12:27:08,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-06 12:27:08,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-06 12:27:08,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-06 12:27:09,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=72
2015-10-06 12:27:09,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-06 12:27:09,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5c1998df03,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 5 msec to generate and 79 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 12:27:09,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 14:02:36,339 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073741830_1006 for rescanning.
2015-10-06 14:02:36,340 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-10-06 14:02:36,341 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting.
2015-10-06 14:03:00,614 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 for rescanning.
2015-10-06 14:15:46,321 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073741830_1006 for rescanning.
2015-10-06 14:24:19,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6c1053c9ebe,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 14:24:19,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 14:56:22,541 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 14:56:25,486 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-06 14:56:25,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-06 14:57:50,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-06 14:57:50,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-06 14:57:51,048 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-06 14:57:51,111 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-06 14:57:51,111 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-06 14:57:51,116 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-06 14:57:51,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-06 14:57:51,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-06 14:57:51,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-06 14:57:51,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-06 14:57:51,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-06 14:57:51,238 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-06 14:57:51,246 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-06 14:57:51,251 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-06 14:57:51,256 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-06 14:57:51,258 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-06 14:57:51,258 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-06 14:57:51,259 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-06 14:57:51,269 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41732
2015-10-06 14:57:51,269 INFO org.mortbay.log: jetty-6.1.26
2015-10-06 14:57:51,414 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41732
2015-10-06 14:57:51,505 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-06 14:57:51,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-06 14:57:51,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-06 14:57:51,546 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-06 14:57:51,557 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-06 14:57:51,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-06 14:57:51,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-06 14:57:51,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-06 14:57:51,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-06 14:57:51,637 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-06 14:57:51,637 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-06 14:57:51,981 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 14284@rushikesh1
2015-10-06 14:57:52,078 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-06 14:57:52,078 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-06 14:57:52,079 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-06 14:57:52,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-06 14:57:52,162 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-06 14:57:52,162 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-06 14:57:52,189 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-06 14:57:52,189 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 14:57:52,189 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 14:57:52,199 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-10-06 14:57:52,201 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 11ms
2015-10-06 14:57:52,201 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 12ms
2015-10-06 14:57:52,201 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 14:57:52,205 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-10-06 14:57:52,205 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-10-06 14:57:52,369 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1728306459 ms.
2015-10-06 14:57:52,371 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444133381371 with interval 21600000
2015-10-06 14:57:52,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-06 14:57:52,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-06 14:57:52,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-06 14:57:52,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=89
2015-10-06 14:57:52,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-06 14:57:52,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x895b225b66a,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 61 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 14:57:52,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 16:15:42,630 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 16:15:45,982 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-06 16:15:45,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-06 16:16:25,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-06 16:16:25,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-06 16:16:25,981 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-06 16:16:26,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-06 16:16:26,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-06 16:16:26,048 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-06 16:16:26,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-06 16:16:26,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-06 16:16:26,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-06 16:16:26,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-06 16:16:26,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-06 16:16:26,170 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-06 16:16:26,178 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-06 16:16:26,183 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-06 16:16:26,188 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-06 16:16:26,190 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-06 16:16:26,191 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-06 16:16:26,191 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-06 16:16:26,201 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38911
2015-10-06 16:16:26,201 INFO org.mortbay.log: jetty-6.1.26
2015-10-06 16:16:26,353 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38911
2015-10-06 16:16:26,435 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-06 16:16:26,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-06 16:16:26,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-06 16:16:26,475 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-06 16:16:26,486 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-06 16:16:26,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-06 16:16:26,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-06 16:16:26,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-06 16:16:26,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-06 16:16:26,564 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-06 16:16:26,564 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-06 16:16:26,883 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 19767@rushikesh1
2015-10-06 16:16:26,970 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-06 16:16:26,970 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-06 16:16:26,971 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-06 16:16:27,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-06 16:16:27,046 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-06 16:16:27,046 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-06 16:16:27,074 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-06 16:16:27,074 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 16:16:27,075 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 16:16:27,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-10-06 16:16:27,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-10-06 16:16:27,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-10-06 16:16:27,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 16:16:27,085 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-10-06 16:16:27,085 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-06 16:16:27,244 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1723591584 ms.
2015-10-06 16:16:27,245 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444147442245 with interval 21600000
2015-10-06 16:16:27,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-06 16:16:27,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-06 16:16:27,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-06 16:16:27,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=94
2015-10-06 16:16:27,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-06 16:16:27,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xcdf7709defe,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 59 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 16:16:27,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 18:03:02,558 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 18:03:04,820 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-06 18:03:04,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-06 18:06:20,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-06 18:06:20,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-06 18:06:20,706 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-06 18:06:20,769 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-06 18:06:20,769 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-06 18:06:20,774 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-06 18:06:20,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-06 18:06:20,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-06 18:06:20,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-06 18:06:20,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-06 18:06:20,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-06 18:06:20,893 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-06 18:06:20,900 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-06 18:06:20,906 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-06 18:06:20,911 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-06 18:06:20,913 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-06 18:06:20,913 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-06 18:06:20,913 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-06 18:06:20,923 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 51968
2015-10-06 18:06:20,923 INFO org.mortbay.log: jetty-6.1.26
2015-10-06 18:06:21,076 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:51968
2015-10-06 18:06:21,158 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-06 18:06:21,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-06 18:06:21,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-06 18:06:21,198 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-06 18:06:21,209 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-06 18:06:21,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-06 18:06:21,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-06 18:06:21,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-06 18:06:21,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-06 18:06:21,288 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-06 18:06:21,288 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-06 18:06:21,680 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 28230@rushikesh1
2015-10-06 18:06:21,777 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-06 18:06:21,777 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-06 18:06:21,777 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-06 18:06:21,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-06 18:06:21,860 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-06 18:06:21,860 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-06 18:06:21,890 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-06 18:06:21,891 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 18:06:21,891 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 18:06:21,898 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-10-06 18:06:21,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-10-06 18:06:21,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-10-06 18:06:21,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 18:06:21,902 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-10-06 18:06:21,902 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-06 18:06:22,072 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1716996756 ms.
2015-10-06 18:06:22,074 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444145583074 with interval 21600000
2015-10-06 18:06:22,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-06 18:06:22,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-06 18:06:22,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-06 18:06:22,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=99
2015-10-06 18:06:22,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-06 18:06:22,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x12deedcb94f2,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 51 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 18:06:22,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 18:12:21,282 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 18:12:25,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 18:12:26,005 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-06 18:12:26,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-06 18:14:04,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-06 18:14:04,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-06 18:14:05,434 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-06 18:14:05,497 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-06 18:14:05,497 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-06 18:14:05,503 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-06 18:14:05,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-06 18:14:05,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-06 18:14:05,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-06 18:14:05,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-06 18:14:05,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-06 18:14:05,622 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-06 18:14:05,629 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-06 18:14:05,635 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-06 18:14:05,640 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-06 18:14:05,642 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-06 18:14:05,642 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-06 18:14:05,642 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-06 18:14:05,652 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35543
2015-10-06 18:14:05,653 INFO org.mortbay.log: jetty-6.1.26
2015-10-06 18:14:05,802 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35543
2015-10-06 18:14:05,890 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-06 18:14:05,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-06 18:14:05,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-06 18:14:05,930 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-06 18:14:05,941 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-06 18:14:05,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-06 18:14:05,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-06 18:14:06,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-06 18:14:06,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-06 18:14:06,020 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-06 18:14:06,020 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-06 18:14:06,382 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 30457@rushikesh1
2015-10-06 18:14:06,478 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-06 18:14:06,478 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-06 18:14:06,478 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-06 18:14:06,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-06 18:14:06,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-06 18:14:06,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-06 18:14:06,588 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-06 18:14:06,588 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 18:14:06,588 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 18:14:06,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-10-06 18:14:06,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 11ms
2015-10-06 18:14:06,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 11ms
2015-10-06 18:14:06,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 18:14:06,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-10-06 18:14:06,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-10-06 18:14:06,764 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1716532064 ms.
2015-10-06 18:14:06,766 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444139159766 with interval 21600000
2015-10-06 18:14:06,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-06 18:14:06,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-06 18:14:06,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-06 18:14:06,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=102
2015-10-06 18:14:06,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-06 18:14:06,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x134b21c19fe4,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 2 msec to generate and 47 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 18:14:06,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 18:15:27,015 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 18:15:31,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 18:15:31,784 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-06 18:15:31,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-07 13:29:06,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-07 13:29:06,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-07 13:29:06,934 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-07 13:29:07,002 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-07 13:29:07,002 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-07 13:29:07,008 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-07 13:29:07,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-07 13:29:07,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-07 13:29:07,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-07 13:29:07,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-07 13:29:07,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-07 13:29:07,308 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-07 13:29:07,318 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-07 13:29:07,324 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-07 13:29:07,329 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-07 13:29:07,332 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-07 13:29:07,332 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-07 13:29:07,332 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-07 13:29:07,343 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50111
2015-10-07 13:29:07,343 INFO org.mortbay.log: jetty-6.1.26
2015-10-07 13:29:07,612 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:50111
2015-10-07 13:29:08,975 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-07 13:29:08,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-07 13:29:08,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-07 13:29:09,045 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-07 13:29:09,057 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-07 13:29:09,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-07 13:29:09,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-07 13:29:09,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-07 13:29:09,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-07 13:29:09,224 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-07 13:29:09,225 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-07 13:29:09,684 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 11766@rushikesh1
2015-10-07 13:29:09,821 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-07 13:29:09,821 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-07 13:29:09,836 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-07 13:29:09,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-07 13:29:09,957 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-07 13:29:09,957 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-07 13:29:09,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-07 13:29:09,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 13:29:10,004 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 13:29:10,052 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 48ms
2015-10-07 13:29:10,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 57ms
2015-10-07 13:29:10,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 13:29:10,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-10-07 13:29:10,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-10-07 13:29:10,293 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1647228535 ms.
2015-10-07 13:29:10,295 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444215003295 with interval 21600000
2015-10-07 13:29:10,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-07 13:29:10,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-07 13:29:10,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-07 13:29:10,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=105
2015-10-07 13:29:10,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-07 13:29:10,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc5e6bee3ea,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 5 msec to generate and 78 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-07 13:29:10,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 13:29:45,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 to 192.168.6.249:50010 
2015-10-07 13:29:45,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 to 192.168.6.249:50010 
2015-10-07 13:30:08,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 (numBytes=134217728) to /192.168.6.249:50010
2015-10-07 13:30:08,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 (numBytes=134217728) to /192.168.6.249:50010
2015-10-07 13:30:09,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 to 192.168.6.249:50010 
2015-10-07 13:30:09,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 to 192.168.6.249:50010 
2015-10-07 13:30:09,200 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 for rescanning.
2015-10-07 13:30:09,201 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-10-07 13:30:09,203 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting.
2015-10-07 13:30:09,201 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0):Failed to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 to 192.168.6.249:50010 got 
java.net.SocketException: Original Exception : java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
	at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:434)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:566)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:579)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:759)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:706)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2126)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	... 9 more
2015-10-07 13:30:09,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting CheckDiskError Thread
2015-10-07 13:30:09,238 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 for rescanning.
2015-10-07 13:30:09,239 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0):Failed to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 to 192.168.6.249:50010 got 
java.net.SocketException: Original Exception : java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
	at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:434)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:566)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:579)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:759)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:706)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2126)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	... 9 more
2015-10-07 13:31:21,140 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-07 13:31:24,191 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-07 13:31:24,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-07 13:34:14,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-07 13:34:14,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-07 13:34:15,186 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-07 13:34:15,249 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-07 13:34:15,249 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-07 13:34:15,254 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-07 13:34:15,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-07 13:34:15,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-07 13:34:15,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-07 13:34:15,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-07 13:34:15,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-07 13:34:15,375 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-07 13:34:15,383 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-07 13:34:15,388 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-07 13:34:15,393 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-07 13:34:15,395 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-07 13:34:15,395 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-07 13:34:15,395 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-07 13:34:15,405 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 33339
2015-10-07 13:34:15,405 INFO org.mortbay.log: jetty-6.1.26
2015-10-07 13:34:15,555 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33339
2015-10-07 13:34:15,644 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-07 13:34:15,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-07 13:34:15,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-07 13:34:15,684 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-07 13:34:15,695 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-07 13:34:15,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-07 13:34:15,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-07 13:34:15,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-07 13:34:15,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-07 13:34:15,775 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-07 13:34:15,775 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-07 13:34:16,116 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 13797@rushikesh1
2015-10-07 13:34:16,204 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-07 13:34:16,205 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-07 13:34:16,205 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-07 13:34:16,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-07 13:34:16,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-07 13:34:16,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-07 13:34:16,312 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-07 13:34:16,313 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 13:34:16,313 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 13:34:16,320 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-10-07 13:34:16,321 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-10-07 13:34:16,321 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-10-07 13:34:16,321 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 13:34:16,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-10-07 13:34:16,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2015-10-07 13:34:16,485 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1646922343 ms.
2015-10-07 13:34:16,487 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444211447487 with interval 21600000
2015-10-07 13:34:16,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-07 13:34:16,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-07 13:34:16,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-07 13:34:16,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=106
2015-10-07 13:34:16,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-07 13:34:16,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10d2ff769da,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 5 msec to generate and 73 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-07 13:34:16,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 14:08:33,770 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-07 14:08:37,065 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-07 14:08:37,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-07 14:09:19,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-07 14:09:19,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-07 14:09:20,602 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-07 14:09:20,664 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-07 14:09:20,664 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-07 14:09:20,669 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-07 14:09:20,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-07 14:09:20,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-07 14:09:20,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-07 14:09:20,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-07 14:09:20,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-07 14:09:20,789 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-07 14:09:20,797 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-07 14:09:20,802 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-07 14:09:20,807 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-07 14:09:20,809 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-07 14:09:20,809 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-07 14:09:20,809 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-07 14:09:20,819 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 59720
2015-10-07 14:09:20,819 INFO org.mortbay.log: jetty-6.1.26
2015-10-07 14:09:20,966 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:59720
2015-10-07 14:09:21,053 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-07 14:09:21,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-07 14:09:21,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-07 14:09:21,093 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-07 14:09:21,104 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-07 14:09:21,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-07 14:09:21,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-07 14:09:21,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-07 14:09:21,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-07 14:09:21,183 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-07 14:09:21,184 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-07 14:09:21,499 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 17548@rushikesh1
2015-10-07 14:09:21,596 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-07 14:09:21,596 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-07 14:09:21,597 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-07 14:09:21,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-07 14:09:21,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-07 14:09:21,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-07 14:09:21,706 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-07 14:09:21,707 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 14:09:21,707 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 14:09:21,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-10-07 14:09:21,715 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 7ms
2015-10-07 14:09:21,715 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-10-07 14:09:21,715 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 14:09:21,718 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-10-07 14:09:21,718 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2015-10-07 14:09:21,876 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1644816952 ms.
2015-10-07 14:09:21,877 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444221695877 with interval 21600000
2015-10-07 14:09:21,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-07 14:09:21,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-07 14:09:21,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-07 14:09:21,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=107
2015-10-07 14:09:21,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-07 14:09:22,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2f7621aa9d9,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 5 msec to generate and 66 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-07 14:09:22,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 15:43:06,177 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-07 15:43:10,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-07 15:43:10,641 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-07 15:43:10,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-07 15:45:10,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-07 15:45:10,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-07 15:45:11,300 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-07 15:45:11,362 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-07 15:45:11,362 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-07 15:45:11,367 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-07 15:45:11,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-07 15:45:11,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-07 15:45:11,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-07 15:45:11,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-07 15:45:11,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-07 15:45:11,485 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-07 15:45:11,492 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-07 15:45:11,498 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-07 15:45:11,503 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-07 15:45:11,505 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-07 15:45:11,505 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-07 15:45:11,505 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-07 15:45:11,515 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 59341
2015-10-07 15:45:11,516 INFO org.mortbay.log: jetty-6.1.26
2015-10-07 15:45:11,667 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:59341
2015-10-07 15:45:11,750 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-07 15:45:11,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-07 15:45:11,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-07 15:45:11,789 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-07 15:45:11,800 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-07 15:45:11,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-07 15:45:11,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-07 15:45:11,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-07 15:45:11,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-07 15:45:11,881 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-07 15:45:11,881 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-07 15:45:12,238 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 24911@rushikesh1
2015-10-07 15:45:12,326 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-07 15:45:12,327 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-07 15:45:12,327 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-07 15:45:12,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-07 15:45:12,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-07 15:45:12,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-07 15:45:12,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-07 15:45:12,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 15:45:12,450 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 15:45:12,457 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-10-07 15:45:12,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-10-07 15:45:12,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-10-07 15:45:12,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 15:45:12,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-10-07 15:45:12,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-07 15:45:12,631 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1639066197 ms.
2015-10-07 15:45:12,633 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444213257633 with interval 21600000
2015-10-07 15:45:12,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-07 15:45:12,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-07 15:45:12,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-07 15:45:12,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=113
2015-10-07 15:45:12,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-07 15:45:12,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8325550fa50,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 48 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-07 15:45:12,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 15:50:57,643 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-07 17:06:56,875 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-07 17:06:59,602 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-07 17:06:59,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-10-15 10:59:00,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-15 10:59:00,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-15 10:59:01,517 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-15 10:59:01,580 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-15 10:59:01,580 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-15 10:59:01,585 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-15 10:59:01,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-10-15 10:59:01,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-15 10:59:01,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-15 10:59:01,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-15 10:59:01,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-15 10:59:01,760 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-15 10:59:01,768 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-15 10:59:01,773 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-15 10:59:01,778 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-15 10:59:01,781 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-15 10:59:01,781 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-15 10:59:01,781 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-15 10:59:01,791 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 47253
2015-10-15 10:59:01,791 INFO org.mortbay.log: jetty-6.1.26
2015-10-15 10:59:01,951 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:47253
2015-10-15 10:59:02,062 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-15 10:59:02,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-15 10:59:02,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-15 10:59:02,106 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-15 10:59:02,117 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-15 10:59:02,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-15 10:59:02,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-15 10:59:02,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-15 10:59:02,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-15 10:59:02,245 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-15 10:59:02,246 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-15 10:59:02,775 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3627@rushikesh1
2015-10-15 10:59:02,870 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-15 10:59:02,870 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-15 10:59:02,889 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-15 10:59:02,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-10-15 10:59:03,016 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-10-15 10:59:03,016 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-15 10:59:03,051 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-15 10:59:03,051 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-15 10:59:03,052 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-15 10:59:03,097 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 44ms
2015-10-15 10:59:03,097 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 45ms
2015-10-15 10:59:03,097 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-15 10:59:03,102 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-10-15 10:59:03,102 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 6ms
2015-10-15 10:59:03,404 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 965035424 ms.
2015-10-15 10:59:03,405 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444903508405 with interval 21600000
2015-10-15 10:59:03,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-15 10:59:03,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-15 10:59:03,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-15 10:59:03,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=118
2015-10-15 10:59:03,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-10-15 10:59:03,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x24cd880906,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 48 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-15 10:59:03,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-15 13:52:11,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9975f0c3b20,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-15 13:52:11,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-15 15:35:08,414 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-15 16:03:23,196 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-15 16:03:25,708 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-15 16:03:25,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-17 10:27:58,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-17 10:27:58,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-17 10:27:59,251 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-17 10:27:59,314 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-17 10:27:59,314 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-17 10:27:59,319 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-17 10:27:59,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-17 10:27:59,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-17 10:27:59,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-17 10:27:59,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-17 10:27:59,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-17 10:27:59,459 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-17 10:27:59,467 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-17 10:27:59,472 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-17 10:27:59,477 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-17 10:27:59,479 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-17 10:27:59,479 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-17 10:27:59,479 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-17 10:27:59,489 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37266
2015-11-17 10:27:59,489 INFO org.mortbay.log: jetty-6.1.26
2015-11-17 10:27:59,656 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37266
2015-11-17 10:27:59,775 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-17 10:27:59,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-17 10:27:59,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-17 10:27:59,817 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-17 10:27:59,828 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-17 10:27:59,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-17 10:27:59,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-17 10:27:59,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-17 10:27:59,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-17 10:27:59,933 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-17 10:27:59,934 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-17 10:28:00,480 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 15649@rushikesh1
2015-11-17 10:28:00,630 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-17 10:28:00,630 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-17 10:28:00,631 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-17 10:28:00,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-17 10:28:00,744 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-17 10:28:00,744 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-17 10:28:00,772 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-17 10:28:00,772 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-17 10:28:00,773 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-17 10:28:00,827 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 55ms
2015-11-17 10:28:00,827 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 56ms
2015-11-17 10:28:00,828 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-17 10:28:00,830 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-17 10:28:00,831 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-17 10:28:01,066 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data, after more than 504 hour(s)
2015-11-17 10:28:01,068 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447750485068 with interval 21600000
2015-11-17 10:28:01,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-17 10:28:01,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-17 10:28:01,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-17 10:28:01,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=129
2015-11-17 10:28:01,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-17 10:28:01,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1040a73b3506,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 4 msec to generate and 81 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-17 10:28:01,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-17 10:32:44,903 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-17 10:32:48,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 10:32:49,044 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-17 10:32:49,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-17 10:33:26,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-17 10:33:26,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-17 10:33:27,091 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-17 10:33:27,154 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-17 10:33:27,154 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-17 10:33:27,159 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-17 10:33:27,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-17 10:33:27,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-17 10:33:27,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-17 10:33:27,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-17 10:33:27,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-17 10:33:27,276 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-17 10:33:27,284 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-17 10:33:27,289 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-17 10:33:27,294 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-17 10:33:27,296 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-17 10:33:27,296 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-17 10:33:27,296 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-17 10:33:27,306 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34074
2015-11-17 10:33:27,306 INFO org.mortbay.log: jetty-6.1.26
2015-11-17 10:33:27,458 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34074
2015-11-17 10:33:27,540 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-17 10:33:27,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-17 10:33:27,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-17 10:33:27,579 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-17 10:33:27,590 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-17 10:33:27,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-17 10:33:27,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-17 10:33:27,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-17 10:33:27,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-17 10:33:27,668 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-17 10:33:27,668 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-17 10:33:27,987 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 17755@rushikesh1
2015-11-17 10:33:28,091 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-17 10:33:28,091 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-17 10:33:28,091 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-17 10:33:28,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-17 10:33:28,175 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-17 10:33:28,175 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-17 10:33:28,200 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-17 10:33:28,200 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-17 10:33:28,201 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-17 10:33:28,213 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-11-17 10:33:28,215 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 14ms
2015-11-17 10:33:28,215 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 15ms
2015-11-17 10:33:28,216 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-17 10:33:28,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-11-17 10:33:28,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-11-17 10:33:28,378 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data, after more than 504 hour(s)
2015-11-17 10:33:28,380 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447750186380 with interval 21600000
2015-11-17 10:33:28,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-17 10:33:28,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-17 10:33:28,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-17 10:33:28,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=132
2015-11-17 10:33:28,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-17 10:33:28,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x108cd25866d4,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 63 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-17 10:33:28,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-17 10:38:54,208 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): finished scanning block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-17 10:38:54,228 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1814074150 ms.
2015-11-17 12:03:54,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x157c346399b3,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 1 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-17 12:03:54,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-17 14:19:46,388 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-17 14:57:12,373 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-17 14:57:12,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-18 13:15:26,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-18 13:15:26,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-18 13:15:26,786 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-18 13:15:26,849 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-18 13:15:26,849 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-18 13:15:26,854 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-18 13:15:26,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-18 13:15:26,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-18 13:15:26,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-18 13:15:26,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-18 13:15:26,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-18 13:15:27,001 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-18 13:15:27,009 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-18 13:15:27,014 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-18 13:15:27,019 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-18 13:15:27,021 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-18 13:15:27,021 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-18 13:15:27,022 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-18 13:15:27,031 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34572
2015-11-18 13:15:27,032 INFO org.mortbay.log: jetty-6.1.26
2015-11-18 13:15:27,198 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34572
2015-11-18 13:15:27,332 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-18 13:15:27,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-18 13:15:27,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-18 13:15:27,404 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-18 13:15:27,417 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-18 13:15:27,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-18 13:15:27,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-18 13:15:27,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-18 13:15:27,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-18 13:15:27,528 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-18 13:15:27,528 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-18 13:15:28,003 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 14032@rushikesh1
2015-11-18 13:15:28,115 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-18 13:15:28,115 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-18 13:15:28,116 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-18 13:15:28,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-18 13:15:28,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-18 13:15:28,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-18 13:15:28,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-18 13:15:28,247 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 13:15:28,247 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 13:15:28,307 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 60ms
2015-11-18 13:15:28,307 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 60ms
2015-11-18 13:15:28,308 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 13:15:28,312 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-11-18 13:15:28,312 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-11-18 13:15:28,628 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1718279751 ms.
2015-11-18 13:15:28,630 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447845412630 with interval 21600000
2015-11-18 13:15:28,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-18 13:15:28,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-18 13:15:28,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-18 13:15:28,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=143
2015-11-18 13:15:28,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-18 13:15:28,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x7f0ea5a1e84,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-18 13:15:28,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 13:17:00,494 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-18 13:17:03,408 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-18 13:17:03,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-18 13:54:31,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-18 13:54:31,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-18 13:54:31,779 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-18 13:54:31,841 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-18 13:54:31,841 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-18 13:54:31,846 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-18 13:54:31,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-18 13:54:31,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-18 13:54:31,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-18 13:54:31,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-18 13:54:31,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-18 13:54:31,967 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-18 13:54:31,974 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-18 13:54:31,980 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-18 13:54:31,984 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-18 13:54:31,987 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-18 13:54:31,987 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-18 13:54:31,987 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-18 13:54:31,997 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 54538
2015-11-18 13:54:31,997 INFO org.mortbay.log: jetty-6.1.26
2015-11-18 13:54:32,149 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54538
2015-11-18 13:54:32,230 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-18 13:54:32,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-18 13:54:32,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-18 13:54:32,270 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-18 13:54:32,281 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-18 13:54:32,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-18 13:54:32,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-18 13:54:32,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-18 13:54:32,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-18 13:54:32,359 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-18 13:54:32,359 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-18 13:54:32,713 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 17642@rushikesh1
2015-11-18 13:54:32,818 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-18 13:54:32,818 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-18 13:54:32,818 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-18 13:54:32,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-18 13:54:32,902 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-18 13:54:32,902 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-18 13:54:32,928 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-18 13:54:32,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 13:54:32,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 13:54:32,944 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 15ms
2015-11-18 13:54:32,944 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 15ms
2015-11-18 13:54:32,945 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 13:54:32,949 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-11-18 13:54:32,949 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-11-18 13:54:33,110 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1715935268 ms.
2015-11-18 13:54:33,112 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447852033112 with interval 21600000
2015-11-18 13:54:33,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-18 13:54:33,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-18 13:54:33,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-18 13:54:33,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=146
2015-11-18 13:54:33,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-18 13:54:33,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa12c5c4b2be,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 63 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-18 13:54:33,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 14:31:08,354 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-18 14:31:10,801 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-18 14:31:10,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-18 14:32:06,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-18 14:32:06,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-18 14:32:07,075 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-18 14:32:07,138 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-18 14:32:07,138 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-18 14:32:07,143 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-18 14:32:07,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-18 14:32:07,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-18 14:32:07,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-18 14:32:07,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-18 14:32:07,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-18 14:32:07,261 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-18 14:32:07,268 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-18 14:32:07,274 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-18 14:32:07,279 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-18 14:32:07,281 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-18 14:32:07,281 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-18 14:32:07,281 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-18 14:32:07,291 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 60281
2015-11-18 14:32:07,291 INFO org.mortbay.log: jetty-6.1.26
2015-11-18 14:32:07,445 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:60281
2015-11-18 14:32:07,527 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-18 14:32:07,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-18 14:32:07,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-18 14:32:07,566 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-18 14:32:07,577 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-18 14:32:07,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-18 14:32:07,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-18 14:32:07,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-18 14:32:07,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-18 14:32:07,656 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-18 14:32:07,656 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-18 14:32:08,080 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 21404@rushikesh1
2015-11-18 14:32:08,202 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-18 14:32:08,203 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-18 14:32:08,203 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-18 14:32:08,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-18 14:32:08,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-18 14:32:08,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-18 14:32:08,335 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-18 14:32:08,336 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 14:32:08,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 14:32:08,349 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-11-18 14:32:08,350 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 13ms
2015-11-18 14:32:08,351 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 15ms
2015-11-18 14:32:08,351 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 14:32:08,355 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-11-18 14:32:08,355 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-11-18 14:32:08,530 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1713679848 ms.
2015-11-18 14:32:08,532 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447848061532 with interval 21600000
2015-11-18 14:32:08,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-18 14:32:08,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-18 14:32:08,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-18 14:32:08,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=149
2015-11-18 14:32:08,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-18 14:32:08,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc1fe4c9f4c4,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 4 msec to generate and 28 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-18 14:32:08,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 14:32:40,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004 to 192.168.6.238:50010 
2015-11-18 14:32:40,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 to 192.168.6.238:50010 
2015-11-18 14:32:42,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004 (numBytes=4045946) to /192.168.6.238:50010
2015-11-18 14:33:01,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 (numBytes=134217728) to /192.168.6.238:50010
2015-11-18 14:33:31,651 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-18 14:33:35,032 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-18 14:33:35,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-19 12:16:41,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-19 12:16:41,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-19 12:16:41,892 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-19 12:16:41,955 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-19 12:16:41,955 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-19 12:16:41,960 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-19 12:16:41,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-19 12:16:41,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-19 12:16:42,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-19 12:16:42,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-19 12:16:42,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-19 12:16:42,122 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-19 12:16:42,130 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-19 12:16:42,135 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-19 12:16:42,140 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-19 12:16:42,142 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-19 12:16:42,142 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-19 12:16:42,143 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-19 12:16:42,152 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45021
2015-11-19 12:16:42,153 INFO org.mortbay.log: jetty-6.1.26
2015-11-19 12:16:42,319 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45021
2015-11-19 12:16:42,428 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-19 12:16:42,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-19 12:16:42,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-19 12:16:42,486 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-19 12:16:42,497 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-19 12:16:42,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-19 12:16:42,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-19 12:16:42,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-19 12:16:42,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-19 12:16:42,609 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-19 12:16:42,610 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-19 12:16:43,080 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 4288@rushikesh1
2015-11-19 12:16:43,166 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-19 12:16:43,166 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-19 12:16:43,167 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-19 12:16:43,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-19 12:16:43,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-19 12:16:43,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-19 12:16:43,370 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-19 12:16:43,371 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 12:16:43,372 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 12:16:43,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 46ms
2015-11-19 12:16:43,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 48ms
2015-11-19 12:16:43,419 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 12:16:43,423 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-11-19 12:16:43,423 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-11-19 12:16:43,683 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1635404695 ms.
2015-11-19 12:16:43,685 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447925857685 with interval 21600000
2015-11-19 12:16:43,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 12:16:43,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 12:16:43,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-19 12:16:43,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=152
2015-11-19 12:16:43,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-19 12:16:43,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x94658d60c4,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 4 msec to generate and 67 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 12:16:43,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 12:22:42,575 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-19 12:22:46,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 12:22:47,200 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-19 12:22:47,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-19 12:27:56,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-19 12:27:56,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-19 12:27:56,716 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-19 12:27:56,778 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-19 12:27:56,778 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-19 12:27:56,783 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-19 12:27:56,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-19 12:27:56,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-19 12:27:56,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-19 12:27:56,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-19 12:27:56,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-19 12:27:56,901 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-19 12:27:56,909 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-19 12:27:56,914 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-19 12:27:56,919 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-19 12:27:56,921 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-19 12:27:56,921 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-19 12:27:56,921 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-19 12:27:56,931 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35931
2015-11-19 12:27:56,931 INFO org.mortbay.log: jetty-6.1.26
2015-11-19 12:27:57,082 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35931
2015-11-19 12:27:57,164 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-19 12:27:57,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-19 12:27:57,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-19 12:27:57,203 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-19 12:27:57,214 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-19 12:27:57,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-19 12:27:57,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-19 12:27:57,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-19 12:27:57,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-19 12:27:57,293 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-19 12:27:57,293 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-19 12:27:57,687 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 6564@rushikesh1
2015-11-19 12:27:57,782 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-19 12:27:57,783 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-19 12:27:57,783 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-19 12:27:57,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-19 12:27:57,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-19 12:27:57,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-19 12:27:57,891 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-19 12:27:57,891 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 12:27:57,892 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 12:27:57,903 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-11-19 12:27:57,904 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 12ms
2015-11-19 12:27:57,904 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 12ms
2015-11-19 12:27:57,905 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 12:27:57,908 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-11-19 12:27:57,908 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-11-19 12:27:58,068 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1634730310 ms.
2015-11-19 12:27:58,069 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447924111069 with interval 21600000
2015-11-19 12:27:58,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 12:27:58,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 12:27:58,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-19 12:27:58,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=153
2015-11-19 12:27:58,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-19 12:27:58,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x13165238add,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 2 msec to generate and 47 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 12:27:58,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 13:02:51,287 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-19 13:02:54,291 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-19 13:02:54,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-19 13:08:09,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-19 13:08:09,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-19 13:08:10,534 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-19 13:08:10,597 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-19 13:08:10,597 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-19 13:08:10,601 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-19 13:08:10,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-19 13:08:10,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-19 13:08:10,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-19 13:08:10,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-19 13:08:10,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-19 13:08:10,722 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-19 13:08:10,729 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-19 13:08:10,735 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-19 13:08:10,740 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-19 13:08:10,742 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-19 13:08:10,742 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-19 13:08:10,742 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-19 13:08:10,752 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42589
2015-11-19 13:08:10,752 INFO org.mortbay.log: jetty-6.1.26
2015-11-19 13:08:10,899 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42589
2015-11-19 13:08:10,989 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-19 13:08:11,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-19 13:08:11,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-19 13:08:11,028 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-19 13:08:11,040 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-19 13:08:11,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-19 13:08:11,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-19 13:08:11,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-19 13:08:11,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-19 13:08:11,119 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-19 13:08:11,120 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-19 13:08:11,334 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 10695@rushikesh1
2015-11-19 13:08:11,431 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-19 13:08:11,431 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-19 13:08:11,432 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-19 13:08:11,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-19 13:08:11,514 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-19 13:08:11,515 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-19 13:08:11,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-19 13:08:11,547 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 13:08:11,547 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 13:08:11,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-11-19 13:08:11,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-19 13:08:11,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-19 13:08:11,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 13:08:11,559 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-11-19 13:08:11,559 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-19 13:08:11,729 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1632316649 ms.
2015-11-19 13:08:11,731 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447923235731 with interval 21600000
2015-11-19 13:08:11,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 13:08:11,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 13:08:11,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-19 13:08:11,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=154
2015-11-19 13:08:11,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-19 13:08:11,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3635e4c0d22,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 20 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 13:08:11,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 13:10:59,113 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-19 13:11:03,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 13:11:03,516 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-19 13:11:03,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-19 14:09:18,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-19 14:09:18,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-19 14:09:19,300 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-19 14:09:19,364 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-19 14:09:19,365 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-19 14:09:19,370 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-19 14:09:19,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-19 14:09:19,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-19 14:09:19,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-19 14:09:19,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-19 14:09:19,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-19 14:09:19,490 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-19 14:09:19,498 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-19 14:09:19,503 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-19 14:09:19,508 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-19 14:09:19,510 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-19 14:09:19,510 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-19 14:09:19,510 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-19 14:09:19,520 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 48912
2015-11-19 14:09:19,520 INFO org.mortbay.log: jetty-6.1.26
2015-11-19 14:09:19,667 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:48912
2015-11-19 14:09:19,754 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-19 14:09:19,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-19 14:09:19,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-19 14:09:19,794 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-19 14:09:19,806 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-19 14:09:19,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-19 14:09:19,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-19 14:09:19,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-19 14:09:19,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-19 14:09:19,888 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-19 14:09:19,888 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-19 14:09:20,216 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 15122@rushikesh1
2015-11-19 14:09:20,312 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-19 14:09:20,312 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-19 14:09:20,313 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-19 14:09:20,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-19 14:09:20,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-19 14:09:20,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-19 14:09:20,422 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-19 14:09:20,422 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 14:09:20,423 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 14:09:20,432 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 9ms
2015-11-19 14:09:20,432 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 10ms
2015-11-19 14:09:20,433 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 14:09:20,435 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-11-19 14:09:20,435 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2015-11-19 14:09:20,597 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1628647781 ms.
2015-11-19 14:09:20,599 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447924769599 with interval 21600000
2015-11-19 14:09:20,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 14:09:20,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 14:09:20,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-19 14:09:20,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=157
2015-11-19 14:09:20,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-19 14:09:20,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6b99ae907e9,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 54 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 14:09:20,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 14:49:29,608 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-19 16:20:49,881 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-19 16:20:52,676 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-19 16:20:52,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-19 16:21:38,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-19 16:21:38,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-19 16:21:38,659 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-19 16:21:38,722 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-19 16:21:38,722 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-19 16:21:38,727 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-19 16:21:38,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-19 16:21:38,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-19 16:21:38,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-19 16:21:38,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-19 16:21:38,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-19 16:21:38,848 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-19 16:21:38,856 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-19 16:21:38,862 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-19 16:21:38,866 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-19 16:21:38,869 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-19 16:21:38,869 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-19 16:21:38,869 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-19 16:21:38,879 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34832
2015-11-19 16:21:38,879 INFO org.mortbay.log: jetty-6.1.26
2015-11-19 16:21:39,027 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34832
2015-11-19 16:21:39,114 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-19 16:21:39,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-19 16:21:39,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-19 16:21:39,154 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-19 16:21:39,165 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-19 16:21:39,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-19 16:21:39,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-19 16:21:39,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-19 16:21:39,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-19 16:21:39,245 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-19 16:21:39,246 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-19 16:21:39,565 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 15205@rushikesh1
2015-11-19 16:21:39,661 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-19 16:21:39,661 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-19 16:21:39,662 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-19 16:21:39,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-19 16:21:39,764 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-19 16:21:39,764 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-19 16:21:39,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-19 16:21:39,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 16:21:39,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 16:21:39,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-11-19 16:21:39,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 14ms
2015-11-19 16:21:39,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 14ms
2015-11-19 16:21:39,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 16:21:39,808 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-11-19 16:21:39,809 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-11-19 16:21:39,972 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1620708406 ms.
2015-11-19 16:21:39,974 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447941231974 with interval 21600000
2015-11-19 16:21:39,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 16:21:40,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 16:21:40,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-19 16:21:40,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=164
2015-11-19 16:21:40,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-19 16:21:40,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xdf223d1fdd9,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 55 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 16:21:40,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 17:28:09,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1192ef2268c9,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 17:28:09,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 17:53:09,239 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-19 17:53:13,003 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-19 17:53:13,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-19 18:16:31,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-19 18:16:31,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-19 18:16:32,022 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-19 18:16:32,086 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-19 18:16:32,087 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-19 18:16:32,092 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-19 18:16:32,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-19 18:16:32,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-19 18:16:32,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-19 18:16:32,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-19 18:16:32,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-19 18:16:32,212 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-19 18:16:32,219 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-19 18:16:32,225 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-19 18:16:32,230 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-19 18:16:32,232 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-19 18:16:32,232 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-19 18:16:32,232 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-19 18:16:32,242 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40285
2015-11-19 18:16:32,242 INFO org.mortbay.log: jetty-6.1.26
2015-11-19 18:16:32,389 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40285
2015-11-19 18:16:32,477 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-19 18:16:32,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-19 18:16:32,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-19 18:16:32,517 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-19 18:16:32,528 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-19 18:16:32,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-19 18:16:32,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-19 18:16:32,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-19 18:16:32,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-19 18:16:32,607 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-19 18:16:32,608 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-19 18:16:32,908 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 23666@rushikesh1
2015-11-19 18:16:33,012 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-19 18:16:33,012 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-19 18:16:33,012 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-19 18:16:33,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-19 18:16:33,095 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-19 18:16:33,096 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-19 18:16:33,122 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-19 18:16:33,122 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 18:16:33,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 18:16:33,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 9ms
2015-11-19 18:16:33,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-19 18:16:33,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 18:16:33,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-11-19 18:16:33,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-19 18:16:33,295 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1613815083 ms.
2015-11-19 18:16:33,297 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447947960297 with interval 21600000
2015-11-19 18:16:33,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 18:16:33,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 18:16:33,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-19 18:16:33,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=169
2015-11-19 18:16:33,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-19 18:16:33,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x14371c711a34,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 2 msec to generate and 61 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 18:16:33,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 18:29:55,070 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-19 18:29:55,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-20 12:12:24,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-20 12:12:24,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-20 12:12:24,811 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-20 12:12:24,873 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-20 12:12:24,873 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-20 12:12:24,878 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-20 12:12:24,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-20 12:12:24,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-20 12:12:24,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-20 12:12:24,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-20 12:12:24,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-20 12:12:25,063 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-20 12:12:25,071 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-20 12:12:25,077 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-20 12:12:25,081 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-20 12:12:25,084 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-20 12:12:25,084 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-20 12:12:25,084 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-20 12:12:25,094 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45099
2015-11-20 12:12:25,094 INFO org.mortbay.log: jetty-6.1.26
2015-11-20 12:12:25,248 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45099
2015-11-20 12:12:25,418 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-20 12:12:25,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-20 12:12:25,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-20 12:12:25,487 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-20 12:12:25,505 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-20 12:12:25,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-20 12:12:25,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-20 12:12:25,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-20 12:12:25,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-20 12:12:25,641 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-20 12:12:25,641 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-20 12:12:26,189 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3505@rushikesh1
2015-11-20 12:12:26,301 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-20 12:12:26,301 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-20 12:12:26,302 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-20 12:12:26,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-20 12:12:26,423 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-20 12:12:26,423 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-20 12:12:26,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-20 12:12:26,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 12:12:26,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 12:12:26,503 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 44ms
2015-11-20 12:12:26,503 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 44ms
2015-11-20 12:12:26,503 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 12:12:26,508 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 5ms
2015-11-20 12:12:26,508 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-11-20 12:12:26,834 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1549261544 ms.
2015-11-20 12:12:26,836 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448005475836 with interval 21600000
2015-11-20 12:12:26,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-20 12:12:26,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-20 12:12:26,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-20 12:12:26,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=172
2015-11-20 12:12:26,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-20 12:12:27,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1c873f832c,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 5 msec to generate and 76 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-20 12:12:27,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 12:13:40,610 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-20 12:13:44,507 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-20 12:13:44,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-20 12:14:18,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-20 12:14:18,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-20 12:14:19,051 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-20 12:14:19,114 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-20 12:14:19,114 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-20 12:14:19,119 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-20 12:14:19,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-20 12:14:19,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-20 12:14:19,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-20 12:14:19,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-20 12:14:19,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-20 12:14:19,241 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-20 12:14:19,248 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-20 12:14:19,254 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-20 12:14:19,259 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-20 12:14:19,261 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-20 12:14:19,261 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-20 12:14:19,261 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-20 12:14:19,271 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 54056
2015-11-20 12:14:19,271 INFO org.mortbay.log: jetty-6.1.26
2015-11-20 12:14:19,418 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54056
2015-11-20 12:14:19,506 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-20 12:14:19,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-20 12:14:19,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-20 12:14:19,546 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-20 12:14:19,557 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-20 12:14:19,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-20 12:14:19,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-20 12:14:19,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-20 12:14:19,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-20 12:14:19,638 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-20 12:14:19,638 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-20 12:14:19,971 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 5400@rushikesh1
2015-11-20 12:14:20,060 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-20 12:14:20,060 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-20 12:14:20,060 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-20 12:14:20,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-20 12:14:20,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-20 12:14:20,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-20 12:14:20,174 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-20 12:14:20,174 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 12:14:20,175 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 12:14:20,182 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-11-20 12:14:20,183 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-20 12:14:20,183 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-20 12:14:20,183 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 12:14:20,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-20 12:14:20,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-20 12:14:20,356 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1549148022 ms.
2015-11-20 12:14:20,358 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448020647358 with interval 21600000
2015-11-20 12:14:20,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-20 12:14:20,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-20 12:14:20,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-20 12:14:20,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=173
2015-11-20 12:14:20,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-20 12:14:20,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x36f2ec32aa,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 48 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-20 12:14:20,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 12:27:41,226 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 for rescanning.
2015-11-20 12:27:41,227 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-11-20 12:27:41,229 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting.
2015-11-20 12:30:47,284 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 for rescanning, because we rescanned it recently.
2015-11-20 12:35:40,632 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-20 12:35:43,514 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-20 12:35:43,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-20 12:56:51,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-20 12:56:51,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-20 12:56:51,824 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-20 12:56:51,891 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-20 12:56:51,891 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-20 12:56:51,896 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-20 12:56:51,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-20 12:56:51,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-20 12:56:51,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-20 12:56:51,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-20 12:56:51,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-20 12:56:52,027 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-20 12:56:52,035 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-20 12:56:52,043 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-20 12:56:52,049 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-20 12:56:52,051 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-20 12:56:52,051 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-20 12:56:52,052 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-20 12:56:52,062 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39176
2015-11-20 12:56:52,062 INFO org.mortbay.log: jetty-6.1.26
2015-11-20 12:56:52,246 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39176
2015-11-20 12:56:52,332 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-20 12:56:52,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-20 12:56:52,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-20 12:56:52,372 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-20 12:56:52,383 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-20 12:56:52,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-20 12:56:52,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-20 12:56:52,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-20 12:56:52,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-20 12:56:52,463 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-20 12:56:52,463 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-20 12:56:52,864 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 9931@rushikesh1
2015-11-20 12:56:53,008 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-20 12:56:53,008 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-20 12:56:53,009 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-20 12:56:53,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-20 12:56:53,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-20 12:56:53,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-20 12:56:53,175 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-20 12:56:53,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 12:56:53,177 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 12:56:53,192 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 15ms
2015-11-20 12:56:53,192 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 16ms
2015-11-20 12:56:53,192 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 12:56:53,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-20 12:56:53,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-11-20 12:56:53,374 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1546595004 ms.
2015-11-20 12:56:53,376 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448007293376 with interval 21600000
2015-11-20 12:56:53,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-20 12:56:53,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-20 12:56:53,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-20 12:56:53,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=184
2015-11-20 12:56:53,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-20 12:56:53,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2895b42dfbd,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 2 msec to generate and 19 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-20 12:56:53,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 13:30:16,457 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-20 13:30:20,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-20 13:30:21,008 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-20 13:30:21,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-20 13:31:04,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-20 13:31:04,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-20 13:31:04,700 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-20 13:31:04,762 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-20 13:31:04,762 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-20 13:31:04,768 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-20 13:31:04,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-20 13:31:04,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-20 13:31:04,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-20 13:31:04,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-20 13:31:04,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-20 13:31:04,885 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-20 13:31:04,892 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-20 13:31:04,897 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-20 13:31:04,902 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-20 13:31:04,905 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-20 13:31:04,905 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-20 13:31:04,905 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-20 13:31:04,915 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35712
2015-11-20 13:31:04,915 INFO org.mortbay.log: jetty-6.1.26
2015-11-20 13:31:05,061 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35712
2015-11-20 13:31:05,147 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-20 13:31:05,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-20 13:31:05,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-20 13:31:05,186 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-20 13:31:05,197 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-20 13:31:05,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-20 13:31:05,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-20 13:31:05,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-20 13:31:05,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-20 13:31:05,275 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-20 13:31:05,276 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-20 13:31:05,616 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 22123@rushikesh1
2015-11-20 13:31:05,705 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-20 13:31:05,705 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-20 13:31:05,706 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-20 13:31:05,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-20 13:31:05,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-20 13:31:05,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-20 13:31:05,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-20 13:31:05,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 13:31:05,822 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 13:31:05,834 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278740992
2015-11-20 13:31:05,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 14ms
2015-11-20 13:31:05,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 14ms
2015-11-20 13:31:05,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 13:31:05,840 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-11-20 13:31:05,841 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-11-20 13:31:06,000 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1544542378 ms.
2015-11-20 13:31:06,002 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448027688002 with interval 21600000
2015-11-20 13:31:06,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-20 13:31:06,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-20 13:31:06,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-20 13:31:06,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=187
2015-11-20 13:31:06,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-20 13:31:06,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x467493bdff7,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 47 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-20 13:31:06,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 14:05:17,270 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-20 14:05:21,197 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-20 14:05:21,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-20 16:29:50,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-20 16:29:50,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-20 16:29:51,258 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-20 16:29:51,322 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-20 16:29:51,322 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-20 16:29:51,327 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-20 16:29:51,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-20 16:29:51,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-20 16:29:51,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-20 16:29:51,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-20 16:29:51,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-20 16:29:51,499 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-20 16:29:51,507 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-20 16:29:51,512 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-20 16:29:51,517 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-20 16:29:51,519 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-20 16:29:51,519 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-20 16:29:51,519 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-20 16:29:51,529 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43499
2015-11-20 16:29:51,529 INFO org.mortbay.log: jetty-6.1.26
2015-11-20 16:29:51,684 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43499
2015-11-20 16:29:51,829 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-20 16:29:51,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-20 16:29:51,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-20 16:29:51,898 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-20 16:29:51,913 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-20 16:29:51,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-20 16:29:51,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-20 16:29:51,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-20 16:29:52,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-20 16:29:52,033 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-20 16:29:52,034 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-20 16:29:52,341 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 31951@rushikesh1
2015-11-20 16:29:52,435 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-20 16:29:52,435 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-20 16:29:52,436 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-20 16:29:52,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-20 16:29:52,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-20 16:29:52,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-20 16:29:52,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-20 16:29:52,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 16:29:52,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 16:29:52,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 45ms
2015-11-20 16:29:52,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 46ms
2015-11-20 16:29:52,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 16:29:52,640 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-20 16:29:52,640 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-20 16:29:52,884 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1533815494 ms.
2015-11-20 16:29:52,886 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448029630886 with interval 21600000
2015-11-20 16:29:52,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-20 16:29:52,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-20 16:29:52,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-20 16:29:52,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=190
2015-11-20 16:29:52,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-20 16:29:52,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe28d246f239,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 18 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-20 16:29:52,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 17:19:19,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10db68c43557,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-20 17:19:19,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 18:02:37,065 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-20 18:02:37,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-23 15:24:16,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-23 15:24:16,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-23 15:24:17,563 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-23 15:24:17,625 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-23 15:24:17,625 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-23 15:24:17,630 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-23 15:24:17,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-23 15:24:17,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-23 15:24:17,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-23 15:24:17,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-23 15:24:17,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-23 15:24:17,777 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-23 15:24:17,785 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-23 15:24:17,790 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-23 15:24:17,794 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-23 15:24:17,797 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-23 15:24:17,797 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-23 15:24:17,797 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-23 15:24:17,807 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34950
2015-11-23 15:24:17,807 INFO org.mortbay.log: jetty-6.1.26
2015-11-23 15:24:17,969 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34950
2015-11-23 15:24:18,119 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-23 15:24:18,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-23 15:24:18,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-23 15:24:18,201 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-23 15:24:18,217 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-23 15:24:18,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-23 15:24:18,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-23 15:24:18,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-23 15:24:18,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-23 15:24:18,337 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-23 15:24:18,338 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-23 15:24:18,805 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 10304@rushikesh1
2015-11-23 15:24:18,958 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-23 15:24:18,958 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-23 15:24:18,958 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-23 15:24:19,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-23 15:24:19,077 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-23 15:24:19,077 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-23 15:24:19,113 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-23 15:24:19,114 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-23 15:24:19,114 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-23 15:24:19,162 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 48ms
2015-11-23 15:24:19,162 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 48ms
2015-11-23 15:24:19,163 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-23 15:24:19,166 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-23 15:24:19,166 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-11-23 15:24:19,452 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1278548926 ms.
2015-11-23 15:24:19,453 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448277391453 with interval 21600000
2015-11-23 15:24:19,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-23 15:24:19,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-23 15:24:19,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-23 15:24:19,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=196
2015-11-23 15:24:19,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-23 15:24:19,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x71aadf5b4a1,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 5 msec to generate and 68 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-23 15:24:19,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-23 15:27:48,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741831_1007 src: /192.168.6.248:32804 dest: /192.168.6.248:50010
2015-11-23 15:28:00,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32804, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741831_1007, duration: 11570004825
2015-11-23 15:28:00,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:28:00,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741832_1008 src: /192.168.6.248:32808 dest: /192.168.6.248:50010
2015-11-23 15:28:12,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32808, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741832_1008, duration: 11739614539
2015-11-23 15:28:12,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741832_1008, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:28:12,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741833_1009 src: /192.168.6.248:32820 dest: /192.168.6.248:50010
2015-11-23 15:28:20,600 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 517ms (threshold=300ms)
2015-11-23 15:28:24,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32820, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741833_1009, duration: 12248536035
2015-11-23 15:28:24,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741833_1009, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:28:24,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741834_1010 src: /192.168.6.248:32824 dest: /192.168.6.248:50010
2015-11-23 15:28:33,933 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1019ms (threshold=300ms)
2015-11-23 15:28:38,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32824, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741834_1010, duration: 13998803606
2015-11-23 15:28:38,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741834_1010, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:28:38,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741835_1011 src: /192.168.6.248:32831 dest: /192.168.6.248:50010
2015-11-23 15:28:50,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32831, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741835_1011, duration: 11518392680
2015-11-23 15:28:50,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741835_1011, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:28:50,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741836_1012 src: /192.168.6.248:32836 dest: /192.168.6.248:50010
2015-11-23 15:29:02,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32836, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741836_1012, duration: 12260842070
2015-11-23 15:29:02,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741836_1012, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:29:02,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741837_1013 src: /192.168.6.248:32842 dest: /192.168.6.248:50010
2015-11-23 15:29:06,853 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:827ms (threshold=300ms)
2015-11-23 15:29:15,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32842, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741837_1013, duration: 12757955546
2015-11-23 15:29:15,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741837_1013, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:29:15,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741838_1014 src: /192.168.6.248:32846 dest: /192.168.6.248:50010
2015-11-23 15:29:26,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32846, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741838_1014, duration: 11514933927
2015-11-23 15:29:26,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741838_1014, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:29:26,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741839_1015 src: /192.168.6.248:32851 dest: /192.168.6.248:50010
2015-11-23 15:29:29,889 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:622ms (threshold=300ms)
2015-11-23 15:29:41,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32851, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741839_1015, duration: 14234135377
2015-11-23 15:29:41,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741839_1015, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:29:41,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741840_1016 src: /192.168.6.248:32858 dest: /192.168.6.248:50010
2015-11-23 15:29:52,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32858, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741840_1016, duration: 11505968010
2015-11-23 15:29:52,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741840_1016, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:29:52,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741841_1017 src: /192.168.6.248:32862 dest: /192.168.6.248:50010
2015-11-23 15:29:55,544 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 373ms (threshold=300ms)
2015-11-23 15:30:04,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32862, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741841_1017, duration: 12032426920
2015-11-23 15:30:04,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741841_1017, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:30:05,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741842_1018 src: /192.168.6.248:32868 dest: /192.168.6.248:50010
2015-11-23 15:30:18,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32868, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741842_1018, duration: 13081177539
2015-11-23 15:30:18,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741842_1018, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:30:18,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741843_1019 src: /192.168.6.248:32874 dest: /192.168.6.248:50010
2015-11-23 15:30:25,564 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 796ms (threshold=300ms)
2015-11-23 15:30:30,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32874, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741843_1019, duration: 12348335061
2015-11-23 15:30:30,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741843_1019, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:30:30,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741844_1020 src: /192.168.6.248:32879 dest: /192.168.6.248:50010
2015-11-23 15:30:31,256 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 577ms (threshold=300ms)
2015-11-23 15:30:41,427 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:783ms (threshold=300ms)
2015-11-23 15:30:44,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32879, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741844_1020, duration: 14093830004
2015-11-23 15:30:44,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741844_1020, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:30:44,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741845_1021 src: /192.168.6.248:32885 dest: /192.168.6.248:50010
2015-11-23 15:30:56,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32885, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741845_1021, duration: 11506282712
2015-11-23 15:30:56,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741845_1021, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:30:56,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741846_1022 src: /192.168.6.248:32890 dest: /192.168.6.248:50010
2015-11-23 15:31:07,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32890, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741846_1022, duration: 11584796336
2015-11-23 15:31:07,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741846_1022, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:31:08,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741847_1023 src: /192.168.6.248:32895 dest: /192.168.6.248:50010
2015-11-23 15:31:20,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32895, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741847_1023, duration: 12476243853
2015-11-23 15:31:20,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741847_1023, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:31:20,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741848_1024 src: /192.168.6.248:32901 dest: /192.168.6.248:50010
2015-11-23 15:31:32,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32901, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741848_1024, duration: 11522257842
2015-11-23 15:31:32,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741848_1024, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:31:32,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741849_1025 src: /192.168.6.248:32908 dest: /192.168.6.248:50010
2015-11-23 15:31:41,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:598ms (threshold=300ms)
2015-11-23 15:31:46,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32908, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741849_1025, duration: 13804675780
2015-11-23 15:31:46,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741849_1025, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:31:46,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741850_1026 src: /192.168.6.248:32913 dest: /192.168.6.248:50010
2015-11-23 15:31:53,793 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 838ms (threshold=300ms)
2015-11-23 15:31:58,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32913, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741850_1026, duration: 12417294832
2015-11-23 15:31:58,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741850_1026, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:31:58,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741851_1027 src: /192.168.6.248:32918 dest: /192.168.6.248:50010
2015-11-23 15:32:10,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32918, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741851_1027, duration: 11781885043
2015-11-23 15:32:10,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741851_1027, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:32:10,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741852_1028 src: /192.168.6.248:32924 dest: /192.168.6.248:50010
2015-11-23 15:32:23,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32924, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741852_1028, duration: 12742483193
2015-11-23 15:32:23,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741852_1028, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:32:23,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741853_1029 src: /192.168.6.248:32928 dest: /192.168.6.248:50010
2015-11-23 15:32:34,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32928, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741853_1029, duration: 11506010433
2015-11-23 15:32:34,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741853_1029, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:32:34,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741854_1030 src: /192.168.6.248:32935 dest: /192.168.6.248:50010
2015-11-23 15:32:36,911 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 468ms (threshold=300ms)
2015-11-23 15:32:48,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32935, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741854_1030, duration: 13787270276
2015-11-23 15:32:48,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741854_1030, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:32:48,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741855_1031 src: /192.168.6.248:32941 dest: /192.168.6.248:50010
2015-11-23 15:33:00,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32941, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741855_1031, duration: 11505933126
2015-11-23 15:33:00,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741855_1031, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:33:00,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741856_1032 src: /192.168.6.248:32945 dest: /192.168.6.248:50010
2015-11-23 15:33:11,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32945, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741856_1032, duration: 11644600819
2015-11-23 15:33:11,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741856_1032, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:33:12,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741857_1033 src: /192.168.6.248:32957 dest: /192.168.6.248:50010
2015-11-23 15:33:25,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32957, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741857_1033, duration: 12867778726
2015-11-23 15:33:25,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741857_1033, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:33:25,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741858_1034 src: /192.168.6.248:32967 dest: /192.168.6.248:50010
2015-11-23 15:33:36,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32967, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741858_1034, duration: 11509491941
2015-11-23 15:33:36,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741858_1034, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:33:36,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741859_1035 src: /192.168.6.248:32980 dest: /192.168.6.248:50010
2015-11-23 15:33:50,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32980, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741859_1035, duration: 13508408348
2015-11-23 15:33:50,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741859_1035, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:33:50,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741860_1036 src: /192.168.6.248:32986 dest: /192.168.6.248:50010
2015-11-23 15:33:51,930 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 443ms (threshold=300ms)
2015-11-23 15:34:02,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32986, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741860_1036, duration: 12026769852
2015-11-23 15:34:02,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741860_1036, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:34:02,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741861_1037 src: /192.168.6.248:32992 dest: /192.168.6.248:50010
2015-11-23 15:34:14,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32992, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741861_1037, duration: 12179664540
2015-11-23 15:34:14,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741861_1037, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:34:14,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741862_1038 src: /192.168.6.248:32996 dest: /192.168.6.248:50010
2015-11-23 15:34:18,346 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:729ms (threshold=300ms)
2015-11-23 15:34:27,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32996, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741862_1038, duration: 12759556991
2015-11-23 15:34:27,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741862_1038, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:34:27,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741863_1039 src: /192.168.6.248:33002 dest: /192.168.6.248:50010
2015-11-23 15:34:38,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33002, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741863_1039, duration: 11505554961
2015-11-23 15:34:38,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741863_1039, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:34:38,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741864_1040 src: /192.168.6.248:33007 dest: /192.168.6.248:50010
2015-11-23 15:34:51,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33007, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741864_1040, duration: 12997820009
2015-11-23 15:34:51,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741864_1040, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:34:51,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741865_1041 src: /192.168.6.248:33012 dest: /192.168.6.248:50010
2015-11-23 15:35:03,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33012, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741865_1041, duration: 11505980802
2015-11-23 15:35:03,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741865_1041, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:35:03,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741866_1042 src: /192.168.6.248:33018 dest: /192.168.6.248:50010
2015-11-23 15:35:14,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33018, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741866_1042, duration: 11540887033
2015-11-23 15:35:14,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741866_1042, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:35:15,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741867_1043 src: /192.168.6.248:33028 dest: /192.168.6.248:50010
2015-11-23 15:35:28,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33028, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741867_1043, duration: 13017934053
2015-11-23 15:35:28,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741867_1043, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:35:28,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741868_1044 src: /192.168.6.248:33034 dest: /192.168.6.248:50010
2015-11-23 15:35:39,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33034, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741868_1044, duration: 11506683052
2015-11-23 15:35:39,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741868_1044, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:35:40,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741869_1045 src: /192.168.6.248:33040 dest: /192.168.6.248:50010
2015-11-23 15:35:42,032 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 339ms (threshold=300ms)
2015-11-23 15:35:53,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33040, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741869_1045, duration: 13899744384
2015-11-23 15:35:53,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741869_1045, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:35:53,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741870_1046 src: /192.168.6.248:33044 dest: /192.168.6.248:50010
2015-11-23 15:36:05,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33044, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741870_1046, duration: 11507164875
2015-11-23 15:36:05,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741870_1046, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:36:05,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741871_1047 src: /192.168.6.248:33050 dest: /192.168.6.248:50010
2015-11-23 15:36:17,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33050, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741871_1047, duration: 11789030172
2015-11-23 15:36:17,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741871_1047, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:36:17,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741872_1048 src: /192.168.6.248:33061 dest: /192.168.6.248:50010
2015-11-23 15:36:19,947 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 2427ms (threshold=300ms)
2015-11-23 15:36:32,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33061, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741872_1048, duration: 14952020279
2015-11-23 15:36:32,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741872_1048, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:36:32,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741873_1049 src: /192.168.6.248:33069 dest: /192.168.6.248:50010
2015-11-23 15:36:43,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33069, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741873_1049, duration: 11507344777
2015-11-23 15:36:43,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741873_1049, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:36:43,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741874_1050 src: /192.168.6.248:33073 dest: /192.168.6.248:50010
2015-11-23 15:36:53,167 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:762ms (threshold=300ms)
2015-11-23 15:36:57,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33073, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741874_1050, duration: 13973941909
2015-11-23 15:36:57,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741874_1050, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:36:57,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741875_1051 src: /192.168.6.248:33078 dest: /192.168.6.248:50010
2015-11-23 15:37:09,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33078, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741875_1051, duration: 11507145094
2015-11-23 15:37:09,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741875_1051, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:37:09,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741876_1052 src: /192.168.6.248:33083 dest: /192.168.6.248:50010
2015-11-23 15:37:19,747 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:639ms (threshold=300ms)
2015-11-23 15:37:21,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33083, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741876_1052, duration: 12253151906
2015-11-23 15:37:21,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741876_1052, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:37:21,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741877_1053 src: /192.168.6.248:33088 dest: /192.168.6.248:50010
2015-11-23 15:37:34,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33088, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741877_1053, duration: 12413803205
2015-11-23 15:37:34,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741877_1053, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:37:34,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741878_1054 src: /192.168.6.248:33095 dest: /192.168.6.248:50010
2015-11-23 15:37:41,247 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 415ms (threshold=300ms)
2015-11-23 15:37:46,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33095, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741878_1054, duration: 11998920322
2015-11-23 15:37:46,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741878_1054, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:37:46,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741879_1055 src: /192.168.6.248:33100 dest: /192.168.6.248:50010
2015-11-23 15:37:47,437 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 529ms (threshold=300ms)
2015-11-23 15:37:49,901 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:690ms (threshold=300ms)
2015-11-23 15:38:00,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33100, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741879_1055, duration: 14252094166
2015-11-23 15:38:00,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741879_1055, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:38:00,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741880_1056 src: /192.168.6.248:33105 dest: /192.168.6.248:50010
2015-11-23 15:38:11,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33105, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741880_1056, duration: 11506130284
2015-11-23 15:38:11,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741880_1056, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:38:11,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741881_1057 src: /192.168.6.248:33111 dest: /192.168.6.248:50010
2015-11-23 15:38:23,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33111, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741881_1057, duration: 11665451223
2015-11-23 15:38:23,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741881_1057, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:38:23,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741882_1058 src: /192.168.6.248:33115 dest: /192.168.6.248:50010
2015-11-23 15:38:36,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33115, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741882_1058, duration: 12938338344
2015-11-23 15:38:36,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741882_1058, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:38:36,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741883_1059 src: /192.168.6.248:33122 dest: /192.168.6.248:50010
2015-11-23 15:38:48,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33122, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741883_1059, duration: 11527001241
2015-11-23 15:38:48,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741883_1059, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:38:48,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741884_1060 src: /192.168.6.248:33128 dest: /192.168.6.248:50010
2015-11-23 15:38:58,471 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:513ms (threshold=300ms)
2015-11-23 15:39:01,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33128, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741884_1060, duration: 13307890820
2015-11-23 15:39:01,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741884_1060, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:39:01,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741885_1061 src: /192.168.6.248:33134 dest: /192.168.6.248:50010
2015-11-23 15:39:08,937 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 465ms (threshold=300ms)
2015-11-23 15:39:13,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33134, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741885_1061, duration: 12033852777
2015-11-23 15:39:13,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741885_1061, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:39:13,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741886_1062 src: /192.168.6.248:33138 dest: /192.168.6.248:50010
2015-11-23 15:39:25,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33138, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741886_1062, duration: 11991754668
2015-11-23 15:39:25,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741886_1062, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:39:25,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741887_1063 src: /192.168.6.248:33143 dest: /192.168.6.248:50010
2015-11-23 15:39:30,619 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:839ms (threshold=300ms)
2015-11-23 15:39:38,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33143, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741887_1063, duration: 12785667843
2015-11-23 15:39:38,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741887_1063, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:39:38,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741888_1064 src: /192.168.6.248:33149 dest: /192.168.6.248:50010
2015-11-23 15:39:50,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33149, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741888_1064, duration: 11506646046
2015-11-23 15:39:50,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741888_1064, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:39:50,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741889_1065 src: /192.168.6.248:33154 dest: /192.168.6.248:50010
2015-11-23 15:39:52,590 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 474ms (threshold=300ms)
2015-11-23 15:40:00,744 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:832ms (threshold=300ms)
2015-11-23 15:40:04,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33154, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741889_1065, duration: 13922507069
2015-11-23 15:40:04,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741889_1065, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:40:04,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741890_1066 src: /192.168.6.248:33160 dest: /192.168.6.248:50010
2015-11-23 15:40:15,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33160, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741890_1066, duration: 11505572208
2015-11-23 15:40:15,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741890_1066, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:40:15,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741891_1067 src: /192.168.6.248:33165 dest: /192.168.6.248:50010
2015-11-23 15:40:27,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33165, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741891_1067, duration: 11677051329
2015-11-23 15:40:27,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741891_1067, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:40:27,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741892_1068 src: /192.168.6.248:33170 dest: /192.168.6.248:50010
2015-11-23 15:40:40,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33170, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741892_1068, duration: 12434889345
2015-11-23 15:40:40,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741892_1068, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:40:40,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741893_1069 src: /192.168.6.248:33176 dest: /192.168.6.248:50010
2015-11-23 15:40:51,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33176, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741893_1069, duration: 11506289110
2015-11-23 15:40:51,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741893_1069, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:40:51,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741894_1070 src: /192.168.6.248:33180 dest: /192.168.6.248:50010
2015-11-23 15:41:01,694 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1529ms (threshold=300ms)
2015-11-23 15:41:05,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33180, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741894_1070, duration: 14147195683
2015-11-23 15:41:05,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741894_1070, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:41:05,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741895_1071 src: /192.168.6.248:33186 dest: /192.168.6.248:50010
2015-11-23 15:41:17,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33186, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741895_1071, duration: 11506172172
2015-11-23 15:41:17,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741895_1071, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:41:17,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741896_1072 src: /192.168.6.248:33197 dest: /192.168.6.248:50010
2015-11-23 15:41:28,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33197, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741896_1072, duration: 11575166190
2015-11-23 15:41:28,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741896_1072, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:41:29,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741897_1073 src: /192.168.6.248:33203 dest: /192.168.6.248:50010
2015-11-23 15:41:42,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33203, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741897_1073, duration: 12748344486
2015-11-23 15:41:42,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741897_1073, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:41:42,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741898_1074 src: /192.168.6.248:33209 dest: /192.168.6.248:50010
2015-11-23 15:41:45,131 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 2447ms (threshold=300ms)
2015-11-23 15:41:56,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33209, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741898_1074, duration: 14029272724
2015-11-23 15:41:56,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741898_1074, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:41:56,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741899_1075 src: /192.168.6.248:33214 dest: /192.168.6.248:50010
2015-11-23 15:42:05,877 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:668ms (threshold=300ms)
2015-11-23 15:42:10,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33214, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741899_1075, duration: 13792146278
2015-11-23 15:42:10,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741899_1075, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:42:10,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741900_1076 src: /192.168.6.248:33220 dest: /192.168.6.248:50010
2015-11-23 15:42:21,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33220, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741900_1076, duration: 11506456812
2015-11-23 15:42:21,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741900_1076, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:42:21,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741901_1077 src: /192.168.6.248:33224 dest: /192.168.6.248:50010
2015-11-23 15:42:33,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33224, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741901_1077, duration: 11506224930
2015-11-23 15:42:33,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741901_1077, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:42:33,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741902_1078 src: /192.168.6.248:33237 dest: /192.168.6.248:50010
2015-11-23 15:42:37,992 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:722ms (threshold=300ms)
2015-11-23 15:42:46,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33237, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741902_1078, duration: 13350278459
2015-11-23 15:42:46,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741902_1078, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:42:46,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741903_1079 src: /192.168.6.248:33248 dest: /192.168.6.248:50010
2015-11-23 15:42:58,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33248, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741903_1079, duration: 11519139444
2015-11-23 15:42:58,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741903_1079, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:42:58,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741904_1080 src: /192.168.6.248:33252 dest: /192.168.6.248:50010
2015-11-23 15:43:05,605 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 481ms (threshold=300ms)
2015-11-23 15:43:12,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33252, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741904_1080, duration: 13991296022
2015-11-23 15:43:12,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741904_1080, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:43:12,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741905_1081 src: /192.168.6.248:33258 dest: /192.168.6.248:50010
2015-11-23 15:43:12,775 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 523ms (threshold=300ms)
2015-11-23 15:43:24,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33258, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741905_1081, duration: 12181814060
2015-11-23 15:43:24,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741905_1081, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:43:24,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741906_1082 src: /192.168.6.248:33262 dest: /192.168.6.248:50010
2015-11-23 15:43:35,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33262, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741906_1082, duration: 11673082337
2015-11-23 15:43:35,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741906_1082, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:43:36,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741907_1083 src: /192.168.6.248:33269 dest: /192.168.6.248:50010
2015-11-23 15:43:48,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33269, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741907_1083, duration: 12545494278
2015-11-23 15:43:48,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741907_1083, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:43:48,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741908_1084 src: /192.168.6.248:33275 dest: /192.168.6.248:50010
2015-11-23 15:44:00,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33275, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741908_1084, duration: 11507977966
2015-11-23 15:44:00,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741908_1084, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:44:00,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741909_1085 src: /192.168.6.248:33279 dest: /192.168.6.248:50010
2015-11-23 15:44:02,947 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 647ms (threshold=300ms)
2015-11-23 15:44:08,340 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:388ms (threshold=300ms)
2015-11-23 15:44:08,714 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:363ms (threshold=300ms)
2015-11-23 15:44:15,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33279, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741909_1085, duration: 14734063067
2015-11-23 15:44:15,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741909_1085, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:44:15,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741910_1086 src: /192.168.6.248:33289 dest: /192.168.6.248:50010
2015-11-23 15:44:26,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33289, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741910_1086, duration: 11506666718
2015-11-23 15:44:26,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741910_1086, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:44:26,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741911_1087 src: /192.168.6.248:33294 dest: /192.168.6.248:50010
2015-11-23 15:44:38,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33294, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741911_1087, duration: 11680909834
2015-11-23 15:44:38,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741911_1087, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:44:38,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741912_1088 src: /192.168.6.248:33300 dest: /192.168.6.248:50010
2015-11-23 15:44:51,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33300, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741912_1088, duration: 12813871607
2015-11-23 15:44:51,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741912_1088, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:44:51,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741913_1089 src: /192.168.6.248:33306 dest: /192.168.6.248:50010
2015-11-23 15:45:02,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33306, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741913_1089, duration: 11517158204
2015-11-23 15:45:02,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741913_1089, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:45:02,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741914_1090 src: /192.168.6.248:33312 dest: /192.168.6.248:50010
2015-11-23 15:45:05,925 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:377ms (threshold=300ms)
2015-11-23 15:45:11,230 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:681ms (threshold=300ms)
2015-11-23 15:45:11,557 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:316ms (threshold=300ms)
2015-11-23 15:45:16,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33312, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741914_1090, duration: 13640339206
2015-11-23 15:45:16,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741914_1090, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:45:16,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741915_1091 src: /192.168.6.248:33317 dest: /192.168.6.248:50010
2015-11-23 15:45:24,019 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 463ms (threshold=300ms)
2015-11-23 15:45:28,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33317, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741915_1091, duration: 12010835995
2015-11-23 15:45:28,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741915_1091, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:45:28,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741916_1092 src: /192.168.6.248:33322 dest: /192.168.6.248:50010
2015-11-23 15:45:30,829 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 477ms (threshold=300ms)
2015-11-23 15:45:40,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33322, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741916_1092, duration: 12063783254
2015-11-23 15:45:40,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741916_1092, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:45:40,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741917_1093 src: /192.168.6.248:33328 dest: /192.168.6.248:50010
2015-11-23 15:45:53,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33328, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741917_1093, duration: 12856175190
2015-11-23 15:45:53,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741917_1093, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:45:53,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741918_1094 src: /192.168.6.248:33332 dest: /192.168.6.248:50010
2015-11-23 15:46:05,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33332, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741918_1094, duration: 11507232192
2015-11-23 15:46:05,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741918_1094, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:46:05,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741919_1095 src: /192.168.6.248:33338 dest: /192.168.6.248:50010
2015-11-23 15:46:09,090 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 2889ms (threshold=300ms)
2015-11-23 15:46:16,872 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:718ms (threshold=300ms)
2015-11-23 15:46:21,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33338, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741919_1095, duration: 16706437000
2015-11-23 15:46:21,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741919_1095, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:46:21,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741920_1096 src: /192.168.6.248:33344 dest: /192.168.6.248:50010
2015-11-23 15:46:33,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33344, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741920_1096, duration: 11507174536
2015-11-23 15:46:33,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741920_1096, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:46:33,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741921_1097 src: /192.168.6.248:33357 dest: /192.168.6.248:50010
2015-11-23 15:46:45,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33357, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741921_1097, duration: 11711857183
2015-11-23 15:46:45,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741921_1097, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:46:45,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741922_1098 src: /192.168.6.248:33361 dest: /192.168.6.248:50010
2015-11-23 15:46:58,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33361, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741922_1098, duration: 12509048041
2015-11-23 15:46:58,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741922_1098, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:46:58,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741923_1099 src: /192.168.6.248:33366 dest: /192.168.6.248:50010
2015-11-23 15:47:03,022 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1001ms (threshold=300ms)
2015-11-23 15:47:10,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33366, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741923_1099, duration: 12647751386
2015-11-23 15:47:10,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741923_1099, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:47:10,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741924_1100 src: /192.168.6.248:33372 dest: /192.168.6.248:50010
2015-11-23 15:47:13,699 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 816ms (threshold=300ms)
2015-11-23 15:47:24,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33372, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741924_1100, duration: 13504222829
2015-11-23 15:47:24,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741924_1100, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:47:24,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741925_1101 src: /192.168.6.248:33376 dest: /192.168.6.248:50010
2015-11-23 15:47:35,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33376, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741925_1101, duration: 11530569784
2015-11-23 15:47:35,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741925_1101, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:47:35,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741926_1102 src: /192.168.6.248:33383 dest: /192.168.6.248:50010
2015-11-23 15:47:47,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33383, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741926_1102, duration: 12032038717
2015-11-23 15:47:47,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741926_1102, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:47:47,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741927_1103 src: /192.168.6.248:33388 dest: /192.168.6.248:50010
2015-11-23 15:47:51,891 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:625ms (threshold=300ms)
2015-11-23 15:48:00,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33388, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741927_1103, duration: 12752994757
2015-11-23 15:48:00,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741927_1103, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:48:00,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741928_1104 src: /192.168.6.248:33393 dest: /192.168.6.248:50010
2015-11-23 15:48:12,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33393, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741928_1104, duration: 11506801440
2015-11-23 15:48:12,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741928_1104, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:48:12,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741929_1105 src: /192.168.6.248:33399 dest: /192.168.6.248:50010
2015-11-23 15:48:20,076 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 590ms (threshold=300ms)
2015-11-23 15:48:22,139 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:888ms (threshold=300ms)
2015-11-23 15:48:26,104 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 495ms (threshold=300ms)
2015-11-23 15:48:26,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33399, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741929_1105, duration: 14620896395
2015-11-23 15:48:26,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741929_1105, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:48:26,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741930_1106 src: /192.168.6.248:33404 dest: /192.168.6.248:50010
2015-11-23 15:48:38,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33404, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741930_1106, duration: 11505979677
2015-11-23 15:48:38,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741930_1106, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:48:38,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741931_1107 src: /192.168.6.248:33410 dest: /192.168.6.248:50010
2015-11-23 15:48:50,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33410, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741931_1107, duration: 12467503990
2015-11-23 15:48:50,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741931_1107, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:48:51,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741932_1108 src: /192.168.6.248:33416 dest: /192.168.6.248:50010
2015-11-23 15:49:03,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33416, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741932_1108, duration: 12684891931
2015-11-23 15:49:03,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741932_1108, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:49:03,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741933_1109 src: /192.168.6.248:33422 dest: /192.168.6.248:50010
2015-11-23 15:49:15,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33422, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741933_1109, duration: 11508267618
2015-11-23 15:49:15,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741933_1109, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:49:15,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741934_1110 src: /192.168.6.248:33426 dest: /192.168.6.248:50010
2015-11-23 15:49:23,673 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:707ms (threshold=300ms)
2015-11-23 15:49:24,674 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:768ms (threshold=300ms)
2015-11-23 15:49:25,883 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:899ms (threshold=300ms)
2015-11-23 15:49:29,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33426, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741934_1110, duration: 14280638682
2015-11-23 15:49:29,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741934_1110, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:49:29,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741935_1111 src: /192.168.6.248:33433 dest: /192.168.6.248:50010
2015-11-23 15:49:41,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33433, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741935_1111, duration: 11580996697
2015-11-23 15:49:41,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741935_1111, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:49:41,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741936_1112 src: /192.168.6.248:33439 dest: /192.168.6.248:50010
2015-11-23 15:49:52,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33439, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741936_1112, duration: 11692444509
2015-11-23 15:49:52,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741936_1112, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:49:53,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741937_1113 src: /192.168.6.248:33443 dest: /192.168.6.248:50010
2015-11-23 15:50:05,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33443, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741937_1113, duration: 12520764328
2015-11-23 15:50:05,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741937_1113, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:50:05,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741938_1114 src: /192.168.6.248:33449 dest: /192.168.6.248:50010
2015-11-23 15:50:17,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33449, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741938_1114, duration: 11508239782
2015-11-23 15:50:17,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741938_1114, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:50:17,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741939_1115 src: /192.168.6.248:33454 dest: /192.168.6.248:50010
2015-11-23 15:50:31,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33454, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741939_1115, duration: 13751072808
2015-11-23 15:50:31,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741939_1115, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:50:31,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741940_1116 src: /192.168.6.248:33460 dest: /192.168.6.248:50010
2015-11-23 15:50:42,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33460, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741940_1116, duration: 11674368121
2015-11-23 15:50:42,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741940_1116, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:50:42,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741941_1117 src: /192.168.6.248:33466 dest: /192.168.6.248:50010
2015-11-23 15:50:44,352 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 503ms (threshold=300ms)
2015-11-23 15:50:55,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33466, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741941_1117, duration: 12317448296
2015-11-23 15:50:55,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741941_1117, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:50:55,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741942_1118 src: /192.168.6.248:33470 dest: /192.168.6.248:50010
2015-11-23 15:51:00,240 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:716ms (threshold=300ms)
2015-11-23 15:51:08,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33470, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741942_1118, duration: 12727967679
2015-11-23 15:51:08,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741942_1118, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:51:08,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741943_1119 src: /192.168.6.248:33476 dest: /192.168.6.248:50010
2015-11-23 15:51:19,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33476, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741943_1119, duration: 11514759714
2015-11-23 15:51:19,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741943_1119, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:51:19,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741944_1120 src: /192.168.6.248:33482 dest: /192.168.6.248:50010
2015-11-23 15:51:20,925 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 323ms (threshold=300ms)
2015-11-23 15:51:33,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33482, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741944_1120, duration: 13644497413
2015-11-23 15:51:33,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741944_1120, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:51:33,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741945_1121 src: /192.168.6.248:33489 dest: /192.168.6.248:50010
2015-11-23 15:51:44,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33489, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741945_1121, duration: 11508655499
2015-11-23 15:51:44,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741945_1121, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:51:44,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741946_1122 src: /192.168.6.248:33493 dest: /192.168.6.248:50010
2015-11-23 15:51:56,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33493, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741946_1122, duration: 11615226850
2015-11-23 15:51:56,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741946_1122, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:51:56,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741947_1123 src: /192.168.6.248:33498 dest: /192.168.6.248:50010
2015-11-23 15:51:59,458 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 513ms (threshold=300ms)
2015-11-23 15:52:02,211 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:397ms (threshold=300ms)
2015-11-23 15:52:10,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33498, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741947_1123, duration: 13389320359
2015-11-23 15:52:10,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741947_1123, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:52:10,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741948_1124 src: /192.168.6.248:33504 dest: /192.168.6.248:50010
2015-11-23 15:52:21,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33504, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741948_1124, duration: 11509372295
2015-11-23 15:52:21,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741948_1124, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:52:21,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741949_1125 src: /192.168.6.248:33514 dest: /192.168.6.248:50010
2015-11-23 15:52:29,871 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 487ms (threshold=300ms)
2015-11-23 15:52:35,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33514, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741949_1125, duration: 13568343518
2015-11-23 15:52:35,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741949_1125, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:52:35,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741950_1126 src: /192.168.6.248:33521 dest: /192.168.6.248:50010
2015-11-23 15:52:46,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33521, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741950_1126, duration: 11507890345
2015-11-23 15:52:46,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741950_1126, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:52:46,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741951_1127 src: /192.168.6.248:33526 dest: /192.168.6.248:50010
2015-11-23 15:52:59,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33526, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741951_1127, duration: 12176291899
2015-11-23 15:52:59,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741951_1127, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:52:59,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741952_1128 src: /192.168.6.248:33531 dest: /192.168.6.248:50010
2015-11-23 15:53:11,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33531, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741952_1128, duration: 12700192167
2015-11-23 15:53:11,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741952_1128, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:53:11,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741953_1129 src: /192.168.6.248:33537 dest: /192.168.6.248:50010
2015-11-23 15:53:23,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33537, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741953_1129, duration: 11508524534
2015-11-23 15:53:23,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741953_1129, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:53:23,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741954_1130 src: /192.168.6.248:33541 dest: /192.168.6.248:50010
2015-11-23 15:53:36,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33541, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741954_1130, duration: 13594317890
2015-11-23 15:53:36,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741954_1130, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:53:36,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741955_1131 src: /192.168.6.248:33548 dest: /192.168.6.248:50010
2015-11-23 15:53:48,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33548, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741955_1131, duration: 11510976037
2015-11-23 15:53:48,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741955_1131, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:53:48,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741956_1132 src: /192.168.6.248:33553 dest: /192.168.6.248:50010
2015-11-23 15:54:00,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33553, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741956_1132, duration: 11650587078
2015-11-23 15:54:00,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741956_1132, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:54:00,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741957_1133 src: /192.168.6.248:33558 dest: /192.168.6.248:50010
2015-11-23 15:54:13,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33558, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741957_1133, duration: 12620936359
2015-11-23 15:54:13,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741957_1133, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:54:13,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741958_1134 src: /192.168.6.248:33564 dest: /192.168.6.248:50010
2015-11-23 15:54:24,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33564, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741958_1134, duration: 11509814115
2015-11-23 15:54:24,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741958_1134, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:54:24,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741959_1135 src: /192.168.6.248:33568 dest: /192.168.6.248:50010
2015-11-23 15:54:27,432 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:304ms (threshold=300ms)
2015-11-23 15:54:32,638 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 373ms (threshold=300ms)
2015-11-23 15:54:38,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33568, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741959_1135, duration: 13499718179
2015-11-23 15:54:38,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741959_1135, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 15:54:38,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741960_1136 src: /192.168.6.248:33575 dest: /192.168.6.248:50010
2015-11-23 15:54:42,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33575, dest: /192.168.6.248:50010, bytes: 51875046, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741960_1136, duration: 4449128575
2015-11-23 15:54:42,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741960_1136, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:01:48,315 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2015-11-23 16:01:48,317 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2015-11-23 16:01:48,340 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741829_1005 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741829
2015-11-23 16:01:48,340 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741830_1006 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741830
2015-11-23 16:05:33,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741961_1137 src: /192.168.6.248:33746 dest: /192.168.6.248:50010
2015-11-23 16:05:45,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33746, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741961_1137, duration: 11541496446
2015-11-23 16:05:45,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741961_1137, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:05:45,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741962_1138 src: /192.168.6.248:33750 dest: /192.168.6.248:50010
2015-11-23 16:05:57,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33750, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741962_1138, duration: 11640945840
2015-11-23 16:05:57,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741962_1138, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:05:57,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741963_1139 src: /192.168.6.248:33756 dest: /192.168.6.248:50010
2015-11-23 16:06:05,286 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:632ms (threshold=300ms)
2015-11-23 16:06:09,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33756, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741963_1139, duration: 12407110479
2015-11-23 16:06:09,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741963_1139, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:06:09,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741964_1140 src: /192.168.6.248:33760 dest: /192.168.6.248:50010
2015-11-23 16:06:21,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33760, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741964_1140, duration: 11508390734
2015-11-23 16:06:21,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741964_1140, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:06:21,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741965_1141 src: /192.168.6.248:33766 dest: /192.168.6.248:50010
2015-11-23 16:06:32,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33766, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741965_1141, duration: 11507498469
2015-11-23 16:06:32,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741965_1141, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:06:32,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741966_1142 src: /192.168.6.248:33773 dest: /192.168.6.248:50010
2015-11-23 16:06:44,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33773, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741966_1142, duration: 11516199195
2015-11-23 16:06:44,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741966_1142, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:06:44,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741967_1143 src: /192.168.6.248:33777 dest: /192.168.6.248:50010
2015-11-23 16:06:45,948 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 482ms (threshold=300ms)
2015-11-23 16:06:56,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33777, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741967_1143, duration: 12084762636
2015-11-23 16:06:56,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741967_1143, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:06:56,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741968_1144 src: /192.168.6.248:33784 dest: /192.168.6.248:50010
2015-11-23 16:07:05,684 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:665ms (threshold=300ms)
2015-11-23 16:07:08,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33784, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741968_1144, duration: 12316960514
2015-11-23 16:07:08,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741968_1144, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:07:08,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741969_1145 src: /192.168.6.248:33788 dest: /192.168.6.248:50010
2015-11-23 16:07:16,194 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 448ms (threshold=300ms)
2015-11-23 16:07:20,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33788, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741969_1145, duration: 11996151767
2015-11-23 16:07:20,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741969_1145, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:07:20,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741970_1146 src: /192.168.6.248:33794 dest: /192.168.6.248:50010
2015-11-23 16:07:21,916 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 646ms (threshold=300ms)
2015-11-23 16:07:32,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33794, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741970_1146, duration: 12216623705
2015-11-23 16:07:32,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741970_1146, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:07:32,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741971_1147 src: /192.168.6.248:33801 dest: /192.168.6.248:50010
2015-11-23 16:07:35,991 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:635ms (threshold=300ms)
2015-11-23 16:07:45,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33801, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741971_1147, duration: 12138660084
2015-11-23 16:07:45,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741971_1147, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:07:45,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741972_1148 src: /192.168.6.248:33805 dest: /192.168.6.248:50010
2015-11-23 16:07:56,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33805, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741972_1148, duration: 11521882684
2015-11-23 16:07:56,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741972_1148, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:07:56,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741973_1149 src: /192.168.6.248:33811 dest: /192.168.6.248:50010
2015-11-23 16:08:06,093 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:674ms (threshold=300ms)
2015-11-23 16:08:09,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33811, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741973_1149, duration: 12326739664
2015-11-23 16:08:09,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741973_1149, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:08:09,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741974_1150 src: /192.168.6.248:33815 dest: /192.168.6.248:50010
2015-11-23 16:08:20,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33815, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741974_1150, duration: 11506658966
2015-11-23 16:08:20,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741974_1150, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:08:20,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741975_1151 src: /192.168.6.248:33821 dest: /192.168.6.248:50010
2015-11-23 16:08:32,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33821, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741975_1151, duration: 11773699426
2015-11-23 16:08:32,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741975_1151, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:08:32,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741976_1152 src: /192.168.6.248:33828 dest: /192.168.6.248:50010
2015-11-23 16:08:36,312 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:735ms (threshold=300ms)
2015-11-23 16:08:44,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33828, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741976_1152, duration: 12237199947
2015-11-23 16:08:44,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741976_1152, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:08:44,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741977_1153 src: /192.168.6.248:33832 dest: /192.168.6.248:50010
2015-11-23 16:08:56,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33832, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741977_1153, duration: 11506353528
2015-11-23 16:08:56,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741977_1153, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:08:56,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741978_1154 src: /192.168.6.248:33838 dest: /192.168.6.248:50010
2015-11-23 16:09:07,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33838, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741978_1154, duration: 11509153789
2015-11-23 16:09:07,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741978_1154, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:09:07,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741979_1155 src: /192.168.6.248:33842 dest: /192.168.6.248:50010
2015-11-23 16:09:19,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33842, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741979_1155, duration: 11510187713
2015-11-23 16:09:19,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741979_1155, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:09:19,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741980_1156 src: /192.168.6.248:33848 dest: /192.168.6.248:50010
2015-11-23 16:09:31,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33848, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741980_1156, duration: 12220834739
2015-11-23 16:09:31,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741980_1156, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:09:31,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741981_1157 src: /192.168.6.248:33853 dest: /192.168.6.248:50010
2015-11-23 16:09:36,481 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:714ms (threshold=300ms)
2015-11-23 16:09:44,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33853, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741981_1157, duration: 12217450201
2015-11-23 16:09:44,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741981_1157, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:09:44,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741982_1158 src: /192.168.6.248:33859 dest: /192.168.6.248:50010
2015-11-23 16:09:47,088 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 478ms (threshold=300ms)
2015-11-23 16:09:50,568 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073741852_1028 for rescanning.
2015-11-23 16:09:50,569 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-11-23 16:09:50,582 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting.
2015-11-23 16:09:56,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33859, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741982_1158, duration: 12264952073
2015-11-23 16:09:56,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741982_1158, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:09:56,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741983_1159 src: /192.168.6.248:33865 dest: /192.168.6.248:50010
2015-11-23 16:10:08,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33865, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741983_1159, duration: 12076758607
2015-11-23 16:10:08,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741983_1159, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:10:08,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741984_1160 src: /192.168.6.248:33869 dest: /192.168.6.248:50010
2015-11-23 16:10:17,115 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 459ms (threshold=300ms)
2015-11-23 16:10:21,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33869, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741984_1160, duration: 12554469672
2015-11-23 16:10:21,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741984_1160, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:10:21,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741985_1161 src: /192.168.6.248:33875 dest: /192.168.6.248:50010
2015-11-23 16:10:31,519 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:630ms (threshold=300ms)
2015-11-23 16:10:34,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33875, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741985_1161, duration: 12837657457
2015-11-23 16:10:34,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741985_1161, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:10:34,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741986_1162 src: /192.168.6.248:33882 dest: /192.168.6.248:50010
2015-11-23 16:10:45,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33882, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741986_1162, duration: 11878043538
2015-11-23 16:10:45,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741986_1162, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:10:46,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741987_1163 src: /192.168.6.248:33886 dest: /192.168.6.248:50010
2015-11-23 16:10:57,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33886, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741987_1163, duration: 11948301077
2015-11-23 16:10:57,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741987_1163, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:10:58,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741988_1164 src: /192.168.6.248:33892 dest: /192.168.6.248:50010
2015-11-23 16:11:02,558 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 847ms (threshold=300ms)
2015-11-23 16:11:06,744 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:751ms (threshold=300ms)
2015-11-23 16:11:11,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33892, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741988_1164, duration: 13593996572
2015-11-23 16:11:11,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741988_1164, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:11:11,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741989_1165 src: /192.168.6.248:33897 dest: /192.168.6.248:50010
2015-11-23 16:11:23,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33897, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741989_1165, duration: 11902367984
2015-11-23 16:11:23,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741989_1165, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:11:23,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741990_1166 src: /192.168.6.248:33908 dest: /192.168.6.248:50010
2015-11-23 16:11:35,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33908, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741990_1166, duration: 11932146228
2015-11-23 16:11:35,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741990_1166, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:11:35,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741991_1167 src: /192.168.6.248:33915 dest: /192.168.6.248:50010
2015-11-23 16:11:38,432 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:617ms (threshold=300ms)
2015-11-23 16:11:48,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33915, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741991_1167, duration: 12859473323
2015-11-23 16:11:48,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741991_1167, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:11:48,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741992_1168 src: /192.168.6.248:33920 dest: /192.168.6.248:50010
2015-11-23 16:12:00,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33920, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741992_1168, duration: 11924418780
2015-11-23 16:12:00,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741992_1168, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:12:00,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741993_1169 src: /192.168.6.248:33926 dest: /192.168.6.248:50010
2015-11-23 16:12:07,601 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:610ms (threshold=300ms)
2015-11-23 16:12:13,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33926, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741993_1169, duration: 12629458582
2015-11-23 16:12:13,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741993_1169, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:12:13,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741994_1170 src: /192.168.6.248:33931 dest: /192.168.6.248:50010
2015-11-23 16:12:17,714 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 886ms (threshold=300ms)
2015-11-23 16:12:26,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33931, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741994_1170, duration: 13219596958
2015-11-23 16:12:26,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741994_1170, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:12:26,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741995_1171 src: /192.168.6.248:33937 dest: /192.168.6.248:50010
2015-11-23 16:12:38,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33937, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741995_1171, duration: 11964470000
2015-11-23 16:12:38,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741995_1171, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:12:39,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741996_1172 src: /192.168.6.248:33942 dest: /192.168.6.248:50010
2015-11-23 16:12:51,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33942, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741996_1172, duration: 11831375654
2015-11-23 16:12:51,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741996_1172, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:12:51,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741997_1173 src: /192.168.6.248:33949 dest: /192.168.6.248:50010
2015-11-23 16:13:04,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33949, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741997_1173, duration: 12420080100
2015-11-23 16:13:04,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741997_1173, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:13:04,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741998_1174 src: /192.168.6.248:33955 dest: /192.168.6.248:50010
2015-11-23 16:13:06,524 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073741961_1137 for rescanning.
2015-11-23 16:13:16,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33955, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741998_1174, duration: 12064396055
2015-11-23 16:13:16,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741998_1174, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:13:16,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741999_1175 src: /192.168.6.248:33963 dest: /192.168.6.248:50010
2015-11-23 16:13:28,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33963, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741999_1175, duration: 12042311958
2015-11-23 16:13:28,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741999_1175, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:13:28,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742000_1176 src: /192.168.6.248:33969 dest: /192.168.6.248:50010
2015-11-23 16:13:32,673 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 741ms (threshold=300ms)
2015-11-23 16:13:41,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33969, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742000_1176, duration: 12821304120
2015-11-23 16:13:41,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742000_1176, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:13:42,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742001_1177 src: /192.168.6.248:33974 dest: /192.168.6.248:50010
2015-11-23 16:13:54,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33974, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742001_1177, duration: 11838543440
2015-11-23 16:13:54,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742001_1177, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:13:54,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742002_1178 src: /192.168.6.248:33979 dest: /192.168.6.248:50010
2015-11-23 16:14:06,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33979, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742002_1178, duration: 11914708454
2015-11-23 16:14:06,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742002_1178, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:14:06,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742003_1179 src: /192.168.6.248:33984 dest: /192.168.6.248:50010
2015-11-23 16:14:08,823 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1076ms (threshold=300ms)
2015-11-23 16:14:19,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33984, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742003_1179, duration: 13385481294
2015-11-23 16:14:19,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742003_1179, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:14:19,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742004_1180 src: /192.168.6.248:33990 dest: /192.168.6.248:50010
2015-11-23 16:14:31,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33990, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742004_1180, duration: 11930353623
2015-11-23 16:14:31,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742004_1180, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:14:31,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742005_1181 src: /192.168.6.248:33995 dest: /192.168.6.248:50010
2015-11-23 16:14:43,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33995, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742005_1181, duration: 12040540334
2015-11-23 16:14:43,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742005_1181, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:14:43,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742006_1182 src: /192.168.6.248:34001 dest: /192.168.6.248:50010
2015-11-23 16:14:48,715 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 752ms (threshold=300ms)
2015-11-23 16:14:56,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34001, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742006_1182, duration: 12994033943
2015-11-23 16:14:56,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742006_1182, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:14:56,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742007_1183 src: /192.168.6.248:34013 dest: /192.168.6.248:50010
2015-11-23 16:15:08,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34013, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742007_1183, duration: 11978276966
2015-11-23 16:15:08,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742007_1183, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:15:09,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742008_1184 src: /192.168.6.248:34017 dest: /192.168.6.248:50010
2015-11-23 16:15:21,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34017, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742008_1184, duration: 11851934068
2015-11-23 16:15:21,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742008_1184, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:15:21,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742009_1185 src: /192.168.6.248:34023 dest: /192.168.6.248:50010
2015-11-23 16:15:23,895 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 900ms (threshold=300ms)
2015-11-23 16:15:34,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34023, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742009_1185, duration: 13222334082
2015-11-23 16:15:34,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742009_1185, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:15:34,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742010_1186 src: /192.168.6.248:34030 dest: /192.168.6.248:50010
2015-11-23 16:15:46,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34030, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742010_1186, duration: 11925252032
2015-11-23 16:15:46,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742010_1186, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:15:46,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742011_1187 src: /192.168.6.248:34035 dest: /192.168.6.248:50010
2015-11-23 16:15:58,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34035, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742011_1187, duration: 12050984246
2015-11-23 16:15:58,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742011_1187, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:15:58,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742012_1188 src: /192.168.6.248:34040 dest: /192.168.6.248:50010
2015-11-23 16:16:11,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34040, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742012_1188, duration: 13149642462
2015-11-23 16:16:11,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742012_1188, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:16:12,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742013_1189 src: /192.168.6.248:34045 dest: /192.168.6.248:50010
2015-11-23 16:16:24,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34045, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742013_1189, duration: 11881323304
2015-11-23 16:16:24,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742013_1189, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:16:24,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742014_1190 src: /192.168.6.248:34050 dest: /192.168.6.248:50010
2015-11-23 16:16:36,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34050, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742014_1190, duration: 11887454604
2015-11-23 16:16:36,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742014_1190, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:16:36,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742015_1191 src: /192.168.6.248:34056 dest: /192.168.6.248:50010
2015-11-23 16:16:39,481 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1130ms (threshold=300ms)
2015-11-23 16:16:50,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34056, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742015_1191, duration: 13419893331
2015-11-23 16:16:50,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742015_1191, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:16:50,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742016_1192 src: /192.168.6.248:34062 dest: /192.168.6.248:50010
2015-11-23 16:17:02,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34062, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742016_1192, duration: 12400984562
2015-11-23 16:17:02,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742016_1192, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:17:02,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742017_1193 src: /192.168.6.248:34068 dest: /192.168.6.248:50010
2015-11-23 16:17:15,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34068, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742017_1193, duration: 12187658544
2015-11-23 16:17:15,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742017_1193, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:17:15,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742018_1194 src: /192.168.6.248:34072 dest: /192.168.6.248:50010
2015-11-23 16:17:27,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34072, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742018_1194, duration: 12170471425
2015-11-23 16:17:27,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742018_1194, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:17:27,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742019_1195 src: /192.168.6.248:34078 dest: /192.168.6.248:50010
2015-11-23 16:17:40,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34078, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742019_1195, duration: 12294547638
2015-11-23 16:17:40,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742019_1195, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:17:40,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742020_1196 src: /192.168.6.248:34084 dest: /192.168.6.248:50010
2015-11-23 16:17:53,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34084, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742020_1196, duration: 12710883548
2015-11-23 16:17:53,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742020_1196, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:17:53,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742021_1197 src: /192.168.6.248:34089 dest: /192.168.6.248:50010
2015-11-23 16:18:00,184 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:686ms (threshold=300ms)
2015-11-23 16:18:06,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34089, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742021_1197, duration: 12579400367
2015-11-23 16:18:06,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742021_1197, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:18:06,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742022_1198 src: /192.168.6.248:34102 dest: /192.168.6.248:50010
2015-11-23 16:18:14,128 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 678ms (threshold=300ms)
2015-11-23 16:18:18,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34102, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742022_1198, duration: 12846492338
2015-11-23 16:18:18,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742022_1198, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:18:18,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742023_1199 src: /192.168.6.248:34108 dest: /192.168.6.248:50010
2015-11-23 16:18:31,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34108, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742023_1199, duration: 12101320921
2015-11-23 16:18:31,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742023_1199, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:18:31,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742024_1200 src: /192.168.6.248:34113 dest: /192.168.6.248:50010
2015-11-23 16:18:40,407 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1084ms (threshold=300ms)
2015-11-23 16:18:44,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34113, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742024_1200, duration: 12944247647
2015-11-23 16:18:44,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742024_1200, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:18:44,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742025_1201 src: /192.168.6.248:34119 dest: /192.168.6.248:50010
2015-11-23 16:18:57,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34119, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742025_1201, duration: 12545857376
2015-11-23 16:18:57,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742025_1201, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:18:57,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742026_1202 src: /192.168.6.248:34125 dest: /192.168.6.248:50010
2015-11-23 16:19:09,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34125, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742026_1202, duration: 11879576076
2015-11-23 16:19:09,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742026_1202, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:19:09,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742027_1203 src: /192.168.6.248:34129 dest: /192.168.6.248:50010
2015-11-23 16:19:22,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34129, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742027_1203, duration: 12380198787
2015-11-23 16:19:22,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742027_1203, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:19:22,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742028_1204 src: /192.168.6.248:34135 dest: /192.168.6.248:50010
2015-11-23 16:19:24,780 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1133ms (threshold=300ms)
2015-11-23 16:19:27,682 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:619ms (threshold=300ms)
2015-11-23 16:19:36,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34135, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742028_1204, duration: 13732934177
2015-11-23 16:19:36,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742028_1204, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:19:36,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742029_1205 src: /192.168.6.248:34142 dest: /192.168.6.248:50010
2015-11-23 16:19:48,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34142, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742029_1205, duration: 12234917719
2015-11-23 16:19:48,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742029_1205, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:19:49,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742030_1206 src: /192.168.6.248:34148 dest: /192.168.6.248:50010
2015-11-23 16:20:01,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34148, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742030_1206, duration: 12009958737
2015-11-23 16:20:01,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742030_1206, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:20:01,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742031_1207 src: /192.168.6.248:34152 dest: /192.168.6.248:50010
2015-11-23 16:20:13,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34152, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742031_1207, duration: 12286542044
2015-11-23 16:20:13,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742031_1207, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:20:13,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742032_1208 src: /192.168.6.248:34158 dest: /192.168.6.248:50010
2015-11-23 16:20:22,750 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1011ms (threshold=300ms)
2015-11-23 16:20:27,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34158, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742032_1208, duration: 13284085700
2015-11-23 16:20:27,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742032_1208, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:20:27,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742033_1209 src: /192.168.6.248:34164 dest: /192.168.6.248:50010
2015-11-23 16:20:39,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34164, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742033_1209, duration: 12632294511
2015-11-23 16:20:39,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742033_1209, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:20:40,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742034_1210 src: /192.168.6.248:34169 dest: /192.168.6.248:50010
2015-11-23 16:20:52,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34169, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742034_1210, duration: 12040258151
2015-11-23 16:20:52,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742034_1210, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:20:52,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742035_1211 src: /192.168.6.248:34175 dest: /192.168.6.248:50010
2015-11-23 16:21:05,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34175, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742035_1211, duration: 13121067084
2015-11-23 16:21:05,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742035_1211, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:21:05,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742036_1212 src: /192.168.6.248:34181 dest: /192.168.6.248:50010
2015-11-23 16:21:18,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34181, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742036_1212, duration: 12528329868
2015-11-23 16:21:18,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742036_1212, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:21:18,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742037_1213 src: /192.168.6.248:34186 dest: /192.168.6.248:50010
2015-11-23 16:21:30,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34186, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742037_1213, duration: 12038029878
2015-11-23 16:21:30,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742037_1213, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:21:31,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742038_1214 src: /192.168.6.248:34192 dest: /192.168.6.248:50010
2015-11-23 16:21:43,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34192, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742038_1214, duration: 11912136519
2015-11-23 16:21:43,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742038_1214, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:21:43,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742039_1215 src: /192.168.6.248:34197 dest: /192.168.6.248:50010
2015-11-23 16:21:45,607 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 819ms (threshold=300ms)
2015-11-23 16:21:57,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34197, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742039_1215, duration: 13880939080
2015-11-23 16:21:57,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742039_1215, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:21:57,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742040_1216 src: /192.168.6.248:34203 dest: /192.168.6.248:50010
2015-11-23 16:22:09,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34203, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742040_1216, duration: 11914791262
2015-11-23 16:22:09,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742040_1216, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:22:09,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742041_1217 src: /192.168.6.248:34207 dest: /192.168.6.248:50010
2015-11-23 16:22:21,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34207, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742041_1217, duration: 12467316146
2015-11-23 16:22:21,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742041_1217, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:22:22,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742042_1218 src: /192.168.6.248:34213 dest: /192.168.6.248:50010
2015-11-23 16:22:37,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34213, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742042_1218, duration: 11790999433
2015-11-23 16:22:37,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742042_1218, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:22:37,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742043_1219 src: /192.168.6.248:34226 dest: /192.168.6.248:50010
2015-11-23 16:22:49,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34226, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742043_1219, duration: 12155421927
2015-11-23 16:22:49,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742043_1219, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:22:50,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742044_1220 src: /192.168.6.248:34232 dest: /192.168.6.248:50010
2015-11-23 16:23:02,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34232, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742044_1220, duration: 11879725132
2015-11-23 16:23:02,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742044_1220, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:23:02,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742045_1221 src: /192.168.6.248:34238 dest: /192.168.6.248:50010
2015-11-23 16:23:06,731 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1011ms (threshold=300ms)
2015-11-23 16:23:16,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34238, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742045_1221, duration: 13576529003
2015-11-23 16:23:16,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742045_1221, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:23:16,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742046_1222 src: /192.168.6.248:34243 dest: /192.168.6.248:50010
2015-11-23 16:23:28,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34243, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742046_1222, duration: 11943273447
2015-11-23 16:23:28,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742046_1222, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:23:28,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742047_1223 src: /192.168.6.248:34249 dest: /192.168.6.248:50010
2015-11-23 16:23:41,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34249, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742047_1223, duration: 12920603444
2015-11-23 16:23:41,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742047_1223, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:23:42,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742048_1224 src: /192.168.6.248:34254 dest: /192.168.6.248:50010
2015-11-23 16:23:56,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34254, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742048_1224, duration: 11907937987
2015-11-23 16:23:56,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742048_1224, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:23:56,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742049_1225 src: /192.168.6.248:34260 dest: /192.168.6.248:50010
2015-11-23 16:24:08,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34260, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742049_1225, duration: 12034789856
2015-11-23 16:24:08,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742049_1225, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:24:09,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742050_1226 src: /192.168.6.248:34264 dest: /192.168.6.248:50010
2015-11-23 16:24:21,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34264, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742050_1226, duration: 11815728579
2015-11-23 16:24:21,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742050_1226, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:24:21,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742051_1227 src: /192.168.6.248:34270 dest: /192.168.6.248:50010
2015-11-23 16:24:26,917 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1197ms (threshold=300ms)
2015-11-23 16:24:35,549 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:595ms (threshold=300ms)
2015-11-23 16:24:35,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34270, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742051_1227, duration: 13888840082
2015-11-23 16:24:35,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742051_1227, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:24:36,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742052_1228 src: /192.168.6.248:34277 dest: /192.168.6.248:50010
2015-11-23 16:24:48,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34277, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742052_1228, duration: 11908612092
2015-11-23 16:24:48,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742052_1228, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:24:48,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742053_1229 src: /192.168.6.248:34282 dest: /192.168.6.248:50010
2015-11-23 16:25:01,755 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1203ms (threshold=300ms)
2015-11-23 16:25:01,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34282, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742053_1229, duration: 13425027118
2015-11-23 16:25:01,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742053_1229, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:25:01,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742054_1230 src: /192.168.6.248:34287 dest: /192.168.6.248:50010
2015-11-23 16:25:13,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34287, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742054_1230, duration: 11892769508
2015-11-23 16:25:13,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742054_1230, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:25:13,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742055_1231 src: /192.168.6.248:34294 dest: /192.168.6.248:50010
2015-11-23 16:25:21,687 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 323ms (threshold=300ms)
2015-11-23 16:25:26,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34294, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742055_1231, duration: 12374991842
2015-11-23 16:25:26,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742055_1231, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:25:26,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742056_1232 src: /192.168.6.248:34300 dest: /192.168.6.248:50010
2015-11-23 16:25:39,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34300, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742056_1232, duration: 12784583499
2015-11-23 16:25:39,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742056_1232, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:25:39,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742057_1233 src: /192.168.6.248:34306 dest: /192.168.6.248:50010
2015-11-23 16:25:51,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34306, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742057_1233, duration: 12346390794
2015-11-23 16:25:51,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742057_1233, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:25:51,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742058_1234 src: /192.168.6.248:34312 dest: /192.168.6.248:50010
2015-11-23 16:26:03,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34312, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742058_1234, duration: 11916363541
2015-11-23 16:26:03,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742058_1234, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:26:03,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742059_1235 src: /192.168.6.248:34318 dest: /192.168.6.248:50010
2015-11-23 16:26:05,293 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1141ms (threshold=300ms)
2015-11-23 16:26:07,515 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 651ms (threshold=300ms)
2015-11-23 16:26:17,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34318, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742059_1235, duration: 14378730683
2015-11-23 16:26:17,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742059_1235, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:26:17,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742060_1236 src: /192.168.6.248:34323 dest: /192.168.6.248:50010
2015-11-23 16:26:30,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34323, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742060_1236, duration: 12225091253
2015-11-23 16:26:30,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742060_1236, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:26:30,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742061_1237 src: /192.168.6.248:34329 dest: /192.168.6.248:50010
2015-11-23 16:26:42,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34329, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742061_1237, duration: 12560685905
2015-11-23 16:26:42,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742061_1237, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:26:42,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742062_1238 src: /192.168.6.248:34334 dest: /192.168.6.248:50010
2015-11-23 16:26:55,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34334, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742062_1238, duration: 12379195823
2015-11-23 16:26:55,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742062_1238, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:26:55,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742063_1239 src: /192.168.6.248:34339 dest: /192.168.6.248:50010
2015-11-23 16:27:03,524 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 739ms (threshold=300ms)
2015-11-23 16:27:05,596 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:528ms (threshold=300ms)
2015-11-23 16:27:08,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34339, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742063_1239, duration: 13429626063
2015-11-23 16:27:08,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742063_1239, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:27:08,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742064_1240 src: /192.168.6.248:34344 dest: /192.168.6.248:50010
2015-11-23 16:27:20,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34344, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742064_1240, duration: 12285682985
2015-11-23 16:27:20,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742064_1240, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:27:20,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742065_1241 src: /192.168.6.248:34350 dest: /192.168.6.248:50010
2015-11-23 16:27:33,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34350, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742065_1241, duration: 12442716930
2015-11-23 16:27:33,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742065_1241, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:27:33,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742066_1242 src: /192.168.6.248:34357 dest: /192.168.6.248:50010
2015-11-23 16:27:35,650 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:565ms (threshold=300ms)
2015-11-23 16:27:46,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34357, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742066_1242, duration: 12646416761
2015-11-23 16:27:46,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742066_1242, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:27:46,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742067_1243 src: /192.168.6.248:34361 dest: /192.168.6.248:50010
2015-11-23 16:27:58,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34361, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742067_1243, duration: 12077818339
2015-11-23 16:27:58,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742067_1243, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:27:58,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742068_1244 src: /192.168.6.248:34367 dest: /192.168.6.248:50010
2015-11-23 16:28:10,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34367, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742068_1244, duration: 12391061197
2015-11-23 16:28:10,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742068_1244, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:28:10,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742069_1245 src: /192.168.6.248:34372 dest: /192.168.6.248:50010
2015-11-23 16:28:22,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34372, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742069_1245, duration: 12113196984
2015-11-23 16:28:22,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742069_1245, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:28:22,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742070_1246 src: /192.168.6.248:34377 dest: /192.168.6.248:50010
2015-11-23 16:28:35,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34377, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742070_1246, duration: 13053141543
2015-11-23 16:28:35,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742070_1246, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:28:35,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742071_1247 src: /192.168.6.248:34384 dest: /192.168.6.248:50010
2015-11-23 16:28:48,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34384, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742071_1247, duration: 12204936992
2015-11-23 16:28:48,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742071_1247, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:28:48,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742072_1248 src: /192.168.6.248:34389 dest: /192.168.6.248:50010
2015-11-23 16:29:00,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34389, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742072_1248, duration: 12255063150
2015-11-23 16:29:00,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742072_1248, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:29:00,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742073_1249 src: /192.168.6.248:34394 dest: /192.168.6.248:50010
2015-11-23 16:29:03,437 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 769ms (threshold=300ms)
2015-11-23 16:29:13,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34394, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742073_1249, duration: 13207971675
2015-11-23 16:29:13,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742073_1249, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:29:13,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742074_1250 src: /192.168.6.248:34399 dest: /192.168.6.248:50010
2015-11-23 16:29:25,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34399, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742074_1250, duration: 12242856850
2015-11-23 16:29:25,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742074_1250, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:29:25,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742075_1251 src: /192.168.6.248:34404 dest: /192.168.6.248:50010
2015-11-23 16:29:38,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34404, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742075_1251, duration: 12221328809
2015-11-23 16:29:38,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742075_1251, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:29:38,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742076_1252 src: /192.168.6.248:34410 dest: /192.168.6.248:50010
2015-11-23 16:29:41,782 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 3169ms (threshold=300ms)
2015-11-23 16:29:54,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34410, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742076_1252, duration: 16010775834
2015-11-23 16:29:54,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742076_1252, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:29:54,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742077_1253 src: /192.168.6.248:34416 dest: /192.168.6.248:50010
2015-11-23 16:30:06,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34416, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742077_1253, duration: 12231318617
2015-11-23 16:30:06,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742077_1253, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:30:06,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742078_1254 src: /192.168.6.248:34421 dest: /192.168.6.248:50010
2015-11-23 16:30:19,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34421, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742078_1254, duration: 12585610249
2015-11-23 16:30:19,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742078_1254, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:30:19,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742079_1255 src: /192.168.6.248:34427 dest: /192.168.6.248:50010
2015-11-23 16:30:21,113 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:618ms (threshold=300ms)
2015-11-23 16:30:31,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34427, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742079_1255, duration: 12487056801
2015-11-23 16:30:31,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742079_1255, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:30:31,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742080_1256 src: /192.168.6.248:34432 dest: /192.168.6.248:50010
2015-11-23 16:30:33,586 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 794ms (threshold=300ms)
2015-11-23 16:30:44,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34432, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742080_1256, duration: 13086558644
2015-11-23 16:30:44,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742080_1256, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:30:44,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742081_1257 src: /192.168.6.248:34438 dest: /192.168.6.248:50010
2015-11-23 16:30:57,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34438, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742081_1257, duration: 12357997069
2015-11-23 16:30:57,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742081_1257, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:30:57,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742082_1258 src: /192.168.6.248:34444 dest: /192.168.6.248:50010
2015-11-23 16:31:05,086 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 585ms (threshold=300ms)
2015-11-23 16:31:09,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34444, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742082_1258, duration: 12691550822
2015-11-23 16:31:09,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742082_1258, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:31:09,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742083_1259 src: /192.168.6.248:34448 dest: /192.168.6.248:50010
2015-11-23 16:31:22,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34448, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742083_1259, duration: 12306583849
2015-11-23 16:31:22,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742083_1259, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:31:22,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742084_1260 src: /192.168.6.248:34454 dest: /192.168.6.248:50010
2015-11-23 16:31:34,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34454, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742084_1260, duration: 12157012585
2015-11-23 16:31:34,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742084_1260, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:31:34,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742085_1261 src: /192.168.6.248:34461 dest: /192.168.6.248:50010
2015-11-23 16:31:46,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34461, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742085_1261, duration: 12210011016
2015-11-23 16:31:46,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742085_1261, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:31:46,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742086_1262 src: /192.168.6.248:34466 dest: /192.168.6.248:50010
2015-11-23 16:31:49,049 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1184ms (threshold=300ms)
2015-11-23 16:32:00,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34466, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742086_1262, duration: 13442244580
2015-11-23 16:32:00,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742086_1262, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:32:00,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742087_1263 src: /192.168.6.248:34471 dest: /192.168.6.248:50010
2015-11-23 16:32:12,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34471, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742087_1263, duration: 12195310644
2015-11-23 16:32:12,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742087_1263, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:32:12,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742088_1264 src: /192.168.6.248:34476 dest: /192.168.6.248:50010
2015-11-23 16:32:24,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34476, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742088_1264, duration: 12253667835
2015-11-23 16:32:24,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742088_1264, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:32:24,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742089_1265 src: /192.168.6.248:34481 dest: /192.168.6.248:50010
2015-11-23 16:32:27,326 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:656ms (threshold=300ms)
2015-11-23 16:32:28,731 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 767ms (threshold=300ms)
2015-11-23 16:32:38,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34481, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742089_1265, duration: 13477420484
2015-11-23 16:32:38,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742089_1265, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:32:38,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742090_1266 src: /192.168.6.248:34487 dest: /192.168.6.248:50010
2015-11-23 16:32:42,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34487, dest: /192.168.6.248:50010, bytes: 51875046, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742090_1266, duration: 4701408359
2015-11-23 16:32:42,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742090_1266, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:34:43,245 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-23 16:34:43,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-23 16:37:57,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-23 16:37:57,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-23 16:37:58,205 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-23 16:37:58,267 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-23 16:37:58,267 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-23 16:37:58,272 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-23 16:37:58,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-23 16:37:58,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-23 16:37:58,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-23 16:37:58,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-23 16:37:58,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-23 16:37:58,441 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-23 16:37:58,449 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-23 16:37:58,454 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-23 16:37:58,459 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-23 16:37:58,461 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-23 16:37:58,461 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-23 16:37:58,462 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-23 16:37:58,471 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45571
2015-11-23 16:37:58,471 INFO org.mortbay.log: jetty-6.1.26
2015-11-23 16:37:58,637 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45571
2015-11-23 16:37:58,784 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-23 16:37:58,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-23 16:37:58,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-23 16:37:58,863 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-23 16:37:58,881 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-23 16:37:58,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-23 16:37:58,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-23 16:37:58,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-23 16:37:59,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-23 16:37:59,007 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-23 16:37:59,008 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-23 16:37:59,516 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 2931@rushikesh1
2015-11-23 16:37:59,627 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-23 16:37:59,627 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-23 16:37:59,628 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-23 16:37:59,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-23 16:37:59,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-23 16:37:59,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-23 16:37:59,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-23 16:37:59,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-23 16:37:59,837 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-23 16:37:59,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35143749632
2015-11-23 16:37:59,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 44ms
2015-11-23 16:37:59,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 45ms
2015-11-23 16:37:59,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-23 16:37:59,937 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 55ms
2015-11-23 16:37:59,937 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 56ms
2015-11-23 16:38:00,245 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1274128133 ms.
2015-11-23 16:38:00,247 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448287511247 with interval 21600000
2015-11-23 16:38:00,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-23 16:38:00,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-23 16:38:00,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-23 16:38:00,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=994
2015-11-23 16:38:00,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-23 16:38:00,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2b10651560,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 19 msec to generate and 77 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-23 16:38:00,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-23 16:38:35,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741902_1078 to 192.168.6.237:50010 
2015-11-23 16:38:35,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741903_1079 to 192.168.6.237:50010 
2015-11-23 16:39:07,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741902_1078 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:39:07,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741899_1075 to 192.168.6.237:50010 
2015-11-23 16:39:15,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741903_1079 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:39:16,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741893_1069 to 192.168.6.237:50010 
2015-11-23 16:39:50,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741899_1075 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:39:55,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742062_1238 to 192.168.6.237:50010 
2015-11-23 16:40:01,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741893_1069 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:40:01,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742060_1236 to 192.168.6.237:50010 
2015-11-23 16:40:35,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742062_1238 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:40:36,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742060_1236 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:40:37,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742056_1232 to 192.168.6.237:50010 
2015-11-23 16:40:37,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741983_1159 to 192.168.6.237:50010 
2015-11-23 16:41:18,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742056_1232 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:41:18,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741983_1159 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:41:19,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742082_1258 to 192.168.6.237:50010 
2015-11-23 16:41:19,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742083_1259 to 192.168.6.237:50010 
2015-11-23 16:42:01,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742083_1259 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:42:01,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742082_1258 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:42:01,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741841_1017 to 192.168.6.237:50010 
2015-11-23 16:42:01,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741840_1016 to 192.168.6.237:50010 
2015-11-23 16:42:22,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742080_1256 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742080 for deletion
2015-11-23 16:42:22,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742081_1257 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742081 for deletion
2015-11-23 16:42:22,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742082_1258 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742082 for deletion
2015-11-23 16:42:22,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742083_1259 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742083 for deletion
2015-11-23 16:42:22,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742084_1260 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742084 for deletion
2015-11-23 16:42:22,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742085_1261 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742085 for deletion
2015-11-23 16:42:22,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742086_1262 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742086 for deletion
2015-11-23 16:42:22,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742087_1263 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742087 for deletion
2015-11-23 16:42:22,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742088_1264 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742088 for deletion
2015-11-23 16:42:22,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742089_1265 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742089 for deletion
2015-11-23 16:42:22,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742090_1266 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742090 for deletion
2015-11-23 16:42:22,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741961_1137 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741961 for deletion
2015-11-23 16:42:22,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741962_1138 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741962 for deletion
2015-11-23 16:42:22,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741963_1139 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741963 for deletion
2015-11-23 16:42:22,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741964_1140 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741964 for deletion
2015-11-23 16:42:22,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741965_1141 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741965 for deletion
2015-11-23 16:42:22,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741966_1142 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741966 for deletion
2015-11-23 16:42:22,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741967_1143 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741967 for deletion
2015-11-23 16:42:22,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741968_1144 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741968 for deletion
2015-11-23 16:42:22,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741969_1145 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741969 for deletion
2015-11-23 16:42:22,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741970_1146 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741970 for deletion
2015-11-23 16:42:22,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741971_1147 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741971 for deletion
2015-11-23 16:42:22,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741972_1148 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741972 for deletion
2015-11-23 16:42:22,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741973_1149 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741973 for deletion
2015-11-23 16:42:22,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741974_1150 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741974 for deletion
2015-11-23 16:42:22,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741975_1151 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741975 for deletion
2015-11-23 16:42:22,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741976_1152 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741976 for deletion
2015-11-23 16:42:22,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741977_1153 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741977 for deletion
2015-11-23 16:42:22,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741978_1154 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741978 for deletion
2015-11-23 16:42:22,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741979_1155 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741979 for deletion
2015-11-23 16:42:22,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741980_1156 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741980 for deletion
2015-11-23 16:42:22,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741981_1157 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741981 for deletion
2015-11-23 16:42:22,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741982_1158 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741982 for deletion
2015-11-23 16:42:22,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741983_1159 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741983 for deletion
2015-11-23 16:42:22,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741984_1160 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741984 for deletion
2015-11-23 16:42:22,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741985_1161 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741985 for deletion
2015-11-23 16:42:22,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741986_1162 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741986 for deletion
2015-11-23 16:42:22,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741987_1163 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741987 for deletion
2015-11-23 16:42:22,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741988_1164 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741988 for deletion
2015-11-23 16:42:22,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741989_1165 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741989 for deletion
2015-11-23 16:42:22,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741990_1166 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741990 for deletion
2015-11-23 16:42:22,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741991_1167 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741991 for deletion
2015-11-23 16:42:22,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741992_1168 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741992 for deletion
2015-11-23 16:42:22,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741993_1169 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741993 for deletion
2015-11-23 16:42:22,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741994_1170 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741994 for deletion
2015-11-23 16:42:22,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741995_1171 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741995 for deletion
2015-11-23 16:42:22,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741996_1172 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741996 for deletion
2015-11-23 16:42:22,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741997_1173 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741997 for deletion
2015-11-23 16:42:22,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741998_1174 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741998 for deletion
2015-11-23 16:42:22,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741999_1175 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741999 for deletion
2015-11-23 16:42:22,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742000_1176 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742000 for deletion
2015-11-23 16:42:22,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742001_1177 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742001 for deletion
2015-11-23 16:42:22,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742002_1178 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742002 for deletion
2015-11-23 16:42:22,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742003_1179 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742003 for deletion
2015-11-23 16:42:22,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742004_1180 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742004 for deletion
2015-11-23 16:42:22,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742005_1181 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742005 for deletion
2015-11-23 16:42:22,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742006_1182 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742006 for deletion
2015-11-23 16:42:22,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742007_1183 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742007 for deletion
2015-11-23 16:42:22,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742008_1184 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742008 for deletion
2015-11-23 16:42:22,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742009_1185 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742009 for deletion
2015-11-23 16:42:22,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742010_1186 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742010 for deletion
2015-11-23 16:42:22,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742011_1187 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742011 for deletion
2015-11-23 16:42:22,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742012_1188 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742012 for deletion
2015-11-23 16:42:22,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742013_1189 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742013 for deletion
2015-11-23 16:42:22,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742014_1190 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742014 for deletion
2015-11-23 16:42:22,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742015_1191 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742015 for deletion
2015-11-23 16:42:22,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742016_1192 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742016 for deletion
2015-11-23 16:42:22,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742017_1193 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742017 for deletion
2015-11-23 16:42:22,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742018_1194 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742018 for deletion
2015-11-23 16:42:22,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742019_1195 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742019 for deletion
2015-11-23 16:42:22,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742020_1196 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742020 for deletion
2015-11-23 16:42:22,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742021_1197 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742021 for deletion
2015-11-23 16:42:22,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742022_1198 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742022 for deletion
2015-11-23 16:42:22,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742023_1199 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742023 for deletion
2015-11-23 16:42:22,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742024_1200 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742024 for deletion
2015-11-23 16:42:22,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742025_1201 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742025 for deletion
2015-11-23 16:42:22,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742026_1202 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742026 for deletion
2015-11-23 16:42:22,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742027_1203 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742027 for deletion
2015-11-23 16:42:22,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742028_1204 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742028 for deletion
2015-11-23 16:42:22,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742029_1205 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742029 for deletion
2015-11-23 16:42:22,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742030_1206 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742030 for deletion
2015-11-23 16:42:22,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742031_1207 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742031 for deletion
2015-11-23 16:42:22,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742032_1208 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742032 for deletion
2015-11-23 16:42:22,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742033_1209 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742033 for deletion
2015-11-23 16:42:22,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742034_1210 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742034 for deletion
2015-11-23 16:42:22,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742035_1211 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742035 for deletion
2015-11-23 16:42:22,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742036_1212 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742036 for deletion
2015-11-23 16:42:22,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742037_1213 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742037 for deletion
2015-11-23 16:42:22,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742038_1214 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742038 for deletion
2015-11-23 16:42:22,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742039_1215 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742039 for deletion
2015-11-23 16:42:22,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742040_1216 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742040 for deletion
2015-11-23 16:42:22,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742041_1217 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742041 for deletion
2015-11-23 16:42:22,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742042_1218 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742042 for deletion
2015-11-23 16:42:22,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742043_1219 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742043 for deletion
2015-11-23 16:42:22,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742044_1220 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742044 for deletion
2015-11-23 16:42:22,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742045_1221 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742045 for deletion
2015-11-23 16:42:22,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742046_1222 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742046 for deletion
2015-11-23 16:42:22,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742047_1223 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742047 for deletion
2015-11-23 16:42:22,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742048_1224 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742048 for deletion
2015-11-23 16:42:22,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742049_1225 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742049 for deletion
2015-11-23 16:42:22,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742050_1226 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742050 for deletion
2015-11-23 16:42:22,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742051_1227 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742051 for deletion
2015-11-23 16:42:22,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742052_1228 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742052 for deletion
2015-11-23 16:42:22,998 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742053_1229 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742053 for deletion
2015-11-23 16:42:22,998 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742054_1230 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742054 for deletion
2015-11-23 16:42:22,998 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742055_1231 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742055 for deletion
2015-11-23 16:42:22,998 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742056_1232 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742056 for deletion
2015-11-23 16:42:22,998 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742057_1233 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742057 for deletion
2015-11-23 16:42:22,998 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742058_1234 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742058 for deletion
2015-11-23 16:42:22,998 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742059_1235 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742059 for deletion
2015-11-23 16:42:22,998 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742060_1236 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742060 for deletion
2015-11-23 16:42:22,999 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742061_1237 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742061 for deletion
2015-11-23 16:42:22,999 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742062_1238 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742062 for deletion
2015-11-23 16:42:22,999 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742063_1239 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742063 for deletion
2015-11-23 16:42:22,999 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742064_1240 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742064 for deletion
2015-11-23 16:42:22,999 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742065_1241 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742065 for deletion
2015-11-23 16:42:22,999 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742066_1242 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742066 for deletion
2015-11-23 16:42:22,999 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742067_1243 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742067 for deletion
2015-11-23 16:42:22,999 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742068_1244 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742068 for deletion
2015-11-23 16:42:22,999 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742069_1245 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742069 for deletion
2015-11-23 16:42:23,000 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742070_1246 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742070 for deletion
2015-11-23 16:42:23,000 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742071_1247 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742071 for deletion
2015-11-23 16:42:23,000 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742072_1248 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742072 for deletion
2015-11-23 16:42:23,000 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742073_1249 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742073 for deletion
2015-11-23 16:42:23,000 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742074_1250 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742074 for deletion
2015-11-23 16:42:23,000 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742075_1251 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742075 for deletion
2015-11-23 16:42:23,000 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742076_1252 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742076 for deletion
2015-11-23 16:42:23,000 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742077_1253 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742077 for deletion
2015-11-23 16:42:23,000 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742078_1254 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742078 for deletion
2015-11-23 16:42:23,001 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742079_1255 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742079 for deletion
2015-11-23 16:42:23,019 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742080_1256 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742080
2015-11-23 16:42:23,019 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742081_1257 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742081
2015-11-23 16:42:23,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742082_1258 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742082
2015-11-23 16:42:23,020 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742083_1259 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742083
2015-11-23 16:42:23,043 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742084_1260 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742084
2015-11-23 16:42:23,043 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742085_1261 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742085
2015-11-23 16:42:23,043 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742086_1262 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742086
2015-11-23 16:42:23,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742087_1263 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742087
2015-11-23 16:42:23,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742088_1264 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742088
2015-11-23 16:42:23,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742089_1265 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742089
2015-11-23 16:42:23,077 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742090_1266 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742090
2015-11-23 16:42:23,103 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741961_1137 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741961
2015-11-23 16:42:23,103 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741962_1138 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741962
2015-11-23 16:42:23,104 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741963_1139 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741963
2015-11-23 16:42:23,104 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741964_1140 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741964
2015-11-23 16:42:23,104 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741965_1141 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741965
2015-11-23 16:42:23,108 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741966_1142 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741966
2015-11-23 16:42:23,109 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741967_1143 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741967
2015-11-23 16:42:23,109 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741968_1144 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741968
2015-11-23 16:42:23,110 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741969_1145 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741969
2015-11-23 16:42:23,110 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741970_1146 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741970
2015-11-23 16:42:23,111 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741971_1147 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741971
2015-11-23 16:42:23,111 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741972_1148 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741972
2015-11-23 16:42:23,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741973_1149 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741973
2015-11-23 16:42:23,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741974_1150 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741974
2015-11-23 16:42:23,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741975_1151 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741975
2015-11-23 16:42:23,113 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741976_1152 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741976
2015-11-23 16:42:23,113 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741977_1153 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741977
2015-11-23 16:42:23,113 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741978_1154 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741978
2015-11-23 16:42:23,114 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741979_1155 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741979
2015-11-23 16:42:23,114 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741980_1156 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741980
2015-11-23 16:42:23,121 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741981_1157 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741981
2015-11-23 16:42:23,121 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741982_1158 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741982
2015-11-23 16:42:23,121 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741983_1159 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741983
2015-11-23 16:42:23,122 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741984_1160 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741984
2015-11-23 16:42:23,122 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741985_1161 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741985
2015-11-23 16:42:23,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741986_1162 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741986
2015-11-23 16:42:23,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741987_1163 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741987
2015-11-23 16:42:23,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741988_1164 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741988
2015-11-23 16:42:23,124 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741989_1165 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741989
2015-11-23 16:42:23,124 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741990_1166 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741990
2015-11-23 16:42:23,125 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741991_1167 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741991
2015-11-23 16:42:23,125 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741992_1168 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741992
2015-11-23 16:42:23,125 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741993_1169 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741993
2015-11-23 16:42:23,125 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741994_1170 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741994
2015-11-23 16:42:23,126 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741995_1171 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741995
2015-11-23 16:42:23,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741996_1172 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741996
2015-11-23 16:42:23,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741997_1173 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741997
2015-11-23 16:42:23,158 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741998_1174 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741998
2015-11-23 16:42:23,158 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741999_1175 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741999
2015-11-23 16:42:23,159 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742000_1176 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742000
2015-11-23 16:42:23,160 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742001_1177 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742001
2015-11-23 16:42:23,160 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742002_1178 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742002
2015-11-23 16:42:23,161 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742003_1179 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742003
2015-11-23 16:42:23,183 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742004_1180 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742004
2015-11-23 16:42:23,183 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742005_1181 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742005
2015-11-23 16:42:23,184 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742006_1182 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742006
2015-11-23 16:42:23,184 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742007_1183 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742007
2015-11-23 16:42:23,184 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742008_1184 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742008
2015-11-23 16:42:23,185 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742009_1185 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742009
2015-11-23 16:42:23,185 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742010_1186 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742010
2015-11-23 16:42:23,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742011_1187 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742011
2015-11-23 16:42:23,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742012_1188 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742012
2015-11-23 16:42:23,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742013_1189 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742013
2015-11-23 16:42:23,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742014_1190 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742014
2015-11-23 16:42:23,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742015_1191 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742015
2015-11-23 16:42:23,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742016_1192 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742016
2015-11-23 16:42:23,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742017_1193 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742017
2015-11-23 16:42:23,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742018_1194 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742018
2015-11-23 16:42:23,198 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742019_1195 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742019
2015-11-23 16:42:23,198 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742020_1196 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742020
2015-11-23 16:42:23,203 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742021_1197 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742021
2015-11-23 16:42:23,203 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742022_1198 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742022
2015-11-23 16:42:23,204 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742023_1199 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742023
2015-11-23 16:42:23,204 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742024_1200 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742024
2015-11-23 16:42:23,204 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742025_1201 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742025
2015-11-23 16:42:23,215 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742026_1202 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742026
2015-11-23 16:42:23,215 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742027_1203 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742027
2015-11-23 16:42:23,216 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742028_1204 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742028
2015-11-23 16:42:23,216 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742029_1205 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742029
2015-11-23 16:42:23,217 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742030_1206 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742030
2015-11-23 16:42:23,217 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742031_1207 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742031
2015-11-23 16:42:23,217 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742032_1208 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742032
2015-11-23 16:42:23,217 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742033_1209 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742033
2015-11-23 16:42:23,218 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742034_1210 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742034
2015-11-23 16:42:23,218 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742035_1211 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742035
2015-11-23 16:42:23,223 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742036_1212 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742036
2015-11-23 16:42:23,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742037_1213 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742037
2015-11-23 16:42:23,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742038_1214 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742038
2015-11-23 16:42:23,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742039_1215 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742039
2015-11-23 16:42:23,230 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742040_1216 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742040
2015-11-23 16:42:23,230 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742041_1217 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742041
2015-11-23 16:42:23,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742042_1218 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742042
2015-11-23 16:42:23,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742043_1219 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742043
2015-11-23 16:42:23,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742044_1220 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742044
2015-11-23 16:42:23,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742045_1221 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742045
2015-11-23 16:42:23,233 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742046_1222 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742046
2015-11-23 16:42:23,233 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742047_1223 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742047
2015-11-23 16:42:23,233 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742048_1224 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742048
2015-11-23 16:42:23,234 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742049_1225 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742049
2015-11-23 16:42:23,234 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742050_1226 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742050
2015-11-23 16:42:23,234 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742051_1227 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742051
2015-11-23 16:42:23,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742052_1228 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742052
2015-11-23 16:42:23,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742053_1229 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742053
2015-11-23 16:42:23,244 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742054_1230 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742054
2015-11-23 16:42:23,244 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742055_1231 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742055
2015-11-23 16:42:23,245 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742056_1232 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742056
2015-11-23 16:42:23,245 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742057_1233 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742057
2015-11-23 16:42:23,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742058_1234 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742058
2015-11-23 16:42:23,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742059_1235 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742059
2015-11-23 16:42:23,247 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742060_1236 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742060
2015-11-23 16:42:23,247 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742061_1237 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742061
2015-11-23 16:42:23,248 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742062_1238 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742062
2015-11-23 16:42:23,248 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742063_1239 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742063
2015-11-23 16:42:23,248 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742064_1240 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742064
2015-11-23 16:42:23,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742065_1241 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742065
2015-11-23 16:42:23,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742066_1242 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742066
2015-11-23 16:42:23,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742067_1243 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742067
2015-11-23 16:42:23,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742068_1244 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742068
2015-11-23 16:42:23,250 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742069_1245 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742069
2015-11-23 16:42:23,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742070_1246 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742070
2015-11-23 16:42:23,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742071_1247 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742071
2015-11-23 16:42:23,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742072_1248 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742072
2015-11-23 16:42:23,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742073_1249 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742073
2015-11-23 16:42:23,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742074_1250 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742074
2015-11-23 16:42:23,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742075_1251 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742075
2015-11-23 16:42:23,253 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742076_1252 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742076
2015-11-23 16:42:23,253 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742077_1253 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742077
2015-11-23 16:42:23,254 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742078_1254 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742078
2015-11-23 16:42:23,254 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742079_1255 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742079
2015-11-23 16:42:37,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741841_1017 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:42:37,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Can't send invalid block BP-1750158012-192.168.6.248-1444037565733:blk_1073742070_1246
2015-11-23 16:42:39,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741840_1016 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 16:42:40,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Can't send invalid block BP-1750158012-192.168.6.248-1444037565733:blk_1073742071_1247
2015-11-23 16:42:46,979 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2015-11-23 16:42:46,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2015-11-23 16:42:46,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2015-11-23 16:42:46,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741834_1010 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741834 for deletion
2015-11-23 16:42:46,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741835_1011 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741835 for deletion
2015-11-23 16:42:46,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2015-11-23 16:42:46,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2015-11-23 16:42:46,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2015-11-23 16:42:46,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2015-11-23 16:42:46,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2015-11-23 16:42:46,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2015-11-23 16:42:46,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2015-11-23 16:42:46,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2015-11-23 16:42:46,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2015-11-23 16:42:46,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2015-11-23 16:42:46,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2015-11-23 16:42:46,981 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2015-11-23 16:42:46,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2015-11-23 16:42:46,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741849_1025 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741849 for deletion
2015-11-23 16:42:46,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2015-11-23 16:42:46,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2015-11-23 16:42:46,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2015-11-23 16:42:46,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2015-11-23 16:42:46,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2015-11-23 16:42:46,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2015-11-23 16:42:46,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2015-11-23 16:42:46,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2015-11-23 16:42:46,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2015-11-23 16:42:46,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741859_1035 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741859 for deletion
2015-11-23 16:42:46,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2015-11-23 16:42:46,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2015-11-23 16:42:46,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2015-11-23 16:42:46,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2015-11-23 16:42:46,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2015-11-23 16:42:46,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2015-11-23 16:42:46,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741866_1042 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741866 for deletion
2015-11-23 16:42:46,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741867_1043 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741867 for deletion
2015-11-23 16:42:46,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2015-11-23 16:42:46,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741869_1045 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741869 for deletion
2015-11-23 16:42:46,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2015-11-23 16:42:46,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741871_1047 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741871 for deletion
2015-11-23 16:42:46,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2015-11-23 16:42:46,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2015-11-23 16:42:46,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2015-11-23 16:42:46,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2015-11-23 16:42:46,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2015-11-23 16:42:46,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741877_1053 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741877 for deletion
2015-11-23 16:42:46,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2015-11-23 16:42:46,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2015-11-23 16:42:46,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741880_1056 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741880 for deletion
2015-11-23 16:42:46,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741881_1057 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741881 for deletion
2015-11-23 16:42:46,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741882_1058 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741882 for deletion
2015-11-23 16:42:46,985 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741883_1059 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741883 for deletion
2015-11-23 16:42:46,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741884_1060 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741884 for deletion
2015-11-23 16:42:46,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2015-11-23 16:42:46,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741886_1062 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741886 for deletion
2015-11-23 16:42:46,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741887_1063 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741887 for deletion
2015-11-23 16:42:46,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741888_1064 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741888 for deletion
2015-11-23 16:42:46,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2015-11-23 16:42:46,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741890_1066 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741890 for deletion
2015-11-23 16:42:46,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2015-11-23 16:42:46,986 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2015-11-23 16:42:46,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2015-11-23 16:42:46,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741894_1070 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741894 for deletion
2015-11-23 16:42:46,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741895_1071 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741895 for deletion
2015-11-23 16:42:46,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741896_1072 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741896 for deletion
2015-11-23 16:42:46,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741897_1073 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741897 for deletion
2015-11-23 16:42:46,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741898_1074 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741898 for deletion
2015-11-23 16:42:46,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741899_1075 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741899 for deletion
2015-11-23 16:42:46,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741900_1076 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741900 for deletion
2015-11-23 16:42:46,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741901_1077 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741901 for deletion
2015-11-23 16:42:46,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741902_1078 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741902 for deletion
2015-11-23 16:42:46,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741903_1079 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741903 for deletion
2015-11-23 16:42:46,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741904_1080 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741904 for deletion
2015-11-23 16:42:46,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741905_1081 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741905 for deletion
2015-11-23 16:42:46,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741906_1082 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741906 for deletion
2015-11-23 16:42:46,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741907_1083 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741907 for deletion
2015-11-23 16:42:46,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741908_1084 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741908 for deletion
2015-11-23 16:42:46,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741909_1085 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741909 for deletion
2015-11-23 16:42:46,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741910_1086 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741910 for deletion
2015-11-23 16:42:46,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741911_1087 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741911 for deletion
2015-11-23 16:42:46,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741912_1088 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741912 for deletion
2015-11-23 16:42:46,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741913_1089 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741913 for deletion
2015-11-23 16:42:46,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741914_1090 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741914 for deletion
2015-11-23 16:42:46,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741915_1091 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741915 for deletion
2015-11-23 16:42:46,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741916_1092 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741916 for deletion
2015-11-23 16:42:46,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741917_1093 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741917 for deletion
2015-11-23 16:42:46,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741918_1094 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741918 for deletion
2015-11-23 16:42:46,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741919_1095 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741919 for deletion
2015-11-23 16:42:46,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741920_1096 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741920 for deletion
2015-11-23 16:42:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741921_1097 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741921 for deletion
2015-11-23 16:42:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741922_1098 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741922 for deletion
2015-11-23 16:42:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741923_1099 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741923 for deletion
2015-11-23 16:42:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741924_1100 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741924 for deletion
2015-11-23 16:42:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741925_1101 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741925 for deletion
2015-11-23 16:42:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741926_1102 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741926 for deletion
2015-11-23 16:42:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741927_1103 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741927 for deletion
2015-11-23 16:42:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741928_1104 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741928 for deletion
2015-11-23 16:42:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741929_1105 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741929 for deletion
2015-11-23 16:42:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741930_1106 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741930 for deletion
2015-11-23 16:42:46,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2015-11-23 16:42:46,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741932_1108 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741932 for deletion
2015-11-23 16:42:46,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741933_1109 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741933 for deletion
2015-11-23 16:42:46,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741934_1110 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741934 for deletion
2015-11-23 16:42:46,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741935_1111 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741935 for deletion
2015-11-23 16:42:46,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741936_1112 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741936 for deletion
2015-11-23 16:42:46,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741937_1113 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741937 for deletion
2015-11-23 16:42:46,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741938_1114 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741938 for deletion
2015-11-23 16:42:46,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741939_1115 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741939 for deletion
2015-11-23 16:42:46,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741940_1116 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741940 for deletion
2015-11-23 16:42:46,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741941_1117 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741941 for deletion
2015-11-23 16:42:46,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741942_1118 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741942 for deletion
2015-11-23 16:42:46,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741943_1119 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741943 for deletion
2015-11-23 16:42:46,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741944_1120 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741944 for deletion
2015-11-23 16:42:46,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741945_1121 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741945 for deletion
2015-11-23 16:42:46,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741946_1122 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741946 for deletion
2015-11-23 16:42:46,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741947_1123 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741947 for deletion
2015-11-23 16:42:46,992 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741948_1124 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741948 for deletion
2015-11-23 16:42:46,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741949_1125 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741949 for deletion
2015-11-23 16:42:46,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741950_1126 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741950 for deletion
2015-11-23 16:42:46,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741951_1127 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741951 for deletion
2015-11-23 16:42:46,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741952_1128 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741952 for deletion
2015-11-23 16:42:46,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741953_1129 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741953 for deletion
2015-11-23 16:42:46,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741954_1130 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741954 for deletion
2015-11-23 16:42:46,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741955_1131 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741955 for deletion
2015-11-23 16:42:46,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741956_1132 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741956 for deletion
2015-11-23 16:42:46,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741957_1133 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741957 for deletion
2015-11-23 16:42:46,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741958_1134 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741958 for deletion
2015-11-23 16:42:46,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741959_1135 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741959 for deletion
2015-11-23 16:42:46,994 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741960_1136 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741960 for deletion
2015-11-23 16:42:46,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741831_1007 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741831
2015-11-23 16:42:47,004 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741832_1008 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741832
2015-11-23 16:42:47,004 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741833_1009 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741833
2015-11-23 16:42:47,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741834_1010 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741834
2015-11-23 16:42:47,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741835_1011 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741835
2015-11-23 16:42:47,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741836_1012 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741836
2015-11-23 16:42:47,058 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741837_1013 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741837
2015-11-23 16:42:47,058 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741838_1014 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741838
2015-11-23 16:42:47,058 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741839_1015 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741839
2015-11-23 16:42:47,059 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741840_1016 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741840
2015-11-23 16:42:47,060 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741841_1017 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741841
2015-11-23 16:42:47,060 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741842_1018 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741842
2015-11-23 16:42:47,061 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741843_1019 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741843
2015-11-23 16:42:47,061 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741844_1020 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741844
2015-11-23 16:42:47,061 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741845_1021 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741845
2015-11-23 16:42:47,062 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741846_1022 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741846
2015-11-23 16:42:47,068 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741847_1023 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741847
2015-11-23 16:42:47,069 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741848_1024 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741848
2015-11-23 16:42:47,069 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741849_1025 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741849
2015-11-23 16:42:47,070 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741850_1026 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741850
2015-11-23 16:42:47,070 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741851_1027 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741851
2015-11-23 16:42:47,071 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741852_1028 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741852
2015-11-23 16:42:47,071 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741853_1029 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741853
2015-11-23 16:42:47,072 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741854_1030 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741854
2015-11-23 16:42:47,072 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741855_1031 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741855
2015-11-23 16:42:47,072 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741856_1032 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741856
2015-11-23 16:42:47,073 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741857_1033 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741857
2015-11-23 16:42:47,073 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741858_1034 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741858
2015-11-23 16:42:47,074 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741859_1035 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741859
2015-11-23 16:42:47,074 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741860_1036 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741860
2015-11-23 16:42:47,074 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741861_1037 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741861
2015-11-23 16:42:47,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741862_1038 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741862
2015-11-23 16:42:47,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741863_1039 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741863
2015-11-23 16:42:47,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741864_1040 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741864
2015-11-23 16:42:47,080 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741865_1041 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741865
2015-11-23 16:42:47,080 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741866_1042 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741866
2015-11-23 16:42:47,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741867_1043 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741867
2015-11-23 16:42:47,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741868_1044 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741868
2015-11-23 16:42:47,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741869_1045 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741869
2015-11-23 16:42:47,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741870_1046 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741870
2015-11-23 16:42:47,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741871_1047 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741871
2015-11-23 16:42:47,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741872_1048 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741872
2015-11-23 16:42:47,083 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741873_1049 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741873
2015-11-23 16:42:47,083 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741874_1050 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741874
2015-11-23 16:42:47,083 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741875_1051 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741875
2015-11-23 16:42:47,083 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741876_1052 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741876
2015-11-23 16:42:47,090 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741877_1053 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741877
2015-11-23 16:42:47,091 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741878_1054 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741878
2015-11-23 16:42:47,091 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741879_1055 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741879
2015-11-23 16:42:47,092 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741880_1056 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741880
2015-11-23 16:42:47,092 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741881_1057 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741881
2015-11-23 16:42:47,093 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741882_1058 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741882
2015-11-23 16:42:47,093 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741883_1059 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741883
2015-11-23 16:42:47,093 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741884_1060 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741884
2015-11-23 16:42:47,093 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741885_1061 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741885
2015-11-23 16:42:47,094 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741886_1062 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741886
2015-11-23 16:42:47,094 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741887_1063 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741887
2015-11-23 16:42:47,094 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741888_1064 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741888
2015-11-23 16:42:47,094 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741889_1065 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741889
2015-11-23 16:42:47,095 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741890_1066 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741890
2015-11-23 16:42:47,095 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741891_1067 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741891
2015-11-23 16:42:47,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741892_1068 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741892
2015-11-23 16:42:47,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741893_1069 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741893
2015-11-23 16:42:47,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741894_1070 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741894
2015-11-23 16:42:47,101 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741895_1071 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741895
2015-11-23 16:42:47,101 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741896_1072 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741896
2015-11-23 16:42:47,102 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741897_1073 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741897
2015-11-23 16:42:47,102 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741898_1074 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741898
2015-11-23 16:42:47,102 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741899_1075 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741899
2015-11-23 16:42:47,108 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741900_1076 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741900
2015-11-23 16:42:47,108 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741901_1077 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741901
2015-11-23 16:42:47,109 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741902_1078 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741902
2015-11-23 16:42:47,110 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741903_1079 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741903
2015-11-23 16:42:47,110 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741904_1080 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741904
2015-11-23 16:42:47,110 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741905_1081 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741905
2015-11-23 16:42:47,111 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741906_1082 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741906
2015-11-23 16:42:47,130 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741907_1083 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741907
2015-11-23 16:42:47,130 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741908_1084 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741908
2015-11-23 16:42:47,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741909_1085 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741909
2015-11-23 16:42:47,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741910_1086 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741910
2015-11-23 16:42:47,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741911_1087 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741911
2015-11-23 16:42:47,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741912_1088 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741912
2015-11-23 16:42:47,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741913_1089 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741913
2015-11-23 16:42:47,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741914_1090 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741914
2015-11-23 16:42:47,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741915_1091 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741915
2015-11-23 16:42:47,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741916_1092 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741916
2015-11-23 16:42:47,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741917_1093 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741917
2015-11-23 16:42:47,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741918_1094 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741918
2015-11-23 16:42:47,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741919_1095 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741919
2015-11-23 16:42:47,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741920_1096 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741920
2015-11-23 16:42:47,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741921_1097 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741921
2015-11-23 16:42:47,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741922_1098 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741922
2015-11-23 16:42:47,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741923_1099 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741923
2015-11-23 16:42:47,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741924_1100 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741924
2015-11-23 16:42:47,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741925_1101 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741925
2015-11-23 16:42:47,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741926_1102 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741926
2015-11-23 16:42:47,147 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741927_1103 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741927
2015-11-23 16:42:47,147 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741928_1104 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741928
2015-11-23 16:42:47,147 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741929_1105 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741929
2015-11-23 16:42:47,153 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741930_1106 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741930
2015-11-23 16:42:47,153 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741931_1107 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741931
2015-11-23 16:42:47,154 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741932_1108 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741932
2015-11-23 16:42:47,154 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741933_1109 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741933
2015-11-23 16:42:47,154 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741934_1110 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741934
2015-11-23 16:42:47,155 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741935_1111 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741935
2015-11-23 16:42:47,155 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741936_1112 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741936
2015-11-23 16:42:47,167 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741937_1113 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741937
2015-11-23 16:42:47,167 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741938_1114 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741938
2015-11-23 16:42:47,167 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741939_1115 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741939
2015-11-23 16:42:47,168 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741940_1116 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741940
2015-11-23 16:42:47,168 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741941_1117 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741941
2015-11-23 16:42:47,169 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741942_1118 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741942
2015-11-23 16:42:47,169 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741943_1119 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741943
2015-11-23 16:42:47,169 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741944_1120 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741944
2015-11-23 16:42:47,170 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741945_1121 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741945
2015-11-23 16:42:47,170 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741946_1122 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741946
2015-11-23 16:42:47,175 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741947_1123 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741947
2015-11-23 16:42:47,175 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741948_1124 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741948
2015-11-23 16:42:47,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741949_1125 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741949
2015-11-23 16:42:47,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741950_1126 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741950
2015-11-23 16:42:47,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741951_1127 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741951
2015-11-23 16:42:47,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741952_1128 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741952
2015-11-23 16:42:47,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741953_1129 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741953
2015-11-23 16:42:47,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741954_1130 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741954
2015-11-23 16:42:47,187 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741955_1131 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741955
2015-11-23 16:42:47,187 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741956_1132 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741956
2015-11-23 16:42:47,187 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741957_1133 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741957
2015-11-23 16:42:47,188 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741958_1134 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741958
2015-11-23 16:42:47,188 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741959_1135 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741959
2015-11-23 16:42:47,188 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741960_1136 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741960
2015-11-23 16:42:58,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742091_1267 src: /192.168.6.248:33851 dest: /192.168.6.248:50010
2015-11-23 16:43:11,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33851, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742091_1267, duration: 12074610982
2015-11-23 16:43:11,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742091_1267, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:43:11,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742092_1268 src: /192.168.6.248:33858 dest: /192.168.6.248:50010
2015-11-23 16:43:23,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33858, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742092_1268, duration: 11868394884
2015-11-23 16:43:23,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742092_1268, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:43:23,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742093_1269 src: /192.168.6.248:33862 dest: /192.168.6.248:50010
2015-11-23 16:43:31,700 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 525ms (threshold=300ms)
2015-11-23 16:43:35,312 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1356ms (threshold=300ms)
2015-11-23 16:43:38,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33862, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742093_1269, duration: 14702124505
2015-11-23 16:43:38,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742093_1269, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:43:38,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742094_1270 src: /192.168.6.248:33867 dest: /192.168.6.248:50010
2015-11-23 16:43:50,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33867, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742094_1270, duration: 11921113425
2015-11-23 16:43:50,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742094_1270, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:43:50,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742095_1271 src: /192.168.6.248:33872 dest: /192.168.6.248:50010
2015-11-23 16:44:02,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33872, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742095_1271, duration: 11871127937
2015-11-23 16:44:02,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742095_1271, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:44:02,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742096_1272 src: /192.168.6.248:33877 dest: /192.168.6.248:50010
2015-11-23 16:44:16,691 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 447ms (threshold=300ms)
2015-11-23 16:44:16,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33877, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742096_1272, duration: 14686555232
2015-11-23 16:44:16,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742096_1272, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:44:16,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742097_1273 src: /192.168.6.248:33884 dest: /192.168.6.248:50010
2015-11-23 16:44:28,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33884, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742097_1273, duration: 11930882246
2015-11-23 16:44:28,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742097_1273, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:44:28,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742098_1274 src: /192.168.6.248:33890 dest: /192.168.6.248:50010
2015-11-23 16:44:40,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33890, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742098_1274, duration: 11860239528
2015-11-23 16:44:40,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742098_1274, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:44:40,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742099_1275 src: /192.168.6.248:33896 dest: /192.168.6.248:50010
2015-11-23 16:44:54,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33896, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742099_1275, duration: 13781268371
2015-11-23 16:44:54,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742099_1275, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:44:54,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742100_1276 src: /192.168.6.248:33900 dest: /192.168.6.248:50010
2015-11-23 16:45:03,286 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 431ms (threshold=300ms)
2015-11-23 16:45:06,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33900, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742100_1276, duration: 12468265949
2015-11-23 16:45:06,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742100_1276, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:45:06,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742101_1277 src: /192.168.6.248:33905 dest: /192.168.6.248:50010
2015-11-23 16:45:13,415 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 547ms (threshold=300ms)
2015-11-23 16:45:19,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33905, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742101_1277, duration: 12554842958
2015-11-23 16:45:19,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742101_1277, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:45:19,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742102_1278 src: /192.168.6.248:33911 dest: /192.168.6.248:50010
2015-11-23 16:45:33,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33911, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742102_1278, duration: 14139407695
2015-11-23 16:45:33,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742102_1278, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:45:33,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742103_1279 src: /192.168.6.248:33916 dest: /192.168.6.248:50010
2015-11-23 16:45:45,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33916, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742103_1279, duration: 11859852318
2015-11-23 16:45:45,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742103_1279, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:45:45,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742104_1280 src: /192.168.6.248:33922 dest: /192.168.6.248:50010
2015-11-23 16:45:57,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33922, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742104_1280, duration: 11861026691
2015-11-23 16:45:57,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742104_1280, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:45:57,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742105_1281 src: /192.168.6.248:33928 dest: /192.168.6.248:50010
2015-11-23 16:46:03,035 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:710ms (threshold=300ms)
2015-11-23 16:46:07,120 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 773ms (threshold=300ms)
2015-11-23 16:46:12,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33928, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742105_1281, duration: 14863450962
2015-11-23 16:46:12,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742105_1281, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:46:12,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742106_1282 src: /192.168.6.248:33935 dest: /192.168.6.248:50010
2015-11-23 16:46:24,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33935, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742106_1282, duration: 11863424486
2015-11-23 16:46:24,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742106_1282, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:46:24,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742107_1283 src: /192.168.6.248:33939 dest: /192.168.6.248:50010
2015-11-23 16:46:36,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33939, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742107_1283, duration: 12207646270
2015-11-23 16:46:36,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742107_1283, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:46:36,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742108_1284 src: /192.168.6.248:33944 dest: /192.168.6.248:50010
2015-11-23 16:46:48,312 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:895ms (threshold=300ms)
2015-11-23 16:46:48,926 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:596ms (threshold=300ms)
2015-11-23 16:46:50,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33944, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742108_1284, duration: 13914505577
2015-11-23 16:46:50,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742108_1284, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:46:50,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742109_1285 src: /192.168.6.248:33950 dest: /192.168.6.248:50010
2015-11-23 16:47:02,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33950, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742109_1285, duration: 11930592429
2015-11-23 16:47:02,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742109_1285, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:47:02,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742110_1286 src: /192.168.6.248:33954 dest: /192.168.6.248:50010
2015-11-23 16:47:06,759 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 304ms (threshold=300ms)
2015-11-23 16:47:15,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33954, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742110_1286, duration: 12328879802
2015-11-23 16:47:15,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742110_1286, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:47:15,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742111_1287 src: /192.168.6.248:33961 dest: /192.168.6.248:50010
2015-11-23 16:47:28,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33961, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742111_1287, duration: 13751419214
2015-11-23 16:47:28,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742111_1287, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:47:28,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742112_1288 src: /192.168.6.248:33967 dest: /192.168.6.248:50010
2015-11-23 16:47:40,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33967, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742112_1288, duration: 11859506313
2015-11-23 16:47:40,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742112_1288, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:47:40,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742113_1289 src: /192.168.6.248:33973 dest: /192.168.6.248:50010
2015-11-23 16:47:52,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33973, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742113_1289, duration: 11859608195
2015-11-23 16:47:52,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742113_1289, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:47:52,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742114_1290 src: /192.168.6.248:33977 dest: /192.168.6.248:50010
2015-11-23 16:48:07,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33977, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742114_1290, duration: 14772190298
2015-11-23 16:48:07,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742114_1290, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:48:07,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742115_1291 src: /192.168.6.248:33982 dest: /192.168.6.248:50010
2015-11-23 16:48:16,058 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 403ms (threshold=300ms)
2015-11-23 16:48:19,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33982, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742115_1291, duration: 12399989432
2015-11-23 16:48:19,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742115_1291, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:48:19,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742116_1292 src: /192.168.6.248:33988 dest: /192.168.6.248:50010
2015-11-23 16:48:26,122 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 467ms (threshold=300ms)
2015-11-23 16:48:32,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33988, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742116_1292, duration: 12939875554
2015-11-23 16:48:32,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742116_1292, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:48:33,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742117_1293 src: /192.168.6.248:33993 dest: /192.168.6.248:50010
2015-11-23 16:48:46,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33993, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742117_1293, duration: 13596725916
2015-11-23 16:48:46,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742117_1293, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:48:47,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742118_1294 src: /192.168.6.248:33999 dest: /192.168.6.248:50010
2015-11-23 16:48:59,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33999, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742118_1294, duration: 11859712515
2015-11-23 16:48:59,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742118_1294, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:48:59,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742119_1295 src: /192.168.6.248:34005 dest: /192.168.6.248:50010
2015-11-23 16:49:11,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34005, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742119_1295, duration: 11861513899
2015-11-23 16:49:11,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742119_1295, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:49:11,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742120_1296 src: /192.168.6.248:34012 dest: /192.168.6.248:50010
2015-11-23 16:49:25,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34012, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742120_1296, duration: 13972718266
2015-11-23 16:49:25,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742120_1296, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:49:25,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742121_1297 src: /192.168.6.248:34016 dest: /192.168.6.248:50010
2015-11-23 16:49:27,412 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 432ms (threshold=300ms)
2015-11-23 16:49:37,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34016, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742121_1297, duration: 12436090456
2015-11-23 16:49:37,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742121_1297, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:49:37,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742122_1298 src: /192.168.6.248:34021 dest: /192.168.6.248:50010
2015-11-23 16:49:49,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34021, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742122_1298, duration: 11862131949
2015-11-23 16:49:49,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742122_1298, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:49:49,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742123_1299 src: /192.168.6.248:34026 dest: /192.168.6.248:50010
2015-11-23 16:49:58,225 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:694ms (threshold=300ms)
2015-11-23 16:50:03,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34026, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742123_1299, duration: 14002332195
2015-11-23 16:50:03,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742123_1299, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:50:03,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742124_1300 src: /192.168.6.248:34031 dest: /192.168.6.248:50010
2015-11-23 16:50:15,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34031, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742124_1300, duration: 11859718906
2015-11-23 16:50:15,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742124_1300, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:50:15,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742125_1301 src: /192.168.6.248:34038 dest: /192.168.6.248:50010
2015-11-23 16:50:17,558 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 563ms (threshold=300ms)
2015-11-23 16:50:27,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34038, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742125_1301, duration: 12536422248
2015-11-23 16:50:27,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742125_1301, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:50:27,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742126_1302 src: /192.168.6.248:34044 dest: /192.168.6.248:50010
2015-11-23 16:50:41,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34044, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742126_1302, duration: 13671581728
2015-11-23 16:50:41,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742126_1302, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:50:41,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742127_1303 src: /192.168.6.248:34050 dest: /192.168.6.248:50010
2015-11-23 16:50:49,868 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 943ms (threshold=300ms)
2015-11-23 16:50:54,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34050, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742127_1303, duration: 12961915953
2015-11-23 16:50:54,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742127_1303, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:50:54,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742128_1304 src: /192.168.6.248:34054 dest: /192.168.6.248:50010
2015-11-23 16:51:06,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34054, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742128_1304, duration: 11860891172
2015-11-23 16:51:06,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742128_1304, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:51:06,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742129_1305 src: /192.168.6.248:34059 dest: /192.168.6.248:50010
2015-11-23 16:51:20,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34059, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742129_1305, duration: 13456394639
2015-11-23 16:51:20,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742129_1305, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:51:20,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742130_1306 src: /192.168.6.248:34065 dest: /192.168.6.248:50010
2015-11-23 16:51:32,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34065, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742130_1306, duration: 11859559997
2015-11-23 16:51:32,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742130_1306, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:51:32,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742131_1307 src: /192.168.6.248:34070 dest: /192.168.6.248:50010
2015-11-23 16:51:41,270 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 399ms (threshold=300ms)
2015-11-23 16:51:43,322 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:728ms (threshold=300ms)
2015-11-23 16:51:46,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34070, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742131_1307, duration: 14362551091
2015-11-23 16:51:46,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742131_1307, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:51:46,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742132_1308 src: /192.168.6.248:34076 dest: /192.168.6.248:50010
2015-11-23 16:51:58,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34076, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742132_1308, duration: 11862045469
2015-11-23 16:51:58,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742132_1308, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:51:58,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742133_1309 src: /192.168.6.248:34082 dest: /192.168.6.248:50010
2015-11-23 16:52:10,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34082, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742133_1309, duration: 11861153689
2015-11-23 16:52:10,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742133_1309, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:52:10,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742134_1310 src: /192.168.6.248:34089 dest: /192.168.6.248:50010
2015-11-23 16:52:23,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34089, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742134_1310, duration: 13533290449
2015-11-23 16:52:23,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742134_1310, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:52:23,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742135_1311 src: /192.168.6.248:34093 dest: /192.168.6.248:50010
2015-11-23 16:52:35,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34093, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742135_1311, duration: 11864228151
2015-11-23 16:52:35,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742135_1311, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:52:35,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742136_1312 src: /192.168.6.248:34097 dest: /192.168.6.248:50010
2015-11-23 16:52:37,780 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 381ms (threshold=300ms)
2015-11-23 16:52:48,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34097, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742136_1312, duration: 12341315447
2015-11-23 16:52:48,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742136_1312, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:52:48,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742137_1313 src: /192.168.6.248:34103 dest: /192.168.6.248:50010
2015-11-23 16:53:01,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34103, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742137_1313, duration: 13369275818
2015-11-23 16:53:01,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742137_1313, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:53:01,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742138_1314 src: /192.168.6.248:34109 dest: /192.168.6.248:50010
2015-11-23 16:53:10,636 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 916ms (threshold=300ms)
2015-11-23 16:53:14,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34109, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742138_1314, duration: 12815571766
2015-11-23 16:53:14,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742138_1314, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:53:14,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742139_1315 src: /192.168.6.248:34116 dest: /192.168.6.248:50010
2015-11-23 16:53:26,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34116, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742139_1315, duration: 12133077719
2015-11-23 16:53:26,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742139_1315, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:53:27,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742140_1316 src: /192.168.6.248:34122 dest: /192.168.6.248:50010
2015-11-23 16:53:33,609 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:784ms (threshold=300ms)
2015-11-23 16:53:36,430 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 579ms (threshold=300ms)
2015-11-23 16:53:41,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34122, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742140_1316, duration: 13705382194
2015-11-23 16:53:41,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742140_1316, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:53:41,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742141_1317 src: /192.168.6.248:34128 dest: /192.168.6.248:50010
2015-11-23 16:53:53,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34128, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742141_1317, duration: 11860443706
2015-11-23 16:53:53,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742141_1317, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:53:53,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742142_1318 src: /192.168.6.248:34132 dest: /192.168.6.248:50010
2015-11-23 16:54:07,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34132, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742142_1318, duration: 13773777826
2015-11-23 16:54:07,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742142_1318, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:54:07,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742143_1319 src: /192.168.6.248:34137 dest: /192.168.6.248:50010
2015-11-23 16:54:08,442 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 428ms (threshold=300ms)
2015-11-23 16:54:19,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34137, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742143_1319, duration: 12491642915
2015-11-23 16:54:19,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742143_1319, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:54:19,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742144_1320 src: /192.168.6.248:34143 dest: /192.168.6.248:50010
2015-11-23 16:54:31,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34143, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742144_1320, duration: 11859886757
2015-11-23 16:54:31,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742144_1320, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:54:31,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742145_1321 src: /192.168.6.248:34148 dest: /192.168.6.248:50010
2015-11-23 16:54:40,195 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 441ms (threshold=300ms)
2015-11-23 16:54:46,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34148, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742145_1321, duration: 14556276527
2015-11-23 16:54:46,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742145_1321, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:54:46,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742146_1322 src: /192.168.6.248:34160 dest: /192.168.6.248:50010
2015-11-23 16:54:58,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34160, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742146_1322, duration: 11868782035
2015-11-23 16:54:58,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742146_1322, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:54:58,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742147_1323 src: /192.168.6.248:34166 dest: /192.168.6.248:50010
2015-11-23 16:55:01,537 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 474ms (threshold=300ms)
2015-11-23 16:55:10,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34166, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742147_1323, duration: 12499603265
2015-11-23 16:55:10,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742147_1323, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:55:10,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742148_1324 src: /192.168.6.248:34173 dest: /192.168.6.248:50010
2015-11-23 16:55:24,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34173, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742148_1324, duration: 13402424644
2015-11-23 16:55:24,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742148_1324, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:55:24,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742149_1325 src: /192.168.6.248:34177 dest: /192.168.6.248:50010
2015-11-23 16:55:35,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34177, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742149_1325, duration: 11860522190
2015-11-23 16:55:35,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742149_1325, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:55:35,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742150_1326 src: /192.168.6.248:34181 dest: /192.168.6.248:50010
2015-11-23 16:55:47,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34181, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742150_1326, duration: 11860841252
2015-11-23 16:55:47,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742150_1326, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:55:48,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742151_1327 src: /192.168.6.248:34187 dest: /192.168.6.248:50010
2015-11-23 16:56:01,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34187, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742151_1327, duration: 12942826458
2015-11-23 16:56:01,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742151_1327, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:56:01,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742152_1328 src: /192.168.6.248:34193 dest: /192.168.6.248:50010
2015-11-23 16:56:13,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34193, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742152_1328, duration: 11859251475
2015-11-23 16:56:13,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742152_1328, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:56:13,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742153_1329 src: /192.168.6.248:34200 dest: /192.168.6.248:50010
2015-11-23 16:56:26,773 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:418ms (threshold=300ms)
2015-11-23 16:56:28,793 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 577ms (threshold=300ms)
2015-11-23 16:56:29,390 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 546ms (threshold=300ms)
2015-11-23 16:56:29,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34200, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742153_1329, duration: 15597487362
2015-11-23 16:56:29,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742153_1329, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:56:29,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742154_1330 src: /192.168.6.248:34206 dest: /192.168.6.248:50010
2015-11-23 16:56:41,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34206, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742154_1330, duration: 11862604638
2015-11-23 16:56:41,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742154_1330, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:56:41,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742155_1331 src: /192.168.6.248:34212 dest: /192.168.6.248:50010
2015-11-23 16:56:46,694 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 458ms (threshold=300ms)
2015-11-23 16:56:53,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34212, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742155_1331, duration: 12482198522
2015-11-23 16:56:53,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742155_1331, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:56:53,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742156_1332 src: /192.168.6.248:34216 dest: /192.168.6.248:50010
2015-11-23 16:57:06,993 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:301ms (threshold=300ms)
2015-11-23 16:57:08,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34216, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742156_1332, duration: 14478601666
2015-11-23 16:57:08,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742156_1332, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:57:08,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742157_1333 src: /192.168.6.248:34221 dest: /192.168.6.248:50010
2015-11-23 16:57:20,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34221, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742157_1333, duration: 11859613523
2015-11-23 16:57:20,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742157_1333, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:57:20,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742158_1334 src: /192.168.6.248:34227 dest: /192.168.6.248:50010
2015-11-23 16:57:32,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34227, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742158_1334, duration: 11866028416
2015-11-23 16:57:32,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742158_1334, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:57:32,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742159_1335 src: /192.168.6.248:34232 dest: /192.168.6.248:50010
2015-11-23 16:57:42,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:561ms (threshold=300ms)
2015-11-23 16:57:45,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34232, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742159_1335, duration: 13601145712
2015-11-23 16:57:45,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742159_1335, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:57:45,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742160_1336 src: /192.168.6.248:34238 dest: /192.168.6.248:50010
2015-11-23 16:57:57,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34238, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742160_1336, duration: 11859806646
2015-11-23 16:57:57,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742160_1336, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:57:57,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742161_1337 src: /192.168.6.248:34244 dest: /192.168.6.248:50010
2015-11-23 16:58:01,777 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 538ms (threshold=300ms)
2015-11-23 16:58:10,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34244, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742161_1337, duration: 12507080501
2015-11-23 16:58:10,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742161_1337, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:58:10,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742162_1338 src: /192.168.6.248:34251 dest: /192.168.6.248:50010
2015-11-23 16:58:24,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34251, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742162_1338, duration: 14106219347
2015-11-23 16:58:24,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742162_1338, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:58:24,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742163_1339 src: /192.168.6.248:34255 dest: /192.168.6.248:50010
2015-11-23 16:58:36,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34255, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742163_1339, duration: 11860158232
2015-11-23 16:58:36,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742163_1339, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:58:36,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742164_1340 src: /192.168.6.248:34260 dest: /192.168.6.248:50010
2015-11-23 16:58:38,903 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 423ms (threshold=300ms)
2015-11-23 16:58:48,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34260, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742164_1340, duration: 12452944215
2015-11-23 16:58:48,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742164_1340, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:58:48,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742165_1341 src: /192.168.6.248:34265 dest: /192.168.6.248:50010
2015-11-23 16:58:50,201 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 676ms (threshold=300ms)
2015-11-23 16:59:03,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34265, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742165_1341, duration: 14251408234
2015-11-23 16:59:03,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742165_1341, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:59:03,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742166_1342 src: /192.168.6.248:34277 dest: /192.168.6.248:50010
2015-11-23 16:59:14,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34277, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742166_1342, duration: 11869179411
2015-11-23 16:59:14,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742166_1342, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:59:14,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742167_1343 src: /192.168.6.248:34284 dest: /192.168.6.248:50010
2015-11-23 16:59:27,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34284, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742167_1343, duration: 12039964083
2015-11-23 16:59:27,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742167_1343, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:59:27,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742168_1344 src: /192.168.6.248:34290 dest: /192.168.6.248:50010
2015-11-23 16:59:32,537 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:694ms (threshold=300ms)
2015-11-23 16:59:40,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34290, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742168_1344, duration: 13184305276
2015-11-23 16:59:40,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742168_1344, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:59:40,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742169_1345 src: /192.168.6.248:34296 dest: /192.168.6.248:50010
2015-11-23 16:59:43,988 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 504ms (threshold=300ms)
2015-11-23 16:59:53,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34296, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742169_1345, duration: 12473439084
2015-11-23 16:59:53,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742169_1345, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 16:59:53,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742170_1346 src: /192.168.6.248:34300 dest: /192.168.6.248:50010
2015-11-23 17:00:06,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34300, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742170_1346, duration: 13565419059
2015-11-23 17:00:06,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742170_1346, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:00:06,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742171_1347 src: /192.168.6.248:34305 dest: /192.168.6.248:50010
2015-11-23 17:00:15,849 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1045ms (threshold=300ms)
2015-11-23 17:00:20,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34305, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742171_1347, duration: 13415620718
2015-11-23 17:00:20,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742171_1347, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:00:20,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742172_1348 src: /192.168.6.248:34311 dest: /192.168.6.248:50010
2015-11-23 17:00:26,957 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 520ms (threshold=300ms)
2015-11-23 17:00:32,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34311, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742172_1348, duration: 12506618039
2015-11-23 17:00:32,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742172_1348, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:00:32,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742173_1349 src: /192.168.6.248:34316 dest: /192.168.6.248:50010
2015-11-23 17:00:46,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34316, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742173_1349, duration: 13735622432
2015-11-23 17:00:46,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742173_1349, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:00:46,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742174_1350 src: /192.168.6.248:34322 dest: /192.168.6.248:50010
2015-11-23 17:00:58,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34322, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742174_1350, duration: 11859605292
2015-11-23 17:00:58,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742174_1350, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:00:58,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742175_1351 src: /192.168.6.248:34328 dest: /192.168.6.248:50010
2015-11-23 17:01:10,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34328, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742175_1351, duration: 11861112598
2015-11-23 17:01:10,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742175_1351, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:01:10,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742176_1352 src: /192.168.6.248:34335 dest: /192.168.6.248:50010
2015-11-23 17:01:24,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34335, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742176_1352, duration: 13541739175
2015-11-23 17:01:24,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742176_1352, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:01:24,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742177_1353 src: /192.168.6.248:34339 dest: /192.168.6.248:50010
2015-11-23 17:01:36,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34339, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742177_1353, duration: 11861093907
2015-11-23 17:01:36,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742177_1353, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:01:36,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742178_1354 src: /192.168.6.248:34343 dest: /192.168.6.248:50010
2015-11-23 17:01:48,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34343, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742178_1354, duration: 12207822773
2015-11-23 17:01:48,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742178_1354, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:01:48,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742179_1355 src: /192.168.6.248:34349 dest: /192.168.6.248:50010
2015-11-23 17:01:57,830 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:813ms (threshold=300ms)
2015-11-23 17:02:02,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34349, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742179_1355, duration: 14353153922
2015-11-23 17:02:02,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742179_1355, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:02:02,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742180_1356 src: /192.168.6.248:34355 dest: /192.168.6.248:50010
2015-11-23 17:02:14,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34355, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742180_1356, duration: 11861736133
2015-11-23 17:02:14,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742180_1356, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:02:14,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742181_1357 src: /192.168.6.248:34362 dest: /192.168.6.248:50010
2015-11-23 17:02:26,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34362, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742181_1357, duration: 11938375326
2015-11-23 17:02:26,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742181_1357, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:02:27,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742182_1358 src: /192.168.6.248:34368 dest: /192.168.6.248:50010
2015-11-23 17:02:33,496 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:760ms (threshold=300ms)
2015-11-23 17:02:40,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34368, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742182_1358, duration: 13025860414
2015-11-23 17:02:40,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742182_1358, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:02:40,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742183_1359 src: /192.168.6.248:34374 dest: /192.168.6.248:50010
2015-11-23 17:02:52,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34374, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742183_1359, duration: 11860341957
2015-11-23 17:02:52,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742183_1359, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:02:52,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742184_1360 src: /192.168.6.248:34378 dest: /192.168.6.248:50010
2015-11-23 17:03:03,545 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:753ms (threshold=300ms)
2015-11-23 17:03:06,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34378, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742184_1360, duration: 14075284283
2015-11-23 17:03:06,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742184_1360, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:03:06,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742185_1361 src: /192.168.6.248:34383 dest: /192.168.6.248:50010
2015-11-23 17:03:15,200 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 487ms (threshold=300ms)
2015-11-23 17:03:19,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34383, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742185_1361, duration: 12489393995
2015-11-23 17:03:19,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742185_1361, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:03:19,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742186_1362 src: /192.168.6.248:34389 dest: /192.168.6.248:50010
2015-11-23 17:03:31,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34389, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742186_1362, duration: 11860206641
2015-11-23 17:03:31,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742186_1362, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:03:31,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742187_1363 src: /192.168.6.248:34394 dest: /192.168.6.248:50010
2015-11-23 17:03:45,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34394, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742187_1363, duration: 13770812485
2015-11-23 17:03:45,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742187_1363, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:03:45,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742188_1364 src: /192.168.6.248:34400 dest: /192.168.6.248:50010
2015-11-23 17:03:56,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34400, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742188_1364, duration: 11860211198
2015-11-23 17:03:56,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742188_1364, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:03:56,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742189_1365 src: /192.168.6.248:34406 dest: /192.168.6.248:50010
2015-11-23 17:04:08,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34406, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742189_1365, duration: 11863722542
2015-11-23 17:04:08,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742189_1365, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:04:08,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742190_1366 src: /192.168.6.248:34409 dest: /192.168.6.248:50010
2015-11-23 17:04:18,933 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1009ms (threshold=300ms)
2015-11-23 17:04:22,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34409, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742190_1366, duration: 13992790760
2015-11-23 17:04:22,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742190_1366, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:04:22,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742191_1367 src: /192.168.6.248:34416 dest: /192.168.6.248:50010
2015-11-23 17:04:34,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34416, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742191_1367, duration: 11860310021
2015-11-23 17:04:34,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742191_1367, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:04:34,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742192_1368 src: /192.168.6.248:34420 dest: /192.168.6.248:50010
2015-11-23 17:04:43,931 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 959ms (threshold=300ms)
2015-11-23 17:04:47,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34420, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742192_1368, duration: 12977784314
2015-11-23 17:04:47,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742192_1368, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:04:47,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742193_1369 src: /192.168.6.248:34426 dest: /192.168.6.248:50010
2015-11-23 17:04:53,474 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:381ms (threshold=300ms)
2015-11-23 17:04:55,348 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 576ms (threshold=300ms)
2015-11-23 17:04:59,074 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:981ms (threshold=300ms)
2015-11-23 17:05:02,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34426, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742193_1369, duration: 15069277184
2015-11-23 17:05:02,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742193_1369, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:05:02,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742194_1370 src: /192.168.6.248:34432 dest: /192.168.6.248:50010
2015-11-23 17:05:14,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34432, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742194_1370, duration: 11875467850
2015-11-23 17:05:14,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742194_1370, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:05:14,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742195_1371 src: /192.168.6.248:34439 dest: /192.168.6.248:50010
2015-11-23 17:05:26,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34439, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742195_1371, duration: 11861353893
2015-11-23 17:05:26,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742195_1371, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:05:26,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742196_1372 src: /192.168.6.248:34445 dest: /192.168.6.248:50010
2015-11-23 17:05:34,130 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:959ms (threshold=300ms)
2015-11-23 17:05:41,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34445, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742196_1372, duration: 14369357053
2015-11-23 17:05:41,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742196_1372, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:05:41,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742197_1373 src: /192.168.6.248:34451 dest: /192.168.6.248:50010
2015-11-23 17:05:52,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34451, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742197_1373, duration: 11863002106
2015-11-23 17:05:52,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742197_1373, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:05:52,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742198_1374 src: /192.168.6.248:34455 dest: /192.168.6.248:50010
2015-11-23 17:06:05,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34455, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742198_1374, duration: 12215567891
2015-11-23 17:06:05,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742198_1374, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:06:05,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742199_1375 src: /192.168.6.248:34459 dest: /192.168.6.248:50010
2015-11-23 17:06:08,718 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:460ms (threshold=300ms)
2015-11-23 17:06:18,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34459, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742199_1375, duration: 13296069052
2015-11-23 17:06:18,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742199_1375, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:06:18,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742200_1376 src: /192.168.6.248:34466 dest: /192.168.6.248:50010
2015-11-23 17:06:30,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34466, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742200_1376, duration: 11860852121
2015-11-23 17:06:30,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742200_1376, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:06:30,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742201_1377 src: /192.168.6.248:34472 dest: /192.168.6.248:50010
2015-11-23 17:06:44,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34472, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742201_1377, duration: 13627623756
2015-11-23 17:06:44,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742201_1377, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:06:44,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742202_1378 src: /192.168.6.248:34478 dest: /192.168.6.248:50010
2015-11-23 17:06:45,474 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 498ms (threshold=300ms)
2015-11-23 17:06:56,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34478, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742202_1378, duration: 12471614404
2015-11-23 17:06:56,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742202_1378, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:06:56,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742203_1379 src: /192.168.6.248:34484 dest: /192.168.6.248:50010
2015-11-23 17:07:08,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34484, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742203_1379, duration: 11862188078
2015-11-23 17:07:08,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742203_1379, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:07:08,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742204_1380 src: /192.168.6.248:34487 dest: /192.168.6.248:50010
2015-11-23 17:07:19,140 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:800ms (threshold=300ms)
2015-11-23 17:07:22,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34487, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742204_1380, duration: 14058821887
2015-11-23 17:07:22,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742204_1380, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:07:22,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742205_1381 src: /192.168.6.248:34500 dest: /192.168.6.248:50010
2015-11-23 17:07:34,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34500, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742205_1381, duration: 11859506091
2015-11-23 17:07:34,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742205_1381, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:07:34,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742206_1382 src: /192.168.6.248:34504 dest: /192.168.6.248:50010
2015-11-23 17:07:46,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34504, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742206_1382, duration: 11863230618
2015-11-23 17:07:46,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742206_1382, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:07:46,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742207_1383 src: /192.168.6.248:34510 dest: /192.168.6.248:50010
2015-11-23 17:07:54,092 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:680ms (threshold=300ms)
2015-11-23 17:07:57,400 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 621ms (threshold=300ms)
2015-11-23 17:08:01,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34510, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742207_1383, duration: 14406526718
2015-11-23 17:08:01,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742207_1383, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:08:01,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742208_1384 src: /192.168.6.248:34516 dest: /192.168.6.248:50010
2015-11-23 17:08:12,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34516, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742208_1384, duration: 11865801415
2015-11-23 17:08:12,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742208_1384, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:08:12,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742209_1385 src: /192.168.6.248:34523 dest: /192.168.6.248:50010
2015-11-23 17:08:24,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34523, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742209_1385, duration: 11862598510
2015-11-23 17:08:24,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742209_1385, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:08:25,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742210_1386 src: /192.168.6.248:34527 dest: /192.168.6.248:50010
2015-11-23 17:08:38,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34527, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742210_1386, duration: 12965443525
2015-11-23 17:08:38,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742210_1386, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:08:38,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742211_1387 src: /192.168.6.248:34532 dest: /192.168.6.248:50010
2015-11-23 17:08:50,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34532, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742211_1387, duration: 11859590612
2015-11-23 17:08:50,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742211_1387, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:08:50,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742212_1388 src: /192.168.6.248:34537 dest: /192.168.6.248:50010
2015-11-23 17:09:04,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34537, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742212_1388, duration: 14347105191
2015-11-23 17:09:04,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742212_1388, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:09:04,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742213_1389 src: /192.168.6.248:34542 dest: /192.168.6.248:50010
2015-11-23 17:09:06,068 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 639ms (threshold=300ms)
2015-11-23 17:09:17,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34542, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742213_1389, duration: 12615920102
2015-11-23 17:09:17,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742213_1389, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:09:17,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742214_1390 src: /192.168.6.248:34549 dest: /192.168.6.248:50010
2015-11-23 17:09:29,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34549, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742214_1390, duration: 11861663820
2015-11-23 17:09:29,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742214_1390, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:09:29,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742215_1391 src: /192.168.6.248:34555 dest: /192.168.6.248:50010
2015-11-23 17:09:37,527 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 490ms (threshold=300ms)
2015-11-23 17:09:42,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34555, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742215_1391, duration: 13897599511
2015-11-23 17:09:42,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742215_1391, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:09:43,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742216_1392 src: /192.168.6.248:34561 dest: /192.168.6.248:50010
2015-11-23 17:09:54,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34561, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742216_1392, duration: 11860894635
2015-11-23 17:09:54,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742216_1392, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:09:54,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742217_1393 src: /192.168.6.248:34565 dest: /192.168.6.248:50010
2015-11-23 17:10:03,853 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1132ms (threshold=300ms)
2015-11-23 17:10:07,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34565, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742217_1393, duration: 13050761415
2015-11-23 17:10:07,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742217_1393, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:10:07,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742218_1394 src: /192.168.6.248:34570 dest: /192.168.6.248:50010
2015-11-23 17:10:14,524 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:671ms (threshold=300ms)
2015-11-23 17:10:22,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34570, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742218_1394, duration: 14113795176
2015-11-23 17:10:22,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742218_1394, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:10:22,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742219_1395 src: /192.168.6.248:34577 dest: /192.168.6.248:50010
2015-11-23 17:10:33,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34577, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742219_1395, duration: 11859468518
2015-11-23 17:10:33,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742219_1395, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:10:34,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742220_1396 src: /192.168.6.248:34581 dest: /192.168.6.248:50010
2015-11-23 17:10:38,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:34581, dest: /192.168.6.248:50010, bytes: 51875046, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742220_1396, duration: 4590118591
2015-11-23 17:10:38,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742220_1396, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 17:30:49,976 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-23 17:30:53,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-23 17:30:54,680 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-23 17:30:54,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-24 16:47:26,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-24 16:47:26,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-24 16:47:27,564 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-24 16:47:27,627 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-24 16:47:27,627 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-24 16:47:27,631 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-24 16:47:27,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-24 16:47:27,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-24 16:47:27,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-24 16:47:27,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-24 16:47:27,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-24 16:47:27,774 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-24 16:47:27,782 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-24 16:47:27,787 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-24 16:47:27,792 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-24 16:47:27,794 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-24 16:47:27,794 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-24 16:47:27,794 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-24 16:47:27,804 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40027
2015-11-24 16:47:27,804 INFO org.mortbay.log: jetty-6.1.26
2015-11-24 16:47:27,970 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40027
2015-11-24 16:47:28,079 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-24 16:47:28,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-24 16:47:28,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-24 16:47:28,147 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-24 16:47:28,163 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-24 16:47:28,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-24 16:47:28,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-24 16:47:28,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-24 16:47:28,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-24 16:47:28,275 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-24 16:47:28,275 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-24 16:47:28,798 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 28226@rushikesh1
2015-11-24 16:47:28,892 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-24 16:47:28,893 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-24 16:47:28,893 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-24 16:47:28,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-24 16:47:29,004 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-24 16:47:29,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-24 16:47:29,036 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-24 16:47:29,036 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-24 16:47:29,037 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-24 16:47:29,093 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 56ms
2015-11-24 16:47:29,094 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 57ms
2015-11-24 16:47:29,094 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-24 16:47:29,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 24ms
2015-11-24 16:47:29,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 25ms
2015-11-24 16:47:29,430 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1187158948 ms.
2015-11-24 16:47:29,432 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448377896432 with interval 21600000
2015-11-24 16:47:29,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-24 16:47:29,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-24 16:47:29,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-24 16:47:29,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1391
2015-11-24 16:47:29,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-24 16:47:29,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe3edc132425,  containing 1 storage report(s), of which we sent 1. The reports had 132 total blocks and used 1 RPC(s). This took 5 msec to generate and 102 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-24 16:47:29,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-24 16:53:35,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 src: /192.168.6.248:35016 dest: /192.168.6.248:50010
2015-11-24 16:53:48,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35016, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397, duration: 12501096414
2015-11-24 16:53:48,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:53:48,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 src: /192.168.6.248:35023 dest: /192.168.6.248:50010
2015-11-24 16:54:00,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35023, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398, duration: 11987829018
2015-11-24 16:54:00,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:54:00,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 src: /192.168.6.248:35035 dest: /192.168.6.248:50010
2015-11-24 16:54:13,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35035, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399, duration: 12856853349
2015-11-24 16:54:13,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:54:13,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 src: /192.168.6.248:35040 dest: /192.168.6.248:50010
2015-11-24 16:54:25,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35040, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400, duration: 11943150692
2015-11-24 16:54:25,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:54:25,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 src: /192.168.6.248:35046 dest: /192.168.6.248:50010
2015-11-24 16:54:37,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35046, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401, duration: 11884753047
2015-11-24 16:54:37,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:54:37,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 src: /192.168.6.248:35049 dest: /192.168.6.248:50010
2015-11-24 16:54:42,265 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:401ms (threshold=300ms)
2015-11-24 16:54:47,869 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 593ms (threshold=300ms)
2015-11-24 16:54:51,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35049, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402, duration: 13656669947
2015-11-24 16:54:51,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:54:51,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 src: /192.168.6.248:35056 dest: /192.168.6.248:50010
2015-11-24 16:55:03,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35056, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403, duration: 12006765481
2015-11-24 16:55:03,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:55:03,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 src: /192.168.6.248:35061 dest: /192.168.6.248:50010
2015-11-24 16:55:15,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35061, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404, duration: 12009821494
2015-11-24 16:55:15,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:55:16,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 src: /192.168.6.248:35066 dest: /192.168.6.248:50010
2015-11-24 16:55:28,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35066, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405, duration: 11994391085
2015-11-24 16:55:28,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:55:28,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 src: /192.168.6.248:35072 dest: /192.168.6.248:50010
2015-11-24 16:55:40,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35072, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406, duration: 11859420343
2015-11-24 16:55:40,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:55:40,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 src: /192.168.6.248:35077 dest: /192.168.6.248:50010
2015-11-24 16:55:44,722 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:568ms (threshold=300ms)
2015-11-24 16:55:50,299 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:581ms (threshold=300ms)
2015-11-24 16:55:53,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35077, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407, duration: 13146251074
2015-11-24 16:55:53,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:55:53,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 src: /192.168.6.248:35082 dest: /192.168.6.248:50010
2015-11-24 16:56:00,800 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 497ms (threshold=300ms)
2015-11-24 16:56:06,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35082, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408, duration: 12621303022
2015-11-24 16:56:06,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:56:06,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 src: /192.168.6.248:35088 dest: /192.168.6.248:50010
2015-11-24 16:56:18,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35088, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409, duration: 12293586514
2015-11-24 16:56:18,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:56:18,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 src: /192.168.6.248:35093 dest: /192.168.6.248:50010
2015-11-24 16:56:25,809 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 489ms (threshold=300ms)
2015-11-24 16:56:31,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35093, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410, duration: 12413856868
2015-11-24 16:56:31,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:56:31,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 src: /192.168.6.248:35099 dest: /192.168.6.248:50010
2015-11-24 16:56:42,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35099, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411, duration: 11888271364
2015-11-24 16:56:42,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:56:42,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 src: /192.168.6.248:35105 dest: /192.168.6.248:50010
2015-11-24 16:56:55,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35105, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412, duration: 12683634715
2015-11-24 16:56:55,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:56:55,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 src: /192.168.6.248:35111 dest: /192.168.6.248:50010
2015-11-24 16:57:00,842 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1010ms (threshold=300ms)
2015-11-24 16:57:08,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35111, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413, duration: 13293696368
2015-11-24 16:57:08,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:57:08,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 src: /192.168.6.248:35115 dest: /192.168.6.248:50010
2015-11-24 16:57:21,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35115, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414, duration: 12065382127
2015-11-24 16:57:21,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:57:21,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 src: /192.168.6.248:35120 dest: /192.168.6.248:50010
2015-11-24 16:57:33,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35120, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415, duration: 12026316105
2015-11-24 16:57:33,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:57:33,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 src: /192.168.6.248:35131 dest: /192.168.6.248:50010
2015-11-24 16:57:42,350 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 509ms (threshold=300ms)
2015-11-24 16:57:46,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35131, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416, duration: 12898491217
2015-11-24 16:57:46,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:57:46,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 src: /192.168.6.248:35138 dest: /192.168.6.248:50010
2015-11-24 16:57:58,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35138, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417, duration: 11990867196
2015-11-24 16:57:58,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:57:58,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 src: /192.168.6.248:35144 dest: /192.168.6.248:50010
2015-11-24 16:58:10,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35144, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418, duration: 12087098058
2015-11-24 16:58:10,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:58:10,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 src: /192.168.6.248:35148 dest: /192.168.6.248:50010
2015-11-24 16:58:22,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35148, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419, duration: 11872335871
2015-11-24 16:58:22,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:58:22,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 src: /192.168.6.248:35153 dest: /192.168.6.248:50010
2015-11-24 16:58:34,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35153, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420, duration: 11860087630
2015-11-24 16:58:34,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:58:34,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 src: /192.168.6.248:35158 dest: /192.168.6.248:50010
2015-11-24 16:58:43,287 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 474ms (threshold=300ms)
2015-11-24 16:58:44,148 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:862ms (threshold=300ms)
2015-11-24 16:58:47,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35158, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421, duration: 13695655608
2015-11-24 16:58:47,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:58:48,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 src: /192.168.6.248:35165 dest: /192.168.6.248:50010
2015-11-24 16:58:59,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35165, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422, duration: 11882837132
2015-11-24 16:58:59,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:58:59,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 src: /192.168.6.248:35172 dest: /192.168.6.248:50010
2015-11-24 16:59:11,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35172, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423, duration: 11987363930
2015-11-24 16:59:11,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:59:12,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 src: /192.168.6.248:35178 dest: /192.168.6.248:50010
2015-11-24 16:59:18,681 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:694ms (threshold=300ms)
2015-11-24 16:59:25,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35178, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424, duration: 12549926726
2015-11-24 16:59:25,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:59:25,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 src: /192.168.6.248:35185 dest: /192.168.6.248:50010
2015-11-24 16:59:37,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35185, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425, duration: 12015777394
2015-11-24 16:59:37,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:59:37,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 src: /192.168.6.248:35190 dest: /192.168.6.248:50010
2015-11-24 16:59:48,354 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:633ms (threshold=300ms)
2015-11-24 16:59:50,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35190, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426, duration: 12643778286
2015-11-24 16:59:50,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 16:59:50,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 src: /192.168.6.248:35196 dest: /192.168.6.248:50010
2015-11-24 17:00:02,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35196, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427, duration: 11900079708
2015-11-24 17:00:02,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:00:02,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 src: /192.168.6.248:35202 dest: /192.168.6.248:50010
2015-11-24 17:00:14,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35202, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428, duration: 12057293717
2015-11-24 17:00:14,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:00:14,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 src: /192.168.6.248:35209 dest: /192.168.6.248:50010
2015-11-24 17:00:23,347 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:602ms (threshold=300ms)
2015-11-24 17:00:26,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35209, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429, duration: 12737477796
2015-11-24 17:00:26,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:00:26,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 src: /192.168.6.248:35216 dest: /192.168.6.248:50010
2015-11-24 17:00:38,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35216, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430, duration: 12003010874
2015-11-24 17:00:38,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:00:38,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 src: /192.168.6.248:35220 dest: /192.168.6.248:50010
2015-11-24 17:00:51,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35220, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431, duration: 12060369511
2015-11-24 17:00:51,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:00:51,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 src: /192.168.6.248:35226 dest: /192.168.6.248:50010
2015-11-24 17:01:02,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35226, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432, duration: 11906555279
2015-11-24 17:01:02,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:01:03,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 src: /192.168.6.248:35231 dest: /192.168.6.248:50010
2015-11-24 17:01:14,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35231, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433, duration: 11892289448
2015-11-24 17:01:14,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:01:14,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 src: /192.168.6.248:35236 dest: /192.168.6.248:50010
2015-11-24 17:01:15,868 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 427ms (threshold=300ms)
2015-11-24 17:01:24,164 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:746ms (threshold=300ms)
2015-11-24 17:01:24,529 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:359ms (threshold=300ms)
2015-11-24 17:01:28,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35236, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434, duration: 13840624811
2015-11-24 17:01:28,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:01:28,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 src: /192.168.6.248:35242 dest: /192.168.6.248:50010
2015-11-24 17:01:40,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35242, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435, duration: 11946366904
2015-11-24 17:01:40,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:01:40,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 src: /192.168.6.248:35247 dest: /192.168.6.248:50010
2015-11-24 17:01:52,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35247, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436, duration: 11882028595
2015-11-24 17:01:52,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:01:53,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 src: /192.168.6.248:35252 dest: /192.168.6.248:50010
2015-11-24 17:01:54,288 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742131_1307 for rescanning.
2015-11-24 17:01:54,289 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-11-24 17:01:54,306 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting.
2015-11-24 17:02:15,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35252, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437, duration: 21746613563
2015-11-24 17:02:15,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:02:15,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 src: /192.168.6.248:35262 dest: /192.168.6.248:50010
2015-11-24 17:02:38,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35262, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438, duration: 23313020459
2015-11-24 17:02:38,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:02:38,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 src: /192.168.6.248:35270 dest: /192.168.6.248:50010
2015-11-24 17:02:49,699 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:632ms (threshold=300ms)
2015-11-24 17:02:51,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35270, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439, duration: 12921993368
2015-11-24 17:02:51,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:02:51,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 src: /192.168.6.248:35277 dest: /192.168.6.248:50010
2015-11-24 17:03:03,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35277, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440, duration: 12044942567
2015-11-24 17:03:03,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:03:03,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 src: /192.168.6.248:35282 dest: /192.168.6.248:50010
2015-11-24 17:03:14,134 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 605ms (threshold=300ms)
2015-11-24 17:03:17,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35282, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441, duration: 13727042581
2015-11-24 17:03:17,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:03:17,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442 src: /192.168.6.248:35289 dest: /192.168.6.248:50010
2015-11-24 17:03:40,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35289, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442, duration: 23267111293
2015-11-24 17:03:40,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:03:40,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443 src: /192.168.6.248:35298 dest: /192.168.6.248:50010
2015-11-24 17:03:50,082 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:564ms (threshold=300ms)
2015-11-24 17:03:50,781 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 460ms (threshold=300ms)
2015-11-24 17:04:04,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35298, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443, duration: 24201361420
2015-11-24 17:04:04,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:04:04,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 src: /192.168.6.248:35305 dest: /192.168.6.248:50010
2015-11-24 17:04:16,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35305, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444, duration: 12170665751
2015-11-24 17:04:16,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:04:17,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 src: /192.168.6.248:35311 dest: /192.168.6.248:50010
2015-11-24 17:04:29,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35311, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445, duration: 12205285003
2015-11-24 17:04:29,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:04:29,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446 src: /192.168.6.248:35317 dest: /192.168.6.248:50010
2015-11-24 17:04:43,939 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1168ms (threshold=300ms)
2015-11-24 17:04:54,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35317, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446, duration: 24129416943
2015-11-24 17:04:54,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:04:54,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447 src: /192.168.6.248:35325 dest: /192.168.6.248:50010
2015-11-24 17:05:06,928 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 359ms (threshold=300ms)
2015-11-24 17:05:07,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35325, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447, duration: 13495567272
2015-11-24 17:05:07,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:05:07,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448 src: /192.168.6.248:35332 dest: /192.168.6.248:50010
2015-11-24 17:05:30,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35332, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448, duration: 23315797735
2015-11-24 17:05:30,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:05:30,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449 src: /192.168.6.248:35340 dest: /192.168.6.248:50010
2015-11-24 17:05:45,956 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 679ms (threshold=300ms)
2015-11-24 17:05:55,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35340, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449, duration: 24247919077
2015-11-24 17:05:55,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:05:55,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450 src: /192.168.6.248:35350 dest: /192.168.6.248:50010
2015-11-24 17:06:18,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35350, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450, duration: 23032766292
2015-11-24 17:06:18,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:06:18,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451 src: /192.168.6.248:35358 dest: /192.168.6.248:50010
2015-11-24 17:06:42,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35358, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451, duration: 24026210651
2015-11-24 17:06:42,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:06:42,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452 src: /192.168.6.248:35368 dest: /192.168.6.248:50010
2015-11-24 17:06:48,345 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 443ms (threshold=300ms)
2015-11-24 17:06:56,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35368, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452, duration: 14572352807
2015-11-24 17:06:56,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:06:57,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453 src: /192.168.6.248:35374 dest: /192.168.6.248:50010
2015-11-24 17:06:57,826 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 306ms (threshold=300ms)
2015-11-24 17:06:59,332 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 520ms (threshold=300ms)
2015-11-24 17:07:03,845 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 447ms (threshold=300ms)
2015-11-24 17:07:20,619 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1424ms (threshold=300ms)
2015-11-24 17:07:21,846 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 442ms (threshold=300ms)
2015-11-24 17:07:22,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35374, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453, duration: 25725445621
2015-11-24 17:07:22,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:07:22,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454 src: /192.168.6.248:35382 dest: /192.168.6.248:50010
2015-11-24 17:07:32,662 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 372ms (threshold=300ms)
2015-11-24 17:07:36,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35382, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454, duration: 13398826314
2015-11-24 17:07:36,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:07:36,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455 src: /192.168.6.248:35388 dest: /192.168.6.248:50010
2015-11-24 17:07:59,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35388, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455, duration: 23573303356
2015-11-24 17:07:59,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:07:59,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456 src: /192.168.6.248:35397 dest: /192.168.6.248:50010
2015-11-24 17:08:23,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35397, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456, duration: 23181292103
2015-11-24 17:08:23,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:08:23,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457 src: /192.168.6.248:35404 dest: /192.168.6.248:50010
2015-11-24 17:08:32,555 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:644ms (threshold=300ms)
2015-11-24 17:08:36,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35404, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457, duration: 12937517905
2015-11-24 17:08:36,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:08:36,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458 src: /192.168.6.248:35410 dest: /192.168.6.248:50010
2015-11-24 17:08:57,853 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 541ms (threshold=300ms)
2015-11-24 17:09:00,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35410, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458, duration: 24224023924
2015-11-24 17:09:00,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:09:00,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459 src: /192.168.6.248:35419 dest: /192.168.6.248:50010
2015-11-24 17:09:19,759 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 559ms (threshold=300ms)
2015-11-24 17:09:24,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35419, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459, duration: 24105972935
2015-11-24 17:09:24,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:09:24,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460 src: /192.168.6.248:35427 dest: /192.168.6.248:50010
2015-11-24 17:09:34,190 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 460ms (threshold=300ms)
2015-11-24 17:09:40,817 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 526ms (threshold=300ms)
2015-11-24 17:09:48,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35427, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460, duration: 23907307538
2015-11-24 17:09:48,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:09:48,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461 src: /192.168.6.248:35437 dest: /192.168.6.248:50010
2015-11-24 17:09:56,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 566ms (threshold=300ms)
2015-11-24 17:09:57,928 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 388ms (threshold=300ms)
2015-11-24 17:09:59,722 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1017ms (threshold=300ms)
2015-11-24 17:10:00,562 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:756ms (threshold=300ms)
2015-11-24 17:10:05,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35437, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461, duration: 17178668823
2015-11-24 17:10:05,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:10:05,648 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462 src: /192.168.6.248:35445 dest: /192.168.6.248:50010
2015-11-24 17:10:11,479 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 767ms (threshold=300ms)
2015-11-24 17:10:14,618 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 642ms (threshold=300ms)
2015-11-24 17:10:17,596 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 665ms (threshold=300ms)
2015-11-24 17:10:20,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35445, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462, duration: 14726396883
2015-11-24 17:10:20,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:10:20,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463 src: /192.168.6.248:35451 dest: /192.168.6.248:50010
2015-11-24 17:10:34,620 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 682ms (threshold=300ms)
2015-11-24 17:10:34,938 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 311ms (threshold=300ms)
2015-11-24 17:10:39,870 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 334ms (threshold=300ms)
2015-11-24 17:10:44,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35451, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463, duration: 24279845439
2015-11-24 17:10:44,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:10:44,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464 src: /192.168.6.248:35461 dest: /192.168.6.248:50010
2015-11-24 17:10:51,385 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 804ms (threshold=300ms)
2015-11-24 17:10:55,560 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 680ms (threshold=300ms)
2015-11-24 17:11:09,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35461, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464, duration: 25012572021
2015-11-24 17:11:09,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:11:09,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465 src: /192.168.6.248:35469 dest: /192.168.6.248:50010
2015-11-24 17:11:27,900 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 587ms (threshold=300ms)
2015-11-24 17:11:32,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35469, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465, duration: 22966977451
2015-11-24 17:11:32,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:11:32,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466 src: /192.168.6.248:35477 dest: /192.168.6.248:50010
2015-11-24 17:11:44,329 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 840ms (threshold=300ms)
2015-11-24 17:11:45,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35477, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466, duration: 13014920860
2015-11-24 17:11:45,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:11:45,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467 src: /192.168.6.248:35483 dest: /192.168.6.248:50010
2015-11-24 17:11:52,868 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 310ms (threshold=300ms)
2015-11-24 17:11:58,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35483, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467, duration: 12439001753
2015-11-24 17:11:58,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:11:58,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468 src: /192.168.6.248:35489 dest: /192.168.6.248:50010
2015-11-24 17:12:09,226 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 418ms (threshold=300ms)
2015-11-24 17:12:11,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35489, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468, duration: 13323454749
2015-11-24 17:12:11,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:12:11,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469 src: /192.168.6.248:35495 dest: /192.168.6.248:50010
2015-11-24 17:12:25,729 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 549ms (threshold=300ms)
2015-11-24 17:12:31,636 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 823ms (threshold=300ms)
2015-11-24 17:12:36,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35495, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469, duration: 24449779503
2015-11-24 17:12:36,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:12:36,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470 src: /192.168.6.248:35502 dest: /192.168.6.248:50010
2015-11-24 17:12:44,268 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 732ms (threshold=300ms)
2015-11-24 17:12:52,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35502, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470, duration: 16019053691
2015-11-24 17:12:52,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:12:52,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471 src: /192.168.6.248:35510 dest: /192.168.6.248:50010
2015-11-24 17:13:05,181 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 899ms (threshold=300ms)
2015-11-24 17:13:16,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35510, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471, duration: 24210870344
2015-11-24 17:13:16,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:13:16,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472 src: /192.168.6.248:35520 dest: /192.168.6.248:50010
2015-11-24 17:13:40,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35520, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472, duration: 24272249504
2015-11-24 17:13:40,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:13:40,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473 src: /192.168.6.248:35529 dest: /192.168.6.248:50010
2015-11-24 17:13:52,086 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 396ms (threshold=300ms)
2015-11-24 17:13:53,118 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 473ms (threshold=300ms)
2015-11-24 17:13:59,534 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 804ms (threshold=300ms)
2015-11-24 17:14:05,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35529, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473, duration: 24821833759
2015-11-24 17:14:05,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:14:05,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474 src: /192.168.6.248:35537 dest: /192.168.6.248:50010
2015-11-24 17:14:12,988 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 966ms (threshold=300ms)
2015-11-24 17:14:18,909 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 591ms (threshold=300ms)
2015-11-24 17:14:22,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35537, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474, duration: 16386959251
2015-11-24 17:14:22,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:14:22,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475 src: /192.168.6.248:35544 dest: /192.168.6.248:50010
2015-11-24 17:14:34,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35544, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475, duration: 12079599795
2015-11-24 17:14:34,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:14:34,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476 src: /192.168.6.248:35549 dest: /192.168.6.248:50010
2015-11-24 17:14:45,522 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:946ms (threshold=300ms)
2015-11-24 17:14:47,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35549, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476, duration: 13415828935
2015-11-24 17:14:47,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:14:47,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477 src: /192.168.6.248:35557 dest: /192.168.6.248:50010
2015-11-24 17:15:10,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35557, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477, duration: 23209726430
2015-11-24 17:15:10,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:15:10,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478 src: /192.168.6.248:35565 dest: /192.168.6.248:50010
2015-11-24 17:15:23,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35565, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478, duration: 12410582190
2015-11-24 17:15:23,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:15:23,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479 src: /192.168.6.248:35571 dest: /192.168.6.248:50010
2015-11-24 17:15:35,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35571, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479, duration: 12030898687
2015-11-24 17:15:35,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:15:35,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480 src: /192.168.6.248:35578 dest: /192.168.6.248:50010
2015-11-24 17:15:57,750 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 372ms (threshold=300ms)
2015-11-24 17:15:58,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35578, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480, duration: 23503837566
2015-11-24 17:15:58,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:15:59,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481 src: /192.168.6.248:35587 dest: /192.168.6.248:50010
2015-11-24 17:16:08,507 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 378ms (threshold=300ms)
2015-11-24 17:16:12,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35587, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481, duration: 13485638850
2015-11-24 17:16:12,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:16:12,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482 src: /192.168.6.248:35592 dest: /192.168.6.248:50010
2015-11-24 17:16:27,853 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 727ms (threshold=300ms)
2015-11-24 17:16:32,800 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 799ms (threshold=300ms)
2015-11-24 17:16:36,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35592, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482, duration: 24203640108
2015-11-24 17:16:36,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:16:36,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483 src: /192.168.6.248:35601 dest: /192.168.6.248:50010
2015-11-24 17:16:48,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35601, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483, duration: 12029899545
2015-11-24 17:16:48,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:16:48,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484 src: /192.168.6.248:35608 dest: /192.168.6.248:50010
2015-11-24 17:17:10,686 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 382ms (threshold=300ms)
2015-11-24 17:17:12,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35608, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484, duration: 23112976179
2015-11-24 17:17:12,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:17:12,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485 src: /192.168.6.248:35618 dest: /192.168.6.248:50010
2015-11-24 17:17:35,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35618, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485, duration: 23554221555
2015-11-24 17:17:35,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:17:35,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486 src: /192.168.6.248:35625 dest: /192.168.6.248:50010
2015-11-24 17:17:47,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35625, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486, duration: 12121251084
2015-11-24 17:17:47,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:17:47,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487 src: /192.168.6.248:35632 dest: /192.168.6.248:50010
2015-11-24 17:17:59,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35632, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487, duration: 12068083351
2015-11-24 17:17:59,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:17:59,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488 src: /192.168.6.248:35639 dest: /192.168.6.248:50010
2015-11-24 17:18:10,173 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:334ms (threshold=300ms)
2015-11-24 17:18:10,778 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 431ms (threshold=300ms)
2015-11-24 17:18:13,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35639, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488, duration: 13285340536
2015-11-24 17:18:13,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:18:13,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489 src: /192.168.6.248:35645 dest: /192.168.6.248:50010
2015-11-24 17:18:25,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35645, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489, duration: 11988111632
2015-11-24 17:18:25,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:18:25,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490 src: /192.168.6.248:35650 dest: /192.168.6.248:50010
2015-11-24 17:18:37,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35650, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490, duration: 12269673039
2015-11-24 17:18:37,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:18:38,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491 src: /192.168.6.248:35656 dest: /192.168.6.248:50010
2015-11-24 17:19:01,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35656, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491, duration: 23000810861
2015-11-24 17:19:01,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:19:01,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492 src: /192.168.6.248:35664 dest: /192.168.6.248:50010
2015-11-24 17:19:24,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35664, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492, duration: 23109944323
2015-11-24 17:19:24,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:19:24,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493 src: /192.168.6.248:35671 dest: /192.168.6.248:50010
2015-11-24 17:19:35,350 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:450ms (threshold=300ms)
2015-11-24 17:19:37,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35671, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493, duration: 12824488829
2015-11-24 17:19:37,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:19:37,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494 src: /192.168.6.248:35678 dest: /192.168.6.248:50010
2015-11-24 17:19:49,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35678, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494, duration: 12300496351
2015-11-24 17:19:49,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:19:49,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495 src: /192.168.6.248:35685 dest: /192.168.6.248:50010
2015-11-24 17:20:00,901 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 499ms (threshold=300ms)
2015-11-24 17:20:02,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35685, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495, duration: 13064776471
2015-11-24 17:20:02,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:20:02,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496 src: /192.168.6.248:35692 dest: /192.168.6.248:50010
2015-11-24 17:20:26,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35692, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496, duration: 23228108439
2015-11-24 17:20:26,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:20:26,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497 src: /192.168.6.248:35701 dest: /192.168.6.248:50010
2015-11-24 17:20:35,538 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:597ms (threshold=300ms)
2015-11-24 17:20:49,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35701, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497, duration: 23165367231
2015-11-24 17:20:49,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:20:49,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498 src: /192.168.6.248:35709 dest: /192.168.6.248:50010
2015-11-24 17:21:01,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35709, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498, duration: 12132625659
2015-11-24 17:21:01,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:21:01,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 src: /192.168.6.248:35716 dest: /192.168.6.248:50010
2015-11-24 17:21:14,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35716, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499, duration: 12516160630
2015-11-24 17:21:14,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:21:14,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500 src: /192.168.6.248:35722 dest: /192.168.6.248:50010
2015-11-24 17:21:37,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35722, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500, duration: 23240928103
2015-11-24 17:21:37,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:21:37,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501 src: /192.168.6.248:35729 dest: /192.168.6.248:50010
2015-11-24 17:21:49,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35729, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501, duration: 12260164554
2015-11-24 17:21:49,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:21:49,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502 src: /192.168.6.248:35735 dest: /192.168.6.248:50010
2015-11-24 17:22:13,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35735, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502, duration: 23220457018
2015-11-24 17:22:13,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:22:13,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503 src: /192.168.6.248:35745 dest: /192.168.6.248:50010
2015-11-24 17:22:25,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35745, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503, duration: 12141068970
2015-11-24 17:22:25,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:22:25,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504 src: /192.168.6.248:35752 dest: /192.168.6.248:50010
2015-11-24 17:22:37,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35752, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504, duration: 12463950410
2015-11-24 17:22:37,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:22:38,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505 src: /192.168.6.248:35756 dest: /192.168.6.248:50010
2015-11-24 17:22:50,698 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:544ms (threshold=300ms)
2015-11-24 17:23:01,279 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 414ms (threshold=300ms)
2015-11-24 17:23:02,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35756, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505, duration: 24099639629
2015-11-24 17:23:02,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:23:02,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506 src: /192.168.6.248:35765 dest: /192.168.6.248:50010
2015-11-24 17:23:25,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35765, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506, duration: 23087935726
2015-11-24 17:23:25,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:23:25,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507 src: /192.168.6.248:35774 dest: /192.168.6.248:50010
2015-11-24 17:23:37,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35774, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507, duration: 12361775954
2015-11-24 17:23:37,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:23:37,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508 src: /192.168.6.248:35778 dest: /192.168.6.248:50010
2015-11-24 17:24:00,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35778, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508, duration: 22722751955
2015-11-24 17:24:00,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:24:00,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509 src: /192.168.6.248:35788 dest: /192.168.6.248:50010
2015-11-24 17:24:23,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35788, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509, duration: 23152604928
2015-11-24 17:24:23,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:24:23,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 src: /192.168.6.248:35795 dest: /192.168.6.248:50010
2015-11-24 17:24:37,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35795, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510, duration: 13365708670
2015-11-24 17:24:37,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:24:37,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511 src: /192.168.6.248:35802 dest: /192.168.6.248:50010
2015-11-24 17:25:00,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35802, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511, duration: 23186356603
2015-11-24 17:25:00,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:25:00,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512 src: /192.168.6.248:35812 dest: /192.168.6.248:50010
2015-11-24 17:25:23,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35812, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512, duration: 23098837806
2015-11-24 17:25:23,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:25:23,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513 src: /192.168.6.248:35820 dest: /192.168.6.248:50010
2015-11-24 17:25:31,365 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 468ms (threshold=300ms)
2015-11-24 17:25:36,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35820, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513, duration: 12442555932
2015-11-24 17:25:36,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:25:36,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514 src: /192.168.6.248:35826 dest: /192.168.6.248:50010
2015-11-24 17:25:59,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35826, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514, duration: 23251489043
2015-11-24 17:25:59,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:25:59,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515 src: /192.168.6.248:35835 dest: /192.168.6.248:50010
2015-11-24 17:26:10,735 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:513ms (threshold=300ms)
2015-11-24 17:26:11,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35835, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515, duration: 12539079731
2015-11-24 17:26:11,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:26:11,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 src: /192.168.6.248:35840 dest: /192.168.6.248:50010
2015-11-24 17:26:34,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35840, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516, duration: 23045458459
2015-11-24 17:26:34,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:26:35,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517 src: /192.168.6.248:35846 dest: /192.168.6.248:50010
2015-11-24 17:26:57,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35846, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517, duration: 22696950130
2015-11-24 17:26:57,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:26:57,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518 src: /192.168.6.248:35857 dest: /192.168.6.248:50010
2015-11-24 17:27:20,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35857, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518, duration: 23115655403
2015-11-24 17:27:20,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:27:20,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519 src: /192.168.6.248:35863 dest: /192.168.6.248:50010
2015-11-24 17:27:43,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35863, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519, duration: 22664008279
2015-11-24 17:27:43,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:27:43,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520 src: /192.168.6.248:35874 dest: /192.168.6.248:50010
2015-11-24 17:27:56,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35874, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520, duration: 12482086482
2015-11-24 17:27:56,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:27:56,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521 src: /192.168.6.248:35881 dest: /192.168.6.248:50010
2015-11-24 17:28:16,730 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 649ms (threshold=300ms)
2015-11-24 17:28:19,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35881, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521, duration: 23741307515
2015-11-24 17:28:19,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:28:19,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522 src: /192.168.6.248:35887 dest: /192.168.6.248:50010
2015-11-24 17:28:32,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35887, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522, duration: 12284406468
2015-11-24 17:28:32,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:28:32,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 src: /192.168.6.248:35894 dest: /192.168.6.248:50010
2015-11-24 17:28:55,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35894, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523, duration: 22807580368
2015-11-24 17:28:55,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:28:55,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 src: /192.168.6.248:35905 dest: /192.168.6.248:50010
2015-11-24 17:29:06,213 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:456ms (threshold=300ms)
2015-11-24 17:29:08,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35905, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524, duration: 12406130735
2015-11-24 17:29:08,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:29:08,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 src: /192.168.6.248:35909 dest: /192.168.6.248:50010
2015-11-24 17:29:31,435 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:664ms (threshold=300ms)
2015-11-24 17:29:32,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35909, dest: /192.168.6.248:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525, duration: 23713291582
2015-11-24 17:29:32,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:29:32,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526 src: /192.168.6.248:35917 dest: /192.168.6.248:50010
2015-11-24 17:29:40,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35917, dest: /192.168.6.248:50010, bytes: 51875046, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526, duration: 8245656640
2015-11-24 17:29:40,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:39:52,566 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 for rescanning.
2015-11-24 17:40:18,977 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 for rescanning, because we rescanned it recently.
2015-11-24 17:48:02,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742351_1527 src: /192.168.6.248:36174 dest: /192.168.6.248:50010
2015-11-24 17:49:46,053 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 for rescanning.
2015-11-24 17:51:30,900 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 for rescanning.
2015-11-24 17:51:30,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36174, dest: /192.168.6.248:50010, bytes: 63918210, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742351_1527, duration: 208599477680
2015-11-24 17:51:30,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742351_1527, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:51:31,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742352_1528 src: /192.168.6.248:36234 dest: /192.168.6.248:50010
2015-11-24 17:53:16,208 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 for rescanning.
2015-11-24 17:55:01,964 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 for rescanning.
2015-11-24 17:55:01,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36234, dest: /192.168.6.248:50010, bytes: 63959903, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742352_1528, duration: 210715713051
2015-11-24 17:55:01,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742352_1528, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:55:02,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742353_1529 src: /192.168.6.248:36298 dest: /192.168.6.248:50010
2015-11-24 17:56:47,767 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 for rescanning.
2015-11-24 17:58:33,075 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 for rescanning.
2015-11-24 17:58:33,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36298, dest: /192.168.6.248:50010, bytes: 63944909, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742353_1529, duration: 210762042533
2015-11-24 17:58:33,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742353_1529, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 17:58:33,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742354_1530 src: /192.168.6.248:36361 dest: /192.168.6.248:50010
2015-11-24 18:00:18,508 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 for rescanning.
2015-11-24 18:02:03,810 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 for rescanning.
2015-11-24 18:02:03,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36361, dest: /192.168.6.248:50010, bytes: 63970677, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742354_1530, duration: 210372042544
2015-11-24 18:02:03,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742354_1530, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:02:04,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742355_1531 src: /192.168.6.248:36422 dest: /192.168.6.248:50010
2015-11-24 18:03:48,618 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 for rescanning.
2015-11-24 18:05:32,754 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 for rescanning.
2015-11-24 18:05:32,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36422, dest: /192.168.6.248:50010, bytes: 63892089, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742355_1531, duration: 208559720522
2015-11-24 18:05:32,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742355_1531, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:05:33,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742356_1532 src: /192.168.6.248:36486 dest: /192.168.6.248:50010
2015-11-24 18:07:16,973 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 for rescanning.
2015-11-24 18:09:01,224 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 for rescanning.
2015-11-24 18:09:01,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36486, dest: /192.168.6.248:50010, bytes: 64089316, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742356_1532, duration: 208140571931
2015-11-24 18:09:01,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742356_1532, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:09:01,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742357_1533 src: /192.168.6.248:36544 dest: /192.168.6.248:50010
2015-11-24 18:10:45,340 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 for rescanning.
2015-11-24 18:12:29,533 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 for rescanning.
2015-11-24 18:12:29,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36544, dest: /192.168.6.248:50010, bytes: 64090454, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742357_1533, duration: 207951282617
2015-11-24 18:12:29,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742357_1533, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:12:29,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742358_1534 src: /192.168.6.248:36602 dest: /192.168.6.248:50010
2015-11-24 18:14:13,882 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 for rescanning.
2015-11-24 18:15:58,027 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 for rescanning.
2015-11-24 18:15:58,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36602, dest: /192.168.6.248:50010, bytes: 64177702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742358_1534, duration: 208050705417
2015-11-24 18:15:58,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742358_1534, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:15:58,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742359_1535 src: /192.168.6.248:36660 dest: /192.168.6.248:50010
2015-11-24 18:17:42,340 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 for rescanning.
2015-11-24 18:19:26,331 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 for rescanning.
2015-11-24 18:19:26,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36660, dest: /192.168.6.248:50010, bytes: 64037038, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742359_1535, duration: 207978307424
2015-11-24 18:19:26,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742359_1535, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:19:26,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742360_1536 src: /192.168.6.248:36717 dest: /192.168.6.248:50010
2015-11-24 18:21:10,691 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 for rescanning.
2015-11-24 18:22:55,075 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 for rescanning.
2015-11-24 18:22:55,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36717, dest: /192.168.6.248:50010, bytes: 64093563, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742360_1536, duration: 208364787902
2015-11-24 18:22:55,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742360_1536, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:22:55,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742361_1537 src: /192.168.6.248:36773 dest: /192.168.6.248:50010
2015-11-24 18:24:40,692 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 for rescanning.
2015-11-24 18:26:24,679 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 for rescanning.
2015-11-24 18:26:24,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36773, dest: /192.168.6.248:50010, bytes: 64115246, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742361_1537, duration: 209238287613
2015-11-24 18:26:24,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742361_1537, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:26:25,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742362_1538 src: /192.168.6.248:36836 dest: /192.168.6.248:50010
2015-11-24 18:28:08,999 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 for rescanning.
2015-11-24 18:29:53,320 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 for rescanning.
2015-11-24 18:29:53,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36836, dest: /192.168.6.248:50010, bytes: 64121965, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742362_1538, duration: 208293907248
2015-11-24 18:29:53,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742362_1538, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:29:53,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742363_1539 src: /192.168.6.248:36893 dest: /192.168.6.248:50010
2015-11-24 18:31:37,511 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 for rescanning.
2015-11-24 18:33:21,587 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 for rescanning.
2015-11-24 18:33:21,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36893, dest: /192.168.6.248:50010, bytes: 64023361, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742363_1539, duration: 207873365124
2015-11-24 18:33:21,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742363_1539, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:33:21,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742364_1540 src: /192.168.6.248:36949 dest: /192.168.6.248:50010
2015-11-24 18:35:05,872 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 for rescanning.
2015-11-24 18:36:49,360 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 for rescanning.
2015-11-24 18:36:49,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36949, dest: /192.168.6.248:50010, bytes: 64095447, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742364_1540, duration: 207381900224
2015-11-24 18:36:49,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742364_1540, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:36:49,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742365_1541 src: /192.168.6.248:37006 dest: /192.168.6.248:50010
2015-11-24 18:38:33,668 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 for rescanning.
2015-11-24 18:40:17,943 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 for rescanning.
2015-11-24 18:40:17,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37006, dest: /192.168.6.248:50010, bytes: 64018975, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742365_1541, duration: 208197022640
2015-11-24 18:40:17,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742365_1541, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:40:18,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742366_1542 src: /192.168.6.248:37063 dest: /192.168.6.248:50010
2015-11-24 18:42:03,108 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 for rescanning.
2015-11-24 18:43:47,604 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 for rescanning.
2015-11-24 18:43:47,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37063, dest: /192.168.6.248:50010, bytes: 64084413, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742366_1542, duration: 209209751557
2015-11-24 18:43:47,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742366_1542, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:43:47,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742367_1543 src: /192.168.6.248:37121 dest: /192.168.6.248:50010
2015-11-24 18:45:31,649 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 for rescanning.
2015-11-24 18:47:15,677 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 for rescanning.
2015-11-24 18:47:15,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37121, dest: /192.168.6.248:50010, bytes: 64070613, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742367_1543, duration: 207683102693
2015-11-24 18:47:15,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742367_1543, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:47:16,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742368_1544 src: /192.168.6.248:37178 dest: /192.168.6.248:50010
2015-11-24 18:48:59,844 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 for rescanning.
2015-11-24 18:50:44,146 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 for rescanning.
2015-11-24 18:50:44,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37178, dest: /192.168.6.248:50010, bytes: 64084139, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742368_1544, duration: 208071846990
2015-11-24 18:50:44,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742368_1544, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:50:44,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742369_1545 src: /192.168.6.248:37237 dest: /192.168.6.248:50010
2015-11-24 18:52:28,256 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 for rescanning.
2015-11-24 18:54:12,533 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 for rescanning.
2015-11-24 18:54:12,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37237, dest: /192.168.6.248:50010, bytes: 64004195, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742369_1545, duration: 208035925500
2015-11-24 18:54:12,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742369_1545, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:54:12,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742370_1546 src: /192.168.6.248:37300 dest: /192.168.6.248:50010
2015-11-24 18:55:56,604 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 for rescanning.
2015-11-24 18:57:40,779 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 for rescanning.
2015-11-24 18:57:40,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37300, dest: /192.168.6.248:50010, bytes: 63987327, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742370_1546, duration: 207890416833
2015-11-24 18:57:40,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742370_1546, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 18:57:41,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742371_1547 src: /192.168.6.248:37356 dest: /192.168.6.248:50010
2015-11-24 18:59:25,179 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 for rescanning.
2015-11-24 19:01:09,444 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 for rescanning.
2015-11-24 19:01:09,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37356, dest: /192.168.6.248:50010, bytes: 64009289, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742371_1547, duration: 208258443621
2015-11-24 19:01:09,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742371_1547, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 19:01:09,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742372_1548 src: /192.168.6.248:37411 dest: /192.168.6.248:50010
2015-11-24 19:02:53,346 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 for rescanning.
2015-11-24 19:04:37,170 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 for rescanning.
2015-11-24 19:04:37,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37411, dest: /192.168.6.248:50010, bytes: 64186338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742372_1548, duration: 207368638733
2015-11-24 19:04:37,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742372_1548, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 19:04:37,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1549 src: /192.168.6.248:37467 dest: /192.168.6.248:50010
2015-11-24 19:04:54,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1549, type=HAS_DOWNSTREAM_IN_PIPELINE
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2278)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:244)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1237)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:04:54,908 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver.run(): 
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:488)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstreamUnprotected(BlockReceiver.java:1473)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstream(BlockReceiver.java:1410)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1323)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:04:54,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1549
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:201)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:472)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:849)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:804)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:04:54,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting CheckDiskError Thread
2015-11-24 19:04:54,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1549, type=HAS_DOWNSTREAM_IN_PIPELINE
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:488)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstreamUnprotected(BlockReceiver.java:1473)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstream(BlockReceiver.java:1410)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1323)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:04:54,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1549, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 19:04:54,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1549 received exception java.io.IOException: Premature EOF from inputStream
2015-11-24 19:04:54,912 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.6.248:37467 dst: /192.168.6.248:50010
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:201)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:472)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:849)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:804)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:04:54,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1549 src: /192.168.6.248:37474 dest: /192.168.6.248:50010
2015-11-24 19:04:54,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recover RBW replica BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1549
2015-11-24 19:04:54,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recovering ReplicaBeingWritten, blk_1073742373_1549, RBW
  getNumBytes()     = 5354496
  getBytesOnDisk()  = 5354496
  getVisibleLength()= 5225472
  getVolume()       = /app/hadoop/tmp/dfs/data/current
  getBlockFile()    = /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/rbw/blk_1073742373
  bytesAcked=5225472
  bytesOnDisk=5354496
2015-11-24 19:04:54,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: truncateBlock: blockFile=/app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/rbw/blk_1073742373, metaFile=/app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/rbw/blk_1073742373_1549.meta, oldlen=5354496, newlen=5225472
2015-11-24 19:06:05,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1550, type=HAS_DOWNSTREAM_IN_PIPELINE
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2278)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:244)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1237)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:06:05,479 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver.run(): 
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:488)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstreamUnprotected(BlockReceiver.java:1473)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstream(BlockReceiver.java:1410)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1323)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:06:05,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1550, type=HAS_DOWNSTREAM_IN_PIPELINE
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:488)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstreamUnprotected(BlockReceiver.java:1473)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstream(BlockReceiver.java:1410)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1323)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:06:05,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1550, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 19:06:05,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1550
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:413)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:472)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:849)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:804)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:06:05,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1550 received exception java.nio.channels.ClosedByInterruptException
2015-11-24 19:06:05,481 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.6.248:37474 dst: /192.168.6.248:50010
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:413)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:472)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:849)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:804)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:06:05,495 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 for rescanning, because we rescanned it recently.
2015-11-24 19:06:05,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742374_1551 src: /192.168.6.248:37494 dest: /192.168.6.248:50010
2015-11-24 19:06:23,885 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/app/hadoop/tmp/dfs/data/current]'}, localName='rushikesh1:50010', datanodeUuid='d629bce3-4072-426c-a3ff-71fefbd485b4', xmitsInProgress=0}:Exception transfering block BP-1750158012-192.168.6.248-1444037565733:blk_1073742374_1551 to mirror 192.168.6.249:50010: java.net.NoRouteToHostException: No route to host
2015-11-24 19:06:23,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1750158012-192.168.6.248-1444037565733:blk_1073742374_1551 received exception java.net.NoRouteToHostException: No route to host
2015-11-24 19:06:23,885 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.6.248:37494 dst: /192.168.6.248:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:06:23,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742375_1552 src: /192.168.6.248:37508 dest: /192.168.6.248:50010
2015-11-24 19:06:58,028 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/app/hadoop/tmp/dfs/data/current]'}, localName='rushikesh1:50010', datanodeUuid='d629bce3-4072-426c-a3ff-71fefbd485b4', xmitsInProgress=0}:Exception transfering block BP-1750158012-192.168.6.248-1444037565733:blk_1073742375_1552 to mirror 192.168.6.237:50010: java.net.NoRouteToHostException: No route to host
2015-11-24 19:06:58,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1750158012-192.168.6.248-1444037565733:blk_1073742375_1552 received exception java.net.NoRouteToHostException: No route to host
2015-11-24 19:06:58,029 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.6.248:37508 dst: /192.168.6.248:50010
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:707)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
	at java.lang.Thread.run(Thread.java:745)
2015-11-24 19:06:58,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742376_1553 src: /192.168.6.248:37520 dest: /192.168.6.248:50010
2015-11-24 19:08:25,843 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 for rescanning.
2015-11-24 19:09:08,386 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 for rescanning, because we rescanned it recently.
2015-11-24 19:09:13,255 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742368_1544 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742368 for deletion
2015-11-24 19:09:13,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742369_1545 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742369 for deletion
2015-11-24 19:09:13,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742370_1546 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742370 for deletion
2015-11-24 19:09:13,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742371_1547 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742371 for deletion
2015-11-24 19:09:13,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742372_1548 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742372 for deletion
2015-11-24 19:09:13,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742351_1527 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742351 for deletion
2015-11-24 19:09:13,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742352_1528 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742352 for deletion
2015-11-24 19:09:13,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742353_1529 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742353 for deletion
2015-11-24 19:09:13,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742354_1530 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742354 for deletion
2015-11-24 19:09:13,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742355_1531 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742355 for deletion
2015-11-24 19:09:13,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742356_1532 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742356 for deletion
2015-11-24 19:09:13,256 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742357_1533 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742357 for deletion
2015-11-24 19:09:13,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742358_1534 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742358 for deletion
2015-11-24 19:09:13,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742359_1535 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742359 for deletion
2015-11-24 19:09:13,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742360_1536 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742360 for deletion
2015-11-24 19:09:13,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742361_1537 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742361 for deletion
2015-11-24 19:09:13,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742362_1538 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742362 for deletion
2015-11-24 19:09:13,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742363_1539 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742363 for deletion
2015-11-24 19:09:13,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742364_1540 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742364 for deletion
2015-11-24 19:09:13,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742365_1541 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742365 for deletion
2015-11-24 19:09:13,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742366_1542 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742366 for deletion
2015-11-24 19:09:13,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742367_1543 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742367 for deletion
2015-11-24 19:09:13,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742368_1544 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742368
2015-11-24 19:09:13,273 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742369_1545 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742369
2015-11-24 19:09:13,281 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742370_1546 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742370
2015-11-24 19:09:13,289 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742371_1547 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742371
2015-11-24 19:09:13,298 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742372_1548 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742372
2015-11-24 19:09:13,323 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742351_1527 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742351
2015-11-24 19:09:13,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742352_1528 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742352
2015-11-24 19:09:13,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742353_1529 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742353
2015-11-24 19:09:13,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742354_1530 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742354
2015-11-24 19:09:13,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742355_1531 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742355
2015-11-24 19:09:13,330 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742356_1532 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742356
2015-11-24 19:09:13,331 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742357_1533 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742357
2015-11-24 19:09:13,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742358_1534 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742358
2015-11-24 19:09:13,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742359_1535 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742359
2015-11-24 19:09:13,335 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742360_1536 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742360
2015-11-24 19:09:13,339 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742361_1537 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742361
2015-11-24 19:09:13,343 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742362_1538 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742362
2015-11-24 19:09:13,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742363_1539 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742363
2015-11-24 19:09:13,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742364_1540 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742364
2015-11-24 19:09:13,361 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742365_1541 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742365
2015-11-24 19:09:13,369 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742366_1542 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742366
2015-11-24 19:09:13,378 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742367_1543 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742367
2015-11-24 19:10:11,545 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 for rescanning.
2015-11-24 19:10:11,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37520, dest: /192.168.6.248:50010, bytes: 64016012, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742376_1553, duration: 193479352427
2015-11-24 19:10:11,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742376_1553, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 19:10:16,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742376_1553 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742376 for deletion
2015-11-24 19:10:16,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742376_1553 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742376
2015-11-24 19:10:39,329 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 is already queued for rescanning.
2015-11-24 19:10:48,023 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 is already queued for rescanning.
2015-11-24 19:10:48,023 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 is already queued for rescanning.
2015-11-24 19:10:48,067 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 is already queued for rescanning.
2015-11-24 19:10:48,067 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 is already queued for rescanning.
2015-11-24 19:10:48,147 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 is already queued for rescanning.
2015-11-24 19:10:48,195 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 is already queued for rescanning.
2015-11-24 19:10:48,239 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 is already queued for rescanning.
2015-11-24 19:10:48,263 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 is already queued for rescanning.
2015-11-24 19:10:48,305 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 is already queued for rescanning.
2015-11-24 19:10:48,316 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 is already queued for rescanning.
2015-11-24 19:10:48,353 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 is already queued for rescanning.
2015-11-24 19:10:48,370 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 is already queued for rescanning.
2015-11-24 19:10:48,415 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 is already queued for rescanning.
2015-11-24 19:10:48,446 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 is already queued for rescanning.
2015-11-24 19:10:48,476 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 is already queued for rescanning.
2015-11-24 19:10:48,495 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 is already queued for rescanning.
2015-11-24 19:10:48,541 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 is already queued for rescanning.
2015-11-24 19:10:48,565 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 is already queued for rescanning.
2015-11-24 19:10:48,649 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 is already queued for rescanning.
2015-11-24 19:10:48,675 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 is already queued for rescanning.
2015-11-24 19:10:48,719 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 is already queued for rescanning.
2015-11-24 19:10:48,741 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 is already queued for rescanning.
2015-11-24 19:10:48,790 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 is already queued for rescanning.
2015-11-24 19:10:48,807 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 is already queued for rescanning.
2015-11-24 19:10:48,858 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 is already queued for rescanning.
2015-11-24 19:10:48,879 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 is already queued for rescanning.
2015-11-24 19:10:48,928 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 is already queued for rescanning.
2015-11-24 19:10:48,948 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 is already queued for rescanning.
2015-11-24 19:10:48,997 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 is already queued for rescanning.
2015-11-24 19:10:49,013 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 is already queued for rescanning.
2015-11-24 19:10:49,028 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 is already queued for rescanning.
2015-11-24 19:10:49,037 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 is already queued for rescanning.
2015-11-24 19:10:49,066 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 is already queued for rescanning.
2015-11-24 19:10:49,073 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 is already queued for rescanning.
2015-11-24 19:10:49,096 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 is already queued for rescanning.
2015-11-24 19:10:49,104 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 is already queued for rescanning.
2015-11-24 19:10:49,131 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 is already queued for rescanning.
2015-11-24 19:10:49,138 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 is already queued for rescanning.
2015-11-24 19:10:49,160 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 is already queued for rescanning.
2015-11-24 19:10:49,167 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 is already queued for rescanning.
2015-11-24 19:10:49,201 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 is already queued for rescanning.
2015-11-24 19:10:49,209 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 is already queued for rescanning.
2015-11-24 19:10:49,237 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 is already queued for rescanning.
2015-11-24 19:10:49,246 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 is already queued for rescanning.
2015-11-24 19:10:49,269 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 for rescanning, because we rescanned it recently.
2015-11-24 19:10:49,275 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 for rescanning, because we rescanned it recently.
2015-11-24 19:10:49,299 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 for rescanning, because we rescanned it recently.
2015-11-24 19:10:49,333 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442 for rescanning.
2015-11-24 19:10:49,365 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443 for rescanning.
2015-11-24 19:10:49,372 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 for rescanning, because we rescanned it recently.
2015-11-24 19:10:49,410 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 for rescanning, because we rescanned it recently.
2015-11-24 19:10:49,422 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446 for rescanning.
2015-11-24 19:10:49,469 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447 for rescanning.
2015-11-24 19:10:49,509 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448 for rescanning.
2015-11-24 19:10:49,550 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449 for rescanning.
2015-11-24 19:10:49,560 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450 for rescanning.
2015-11-24 19:10:49,625 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451 for rescanning.
2015-11-24 19:10:49,648 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452 for rescanning.
2015-11-24 19:10:49,683 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453 for rescanning.
2015-11-24 19:10:49,699 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454 for rescanning.
2015-11-24 19:10:49,759 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455 for rescanning.
2015-11-24 19:10:49,783 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456 for rescanning.
2015-11-24 19:10:49,847 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457 for rescanning.
2015-11-24 19:10:49,875 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458 for rescanning.
2015-11-24 19:10:49,909 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459 for rescanning.
2015-11-24 19:10:49,933 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460 for rescanning.
2015-11-24 19:10:49,992 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461 for rescanning.
2015-11-24 19:10:50,009 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462 for rescanning.
2015-11-24 19:10:50,050 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463 for rescanning.
2015-11-24 19:10:50,067 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464 for rescanning.
2015-11-24 19:10:50,108 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465 for rescanning.
2015-11-24 19:10:50,127 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466 for rescanning.
2015-11-24 19:10:50,170 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467 for rescanning.
2015-11-24 19:10:50,182 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468 for rescanning.
2015-11-24 19:10:50,225 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469 for rescanning.
2015-11-24 19:10:50,245 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470 for rescanning.
2015-11-24 19:10:50,309 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472 for rescanning.
2015-11-24 19:10:50,349 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473 for rescanning.
2015-11-24 19:10:50,371 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474 for rescanning.
2015-11-24 19:10:50,433 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471 for rescanning.
2015-11-24 19:10:50,443 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475 for rescanning.
2015-11-24 19:10:50,463 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476 for rescanning.
2015-11-24 19:10:50,507 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477 for rescanning.
2015-11-24 19:10:50,538 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478 for rescanning.
2015-11-24 19:10:50,604 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479 for rescanning.
2015-11-24 19:10:50,632 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480 for rescanning.
2015-11-24 19:10:50,672 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481 for rescanning.
2015-11-24 19:10:50,688 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482 for rescanning.
2015-11-24 19:10:50,728 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483 for rescanning.
2015-11-24 19:10:50,745 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484 for rescanning.
2015-11-24 19:10:50,789 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485 for rescanning.
2015-11-24 19:10:50,809 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486 for rescanning.
2015-11-24 19:10:50,847 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487 for rescanning.
2015-11-24 19:10:50,861 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488 for rescanning.
2015-11-24 19:10:50,895 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489 for rescanning.
2015-11-24 19:10:50,909 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490 for rescanning.
2015-11-24 19:10:50,941 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491 for rescanning.
2015-11-24 19:10:50,954 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492 for rescanning.
2015-11-24 19:10:50,986 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493 for rescanning.
2015-11-24 19:10:51,003 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494 for rescanning.
2015-11-24 19:10:51,032 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495 for rescanning.
2015-11-24 19:10:51,045 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496 for rescanning.
2015-11-24 19:10:51,091 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498 for rescanning.
2015-11-24 19:10:51,123 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 for rescanning.
2015-11-24 19:10:51,149 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497 for rescanning.
2015-11-24 19:10:51,175 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500 for rescanning.
2015-11-24 19:10:51,213 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501 for rescanning.
2015-11-24 19:10:51,238 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502 for rescanning.
2015-11-24 19:10:51,276 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503 for rescanning.
2015-11-24 19:10:51,333 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504 for rescanning.
2015-11-24 19:10:51,375 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505 for rescanning.
2015-11-24 19:10:51,401 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506 for rescanning.
2015-11-24 19:10:51,435 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507 for rescanning.
2015-11-24 19:10:51,449 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508 for rescanning.
2015-11-24 19:10:51,481 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509 for rescanning.
2015-11-24 19:10:51,494 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 for rescanning.
2015-11-24 19:10:51,527 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511 for rescanning.
2015-11-24 19:10:51,540 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512 for rescanning.
2015-11-24 19:10:51,574 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513 for rescanning.
2015-11-24 19:10:51,588 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514 for rescanning.
2015-11-24 19:10:51,652 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515 for rescanning.
2015-11-24 19:10:51,682 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 for rescanning.
2015-11-24 19:10:51,714 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517 for rescanning.
2015-11-24 19:10:51,727 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518 for rescanning.
2015-11-24 19:10:51,762 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519 for rescanning.
2015-11-24 19:10:51,771 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520 for rescanning.
2015-11-24 19:10:51,818 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521 for rescanning.
2015-11-24 19:10:51,835 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522 for rescanning.
2015-11-24 19:10:51,871 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 for rescanning.
2015-11-24 19:10:51,883 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 for rescanning.
2015-11-24 19:10:51,942 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 for rescanning.
2015-11-24 19:10:51,970 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526 for rescanning.
2015-11-24 19:11:31,205 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 is already queued for rescanning.
2015-11-24 19:11:31,208 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 is already queued for rescanning.
2015-11-24 19:11:31,250 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 is already queued for rescanning.
2015-11-24 19:11:31,264 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 is already queued for rescanning.
2015-11-24 19:11:31,327 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 is already queued for rescanning.
2015-11-24 19:11:31,332 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 is already queued for rescanning.
2015-11-24 19:11:31,398 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 is already queued for rescanning.
2015-11-24 19:11:31,403 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 is already queued for rescanning.
2015-11-24 19:11:31,469 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 is already queued for rescanning.
2015-11-24 19:11:31,474 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 is already queued for rescanning.
2015-11-24 19:11:31,532 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 is already queued for rescanning.
2015-11-24 19:11:31,561 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 is already queued for rescanning.
2015-11-24 19:11:31,599 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 is already queued for rescanning.
2015-11-24 19:11:31,604 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 is already queued for rescanning.
2015-11-24 19:11:31,658 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 is already queued for rescanning.
2015-11-24 19:11:31,674 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 is already queued for rescanning.
2015-11-24 19:11:31,717 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 is already queued for rescanning.
2015-11-24 19:11:31,722 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 is already queued for rescanning.
2015-11-24 19:11:31,770 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 is already queued for rescanning.
2015-11-24 19:11:31,775 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 is already queued for rescanning.
2015-11-24 19:11:31,829 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 is already queued for rescanning.
2015-11-24 19:11:31,839 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 is already queued for rescanning.
2015-11-24 19:11:31,873 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 is already queued for rescanning.
2015-11-24 19:11:31,878 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 is already queued for rescanning.
2015-11-24 19:11:31,921 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 is already queued for rescanning.
2015-11-24 19:11:31,925 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 is already queued for rescanning.
2015-11-24 19:11:31,964 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 is already queued for rescanning.
2015-11-24 19:11:31,970 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 is already queued for rescanning.
2015-11-24 19:11:32,014 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 is already queued for rescanning.
2015-11-24 19:11:32,019 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 is already queued for rescanning.
2015-11-24 19:11:32,085 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 is already queued for rescanning.
2015-11-24 19:11:32,089 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 is already queued for rescanning.
2015-11-24 19:11:32,131 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 is already queued for rescanning.
2015-11-24 19:11:32,135 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 is already queued for rescanning.
2015-11-24 19:11:32,180 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 is already queued for rescanning.
2015-11-24 19:11:32,186 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 is already queued for rescanning.
2015-11-24 19:11:32,235 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 is already queued for rescanning.
2015-11-24 19:11:32,240 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 is already queued for rescanning.
2015-11-24 19:11:32,291 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 is already queued for rescanning.
2015-11-24 19:11:32,296 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 is already queued for rescanning.
2015-11-24 19:11:32,342 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 is already queued for rescanning.
2015-11-24 19:11:32,347 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 is already queued for rescanning.
2015-11-24 19:11:32,396 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,400 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,446 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,451 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,493 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,497 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,557 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,561 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,608 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,613 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,662 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,667 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,723 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,727 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,800 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,825 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,864 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,868 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,931 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,936 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458 for rescanning, because we rescanned it recently.
2015-11-24 19:11:32,982 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,014 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,039 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,064 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,105 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,109 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,149 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,154 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,204 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,208 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,257 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,261 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,309 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,314 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,362 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,366 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,427 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,431 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,480 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,484 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,533 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,537 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,582 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,586 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,643 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,648 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,705 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,711 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,762 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,782 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,810 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,829 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,865 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,869 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,926 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,931 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,979 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495 for rescanning, because we rescanned it recently.
2015-11-24 19:11:33,983 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,034 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,059 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,092 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,124 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,132 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,155 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,184 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,226 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,229 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,233 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,279 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,284 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,327 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,332 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,390 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,395 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,443 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,447 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,492 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,496 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,549 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,576 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,597 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,601 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,650 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,654 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,699 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,703 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,750 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 for rescanning, because we rescanned it recently.
2015-11-24 19:11:34,754 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526 for rescanning, because we rescanned it recently.
2015-11-24 19:11:43,691 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 is already queued for rescanning.
2015-11-24 19:11:43,694 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 is already queued for rescanning.
2015-11-24 19:11:44,000 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 is already queued for rescanning.
2015-11-24 19:11:44,003 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 is already queued for rescanning.
2015-11-24 19:11:44,312 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 is already queued for rescanning.
2015-11-24 19:11:44,317 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 is already queued for rescanning.
2015-11-24 19:11:44,634 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 is already queued for rescanning.
2015-11-24 19:11:44,638 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 is already queued for rescanning.
2015-11-24 19:11:44,950 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 is already queued for rescanning.
2015-11-24 19:11:44,953 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 is already queued for rescanning.
2015-11-24 19:11:45,265 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 is already queued for rescanning.
2015-11-24 19:11:45,270 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 is already queued for rescanning.
2015-11-24 19:11:45,594 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 is already queued for rescanning.
2015-11-24 19:11:45,599 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 is already queued for rescanning.
2015-11-24 19:11:45,919 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 is already queued for rescanning.
2015-11-24 19:11:45,924 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 is already queued for rescanning.
2015-11-24 19:11:46,233 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 is already queued for rescanning.
2015-11-24 19:11:46,237 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 is already queued for rescanning.
2015-11-24 19:11:46,582 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 is already queued for rescanning.
2015-11-24 19:11:46,587 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 is already queued for rescanning.
2015-11-24 19:11:46,913 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 is already queued for rescanning.
2015-11-24 19:11:46,918 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 is already queued for rescanning.
2015-11-24 19:11:47,210 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 is already queued for rescanning.
2015-11-24 19:11:47,213 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 is already queued for rescanning.
2015-11-24 19:11:47,526 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 is already queued for rescanning.
2015-11-24 19:11:47,530 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 is already queued for rescanning.
2015-11-24 19:11:47,886 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 is already queued for rescanning.
2015-11-24 19:11:47,892 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 is already queued for rescanning.
2015-11-24 19:11:48,252 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 is already queued for rescanning.
2015-11-24 19:11:48,256 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 is already queued for rescanning.
2015-11-24 19:11:48,570 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 is already queued for rescanning.
2015-11-24 19:11:48,574 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 is already queued for rescanning.
2015-11-24 19:11:48,887 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 is already queued for rescanning.
2015-11-24 19:11:48,893 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 is already queued for rescanning.
2015-11-24 19:11:49,211 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 is already queued for rescanning.
2015-11-24 19:11:49,216 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 is already queued for rescanning.
2015-11-24 19:11:49,526 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 is already queued for rescanning.
2015-11-24 19:11:49,530 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 is already queued for rescanning.
2015-11-24 19:11:49,847 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 is already queued for rescanning.
2015-11-24 19:11:49,852 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 is already queued for rescanning.
2015-11-24 19:11:50,180 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 is already queued for rescanning.
2015-11-24 19:11:50,184 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 is already queued for rescanning.
2015-11-24 19:11:50,495 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 for rescanning, because we rescanned it recently.
2015-11-24 19:11:50,499 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 for rescanning, because we rescanned it recently.
2015-11-24 19:11:50,829 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 for rescanning, because we rescanned it recently.
2015-11-24 19:11:50,834 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442 for rescanning, because we rescanned it recently.
2015-11-24 19:11:51,139 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443 for rescanning, because we rescanned it recently.
2015-11-24 19:11:51,143 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 for rescanning, because we rescanned it recently.
2015-11-24 19:11:51,463 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 for rescanning, because we rescanned it recently.
2015-11-24 19:11:51,468 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446 for rescanning, because we rescanned it recently.
2015-11-24 19:11:51,813 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447 for rescanning, because we rescanned it recently.
2015-11-24 19:11:51,818 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448 for rescanning, because we rescanned it recently.
2015-11-24 19:11:52,150 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449 for rescanning, because we rescanned it recently.
2015-11-24 19:11:52,154 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450 for rescanning, because we rescanned it recently.
2015-11-24 19:11:52,477 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451 for rescanning, because we rescanned it recently.
2015-11-24 19:11:52,482 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452 for rescanning, because we rescanned it recently.
2015-11-24 19:11:52,834 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453 for rescanning, because we rescanned it recently.
2015-11-24 19:11:52,838 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454 for rescanning, because we rescanned it recently.
2015-11-24 19:11:53,143 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455 for rescanning, because we rescanned it recently.
2015-11-24 19:11:53,146 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456 for rescanning, because we rescanned it recently.
2015-11-24 19:11:53,486 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457 for rescanning, because we rescanned it recently.
2015-11-24 19:11:53,490 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458 for rescanning, because we rescanned it recently.
2015-11-24 19:11:53,804 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459 for rescanning, because we rescanned it recently.
2015-11-24 19:11:53,808 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460 for rescanning, because we rescanned it recently.
2015-11-24 19:11:54,133 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461 for rescanning, because we rescanned it recently.
2015-11-24 19:11:54,138 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462 for rescanning, because we rescanned it recently.
2015-11-24 19:11:54,474 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463 for rescanning, because we rescanned it recently.
2015-11-24 19:11:54,479 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464 for rescanning, because we rescanned it recently.
2015-11-24 19:11:54,815 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465 for rescanning, because we rescanned it recently.
2015-11-24 19:11:54,819 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466 for rescanning, because we rescanned it recently.
2015-11-24 19:11:55,135 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467 for rescanning, because we rescanned it recently.
2015-11-24 19:11:55,139 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468 for rescanning, because we rescanned it recently.
2015-11-24 19:11:55,439 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469 for rescanning, because we rescanned it recently.
2015-11-24 19:11:55,444 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470 for rescanning, because we rescanned it recently.
2015-11-24 19:11:55,736 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471 for rescanning, because we rescanned it recently.
2015-11-24 19:11:55,741 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472 for rescanning, because we rescanned it recently.
2015-11-24 19:11:56,049 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473 for rescanning, because we rescanned it recently.
2015-11-24 19:11:56,057 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474 for rescanning, because we rescanned it recently.
2015-11-24 19:11:56,425 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475 for rescanning, because we rescanned it recently.
2015-11-24 19:11:56,430 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476 for rescanning, because we rescanned it recently.
2015-11-24 19:11:56,742 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477 for rescanning, because we rescanned it recently.
2015-11-24 19:11:56,745 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478 for rescanning, because we rescanned it recently.
2015-11-24 19:11:57,049 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479 for rescanning, because we rescanned it recently.
2015-11-24 19:11:57,054 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480 for rescanning, because we rescanned it recently.
2015-11-24 19:11:57,380 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481 for rescanning, because we rescanned it recently.
2015-11-24 19:11:57,384 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482 for rescanning, because we rescanned it recently.
2015-11-24 19:11:57,722 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483 for rescanning, because we rescanned it recently.
2015-11-24 19:11:57,727 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484 for rescanning, because we rescanned it recently.
2015-11-24 19:11:58,091 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485 for rescanning, because we rescanned it recently.
2015-11-24 19:11:58,097 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486 for rescanning, because we rescanned it recently.
2015-11-24 19:11:58,391 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487 for rescanning, because we rescanned it recently.
2015-11-24 19:11:58,395 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488 for rescanning, because we rescanned it recently.
2015-11-24 19:11:58,718 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489 for rescanning, because we rescanned it recently.
2015-11-24 19:11:58,723 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490 for rescanning, because we rescanned it recently.
2015-11-24 19:11:59,039 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491 for rescanning, because we rescanned it recently.
2015-11-24 19:11:59,045 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492 for rescanning, because we rescanned it recently.
2015-11-24 19:11:59,379 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493 for rescanning, because we rescanned it recently.
2015-11-24 19:11:59,384 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494 for rescanning, because we rescanned it recently.
2015-11-24 19:11:59,693 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495 for rescanning, because we rescanned it recently.
2015-11-24 19:11:59,697 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496 for rescanning, because we rescanned it recently.
2015-11-24 19:12:00,064 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497 for rescanning, because we rescanned it recently.
2015-11-24 19:12:00,069 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498 for rescanning, because we rescanned it recently.
2015-11-24 19:12:00,410 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 for rescanning, because we rescanned it recently.
2015-11-24 19:12:00,415 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500 for rescanning, because we rescanned it recently.
2015-11-24 19:12:00,743 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501 for rescanning, because we rescanned it recently.
2015-11-24 19:12:00,747 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502 for rescanning, because we rescanned it recently.
2015-11-24 19:12:01,075 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503 for rescanning, because we rescanned it recently.
2015-11-24 19:12:01,080 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504 for rescanning, because we rescanned it recently.
2015-11-24 19:12:01,399 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505 for rescanning, because we rescanned it recently.
2015-11-24 19:12:01,428 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506 for rescanning, because we rescanned it recently.
2015-11-24 19:12:01,697 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507 for rescanning, because we rescanned it recently.
2015-11-24 19:12:01,701 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508 for rescanning, because we rescanned it recently.
2015-11-24 19:12:02,022 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509 for rescanning, because we rescanned it recently.
2015-11-24 19:12:02,027 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 for rescanning, because we rescanned it recently.
2015-11-24 19:12:02,377 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511 for rescanning, because we rescanned it recently.
2015-11-24 19:12:02,381 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512 for rescanning, because we rescanned it recently.
2015-11-24 19:12:02,697 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513 for rescanning, because we rescanned it recently.
2015-11-24 19:12:02,703 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514 for rescanning, because we rescanned it recently.
2015-11-24 19:12:03,028 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515 for rescanning, because we rescanned it recently.
2015-11-24 19:12:03,033 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 for rescanning, because we rescanned it recently.
2015-11-24 19:12:03,369 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517 for rescanning, because we rescanned it recently.
2015-11-24 19:12:03,374 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518 for rescanning, because we rescanned it recently.
2015-11-24 19:12:03,690 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519 for rescanning, because we rescanned it recently.
2015-11-24 19:12:03,704 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520 for rescanning, because we rescanned it recently.
2015-11-24 19:12:04,053 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521 for rescanning, because we rescanned it recently.
2015-11-24 19:12:04,058 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522 for rescanning, because we rescanned it recently.
2015-11-24 19:12:04,400 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 for rescanning, because we rescanned it recently.
2015-11-24 19:12:04,404 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 for rescanning, because we rescanned it recently.
2015-11-24 19:12:04,731 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 for rescanning, because we rescanned it recently.
2015-11-24 19:12:04,735 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526 for rescanning, because we rescanned it recently.
2015-11-24 19:13:01,728 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 is already queued for rescanning.
2015-11-24 19:13:15,238 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 is already queued for rescanning.
2015-11-24 19:13:28,386 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-24 19:13:32,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-24 19:13:33,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-24 19:13:33,365 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-24 19:13:33,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-24 19:14:30,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-24 19:14:30,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-24 19:14:31,073 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-24 19:14:31,137 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-24 19:14:31,137 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-24 19:14:31,142 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-24 19:14:31,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-24 19:14:31,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-24 19:14:31,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-24 19:14:31,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-24 19:14:31,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-24 19:14:31,263 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-24 19:14:31,271 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-24 19:14:31,276 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-24 19:14:31,281 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-24 19:14:31,284 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-24 19:14:31,284 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-24 19:14:31,284 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-24 19:14:31,294 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 46775
2015-11-24 19:14:31,294 INFO org.mortbay.log: jetty-6.1.26
2015-11-24 19:14:31,443 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46775
2015-11-24 19:14:31,642 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-24 19:14:31,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-24 19:14:31,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-24 19:14:31,706 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-24 19:14:31,726 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-24 19:14:31,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-24 19:14:31,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-24 19:14:31,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-24 19:14:31,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-24 19:14:31,830 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-24 19:14:31,831 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-24 19:14:32,190 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 7625@rushikesh1
2015-11-24 19:14:32,289 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-24 19:14:32,289 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-24 19:14:32,290 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-24 19:14:32,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-24 19:14:32,379 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-24 19:14:32,379 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-24 19:14:32,406 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-24 19:14:32,406 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-24 19:14:32,407 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-24 19:14:32,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35186042737
2015-11-24 19:14:32,414 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-24 19:14:32,414 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-24 19:14:32,415 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-24 19:14:32,452 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 37ms
2015-11-24 19:14:32,452 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 38ms
2015-11-24 19:14:32,677 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1178335701 ms.
2015-11-24 19:14:32,679 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448384517679 with interval 21600000
2015-11-24 19:14:32,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-24 19:14:32,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-24 19:14:32,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-24 19:14:32,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1960
2015-11-24 19:14:32,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-24 19:14:32,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x16452b1b5219,  containing 1 storage report(s), of which we sent 1. The reports had 265 total blocks and used 1 RPC(s). This took 6 msec to generate and 66 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-24 19:14:32,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-24 19:15:52,823 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-24 19:15:55,106 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-24 19:15:55,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-25 14:05:50,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-25 14:05:50,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-25 14:05:51,171 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-25 14:05:51,234 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-25 14:05:51,234 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-25 14:05:51,238 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-25 14:05:51,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-25 14:05:51,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-25 14:05:51,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-25 14:05:51,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-25 14:05:51,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-25 14:05:51,399 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-25 14:05:51,406 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-25 14:05:51,412 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-25 14:05:51,417 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-25 14:05:51,419 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-25 14:05:51,419 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-25 14:05:51,419 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-25 14:05:51,429 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 51500
2015-11-25 14:05:51,429 INFO org.mortbay.log: jetty-6.1.26
2015-11-25 14:05:51,587 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:51500
2015-11-25 14:05:51,701 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-25 14:05:51,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-25 14:05:51,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-25 14:05:51,769 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-25 14:05:51,786 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-25 14:05:51,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-25 14:05:51,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-25 14:05:51,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-25 14:05:51,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-25 14:05:51,909 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-25 14:05:51,909 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-25 14:05:52,390 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 21776@rushikesh1
2015-11-25 14:05:52,479 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-25 14:05:52,479 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-25 14:05:52,480 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-25 14:05:52,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-25 14:05:52,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-25 14:05:52,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-25 14:05:52,655 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-25 14:05:52,656 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 14:05:52,656 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 14:05:52,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 58ms
2015-11-25 14:05:52,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 58ms
2015-11-25 14:05:52,715 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 14:05:52,817 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 102ms
2015-11-25 14:05:52,818 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 103ms
2015-11-25 14:05:53,154 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1110455224 ms.
2015-11-25 14:05:53,157 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448445724157 with interval 21600000
2015-11-25 14:05:53,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-25 14:05:53,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-25 14:05:53,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-25 14:05:53,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1964
2015-11-25 14:05:53,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-25 14:05:53,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1c24375abc17,  containing 1 storage report(s), of which we sent 1. The reports had 265 total blocks and used 1 RPC(s). This took 7 msec to generate and 96 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-25 14:05:53,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 14:06:41,251 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 for rescanning.
2015-11-25 14:06:41,252 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-11-25 14:06:41,254 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting.
2015-11-25 14:06:41,329 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 for rescanning.
2015-11-25 14:06:41,443 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 for rescanning.
2015-11-25 14:06:41,507 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 for rescanning.
2015-11-25 14:06:41,627 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 for rescanning.
2015-11-25 14:06:41,686 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 for rescanning.
2015-11-25 14:06:41,791 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 for rescanning.
2015-11-25 14:06:41,855 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 for rescanning.
2015-11-25 14:06:41,950 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 for rescanning.
2015-11-25 14:06:42,018 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 for rescanning.
2015-11-25 14:06:42,095 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 for rescanning.
2015-11-25 14:06:42,172 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 for rescanning.
2015-11-25 14:06:42,293 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 for rescanning.
2015-11-25 14:06:42,323 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 for rescanning.
2015-11-25 14:06:42,423 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 for rescanning.
2015-11-25 14:06:42,471 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 for rescanning.
2015-11-25 14:06:42,557 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 for rescanning.
2015-11-25 14:06:42,629 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 for rescanning.
2015-11-25 14:06:42,680 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 for rescanning.
2015-11-25 14:06:42,746 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 for rescanning.
2015-11-25 14:06:42,861 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 for rescanning.
2015-11-25 14:06:42,925 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 for rescanning.
2015-11-25 14:06:43,027 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 for rescanning.
2015-11-25 14:06:43,092 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 for rescanning.
2015-11-25 14:06:43,155 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 for rescanning.
2015-11-25 14:06:43,284 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 for rescanning.
2015-11-25 14:06:43,531 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 for rescanning.
2015-11-25 14:06:43,642 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 for rescanning.
2015-11-25 14:06:43,743 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 for rescanning.
2015-11-25 14:06:43,769 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 for rescanning.
2015-11-25 14:06:43,858 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 for rescanning.
2015-11-25 14:06:43,922 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 for rescanning.
2015-11-25 14:06:43,988 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 for rescanning.
2015-11-25 14:06:44,063 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 for rescanning.
2015-11-25 14:06:44,216 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 for rescanning.
2015-11-25 14:06:44,276 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 for rescanning.
2015-11-25 14:06:44,409 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 for rescanning.
2015-11-25 14:06:44,515 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 for rescanning.
2015-11-25 14:06:44,679 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 for rescanning.
2015-11-25 14:06:44,789 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 for rescanning.
2015-11-25 14:06:44,913 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 for rescanning.
2015-11-25 14:06:44,963 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 for rescanning.
2015-11-25 14:06:45,084 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 for rescanning.
2015-11-25 14:06:45,155 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 for rescanning.
2015-11-25 14:06:45,226 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 for rescanning.
2015-11-25 14:06:45,286 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442 for rescanning.
2015-11-25 14:06:45,386 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443 for rescanning.
2015-11-25 14:06:45,453 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 for rescanning.
2015-11-25 14:06:45,538 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 for rescanning.
2015-11-25 14:06:45,592 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446 for rescanning.
2015-11-25 14:06:45,674 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447 for rescanning.
2015-11-25 14:06:45,743 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448 for rescanning.
2015-11-25 14:06:45,898 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449 for rescanning.
2015-11-25 14:06:46,013 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450 for rescanning.
2015-11-25 14:06:46,146 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451 for rescanning.
2015-11-25 14:06:46,215 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452 for rescanning.
2015-11-25 14:06:46,304 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453 for rescanning.
2015-11-25 14:06:46,366 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454 for rescanning.
2015-11-25 14:06:46,461 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455 for rescanning.
2015-11-25 14:06:46,527 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456 for rescanning.
2015-11-25 14:06:46,646 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457 for rescanning.
2015-11-25 14:06:46,709 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458 for rescanning.
2015-11-25 14:06:46,861 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459 for rescanning.
2015-11-25 14:06:46,917 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460 for rescanning.
2015-11-25 14:06:46,997 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461 for rescanning.
2015-11-25 14:06:47,046 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462 for rescanning.
2015-11-25 14:06:47,146 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463 for rescanning.
2015-11-25 14:06:47,193 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464 for rescanning.
2015-11-25 14:06:47,280 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465 for rescanning.
2015-11-25 14:06:47,337 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466 for rescanning.
2015-11-25 14:06:47,430 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467 for rescanning.
2015-11-25 14:06:47,491 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468 for rescanning.
2015-11-25 14:06:47,566 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469 for rescanning.
2015-11-25 14:06:47,663 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470 for rescanning.
2015-11-25 14:06:47,755 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471 for rescanning.
2015-11-25 14:06:47,819 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472 for rescanning.
2015-11-25 14:06:48,291 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473 for rescanning.
2015-11-25 14:06:48,346 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474 for rescanning.
2015-11-25 14:06:48,427 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475 for rescanning.
2015-11-25 14:06:48,471 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476 for rescanning.
2015-11-25 14:06:48,547 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477 for rescanning.
2015-11-25 14:06:48,628 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478 for rescanning.
2015-11-25 14:06:48,724 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479 for rescanning.
2015-11-25 14:06:48,749 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480 for rescanning.
2015-11-25 14:06:48,856 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481 for rescanning.
2015-11-25 14:06:48,921 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482 for rescanning.
2015-11-25 14:06:49,030 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483 for rescanning.
2015-11-25 14:06:49,094 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484 for rescanning.
2015-11-25 14:06:49,211 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485 for rescanning.
2015-11-25 14:06:49,277 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486 for rescanning.
2015-11-25 14:06:49,357 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487 for rescanning.
2015-11-25 14:06:49,476 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488 for rescanning.
2015-11-25 14:06:49,595 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489 for rescanning.
2015-11-25 14:06:49,657 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490 for rescanning.
2015-11-25 14:06:49,744 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491 for rescanning.
2015-11-25 14:06:49,810 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492 for rescanning.
2015-11-25 14:06:49,904 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493 for rescanning.
2015-11-25 14:06:49,969 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494 for rescanning.
2015-11-25 14:06:50,024 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495 for rescanning.
2015-11-25 14:06:50,085 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496 for rescanning.
2015-11-25 14:06:50,181 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497 for rescanning.
2015-11-25 14:06:50,247 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498 for rescanning.
2015-11-25 14:06:50,340 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 for rescanning.
2015-11-25 14:06:50,364 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500 for rescanning.
2015-11-25 14:06:50,447 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501 for rescanning.
2015-11-25 14:06:50,511 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502 for rescanning.
2015-11-25 14:06:50,629 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503 for rescanning.
2015-11-25 14:06:50,698 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504 for rescanning.
2015-11-25 14:06:50,778 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505 for rescanning.
2015-11-25 14:06:50,840 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506 for rescanning.
2015-11-25 14:06:50,922 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507 for rescanning.
2015-11-25 14:06:50,991 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508 for rescanning.
2015-11-25 14:06:51,073 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509 for rescanning.
2015-11-25 14:06:51,133 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 for rescanning.
2015-11-25 14:06:51,214 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511 for rescanning.
2015-11-25 14:06:51,280 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512 for rescanning.
2015-11-25 14:06:51,362 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513 for rescanning.
2015-11-25 14:06:51,429 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514 for rescanning.
2015-11-25 14:06:51,591 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515 for rescanning.
2015-11-25 14:06:51,654 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 for rescanning.
2015-11-25 14:06:51,745 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517 for rescanning.
2015-11-25 14:06:51,799 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518 for rescanning.
2015-11-25 14:06:51,888 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519 for rescanning.
2015-11-25 14:06:51,960 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520 for rescanning.
2015-11-25 14:06:52,073 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521 for rescanning.
2015-11-25 14:06:52,148 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522 for rescanning.
2015-11-25 14:06:52,265 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 for rescanning.
2015-11-25 14:06:52,334 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 for rescanning.
2015-11-25 14:06:52,431 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 for rescanning.
2015-11-25 14:06:52,527 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526 for rescanning.
2015-11-25 14:08:58,937 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 for rescanning, because we rescanned it recently.
2015-11-25 14:10:36,735 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 for rescanning, because we rescanned it recently.
2015-11-25 14:12:14,786 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 for rescanning, because we rescanned it recently.
2015-11-25 14:13:53,065 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 for rescanning, because we rescanned it recently.
2015-11-25 14:15:30,919 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 for rescanning, because we rescanned it recently.
2015-11-25 14:17:08,978 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 is already queued for rescanning.
2015-11-25 14:18:46,711 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 is already queued for rescanning.
2015-11-25 14:20:24,850 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 is already queued for rescanning.
2015-11-25 14:22:03,167 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 is already queued for rescanning.
2015-11-25 14:23:41,806 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 is already queued for rescanning.
2015-11-25 14:25:19,150 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 is already queued for rescanning.
2015-11-25 14:26:56,234 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 is already queued for rescanning.
2015-11-25 14:28:33,325 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 is already queued for rescanning.
2015-11-25 14:30:10,403 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 is already queued for rescanning.
2015-11-25 14:31:48,101 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 is already queued for rescanning.
2015-11-25 14:33:26,183 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 is already queued for rescanning.
2015-11-25 14:35:03,715 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 is already queued for rescanning.
2015-11-25 14:36:41,051 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 is already queued for rescanning.
2015-11-25 14:38:18,460 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 is already queued for rescanning.
2015-11-25 14:39:55,436 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 is already queued for rescanning.
2015-11-25 14:41:32,990 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 is already queued for rescanning.
2015-11-25 14:43:09,993 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 is already queued for rescanning.
2015-11-25 14:44:47,238 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 is already queued for rescanning.
2015-11-25 14:46:24,270 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 is already queued for rescanning.
2015-11-25 14:48:01,499 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 is already queued for rescanning.
2015-11-25 14:49:38,621 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 is already queued for rescanning.
2015-11-25 14:51:15,659 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 is already queued for rescanning.
2015-11-25 14:52:52,956 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 is already queued for rescanning.
2015-11-25 14:54:30,207 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 is already queued for rescanning.
2015-11-25 14:56:07,264 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 is already queued for rescanning.
2015-11-25 14:57:44,269 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 is already queued for rescanning.
2015-11-25 14:59:20,988 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 is already queued for rescanning.
2015-11-25 15:00:58,104 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 is already queued for rescanning.
2015-11-25 15:02:35,365 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 is already queued for rescanning.
2015-11-25 15:04:12,354 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 is already queued for rescanning.
2015-11-25 15:04:46,460 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 is already queued for rescanning.
2015-11-25 15:07:27,722 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 is already queued for rescanning.
2015-11-25 15:09:05,056 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 is already queued for rescanning.
2015-11-25 15:10:42,218 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 is already queued for rescanning.
2015-11-25 15:12:19,812 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 is already queued for rescanning.
2015-11-25 15:13:56,963 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 is already queued for rescanning.
2015-11-25 15:15:34,257 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 is already queued for rescanning.
2015-11-25 15:17:11,291 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 is already queued for rescanning.
2015-11-25 15:18:48,096 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 is already queued for rescanning.
2015-11-25 15:20:25,290 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442 is already queued for rescanning.
2015-11-25 15:22:02,232 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443 is already queued for rescanning.
2015-11-25 15:23:39,967 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 is already queued for rescanning.
2015-11-25 15:25:17,581 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 is already queued for rescanning.
2015-11-25 15:26:54,508 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446 is already queued for rescanning.
2015-11-25 15:28:31,366 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447 is already queued for rescanning.
2015-11-25 15:30:08,629 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448 is already queued for rescanning.
2015-11-25 15:31:45,898 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449 is already queued for rescanning.
2015-11-25 15:32:04,200 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 262, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-25 15:33:22,725 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450 is already queued for rescanning.
2015-11-25 15:34:59,851 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451 is already queued for rescanning.
2015-11-25 15:36:38,449 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452 is already queued for rescanning.
2015-11-25 15:38:16,477 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453 is already queued for rescanning.
2015-11-25 15:39:53,457 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454 is already queued for rescanning.
2015-11-25 15:41:30,819 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455 is already queued for rescanning.
2015-11-25 15:43:08,658 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456 is already queued for rescanning.
2015-11-25 15:44:46,620 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457 is already queued for rescanning.
2015-11-25 15:46:24,864 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458 is already queued for rescanning.
2015-11-25 15:48:03,309 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459 is already queued for rescanning.
2015-11-25 15:49:40,920 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460 is already queued for rescanning.
2015-11-25 15:51:18,321 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461 is already queued for rescanning.
2015-11-25 15:52:55,399 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462 is already queued for rescanning.
2015-11-25 15:54:32,595 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463 is already queued for rescanning.
2015-11-25 15:56:11,555 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464 is already queued for rescanning.
2015-11-25 15:57:50,617 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465 is already queued for rescanning.
2015-11-25 15:59:28,361 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466 is already queued for rescanning.
2015-11-25 16:01:05,935 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467 is already queued for rescanning.
2015-11-25 16:02:42,899 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468 is already queued for rescanning.
2015-11-25 16:04:19,838 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469 is already queued for rescanning.
2015-11-25 16:05:57,125 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470 is already queued for rescanning.
2015-11-25 16:07:34,202 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471 is already queued for rescanning.
2015-11-25 16:09:11,195 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472 is already queued for rescanning.
2015-11-25 16:10:48,037 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473 is already queued for rescanning.
2015-11-25 16:12:24,833 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474 is already queued for rescanning.
2015-11-25 16:14:02,030 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475 is already queued for rescanning.
2015-11-25 16:15:39,082 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476 is already queued for rescanning.
2015-11-25 16:17:16,034 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477 is already queued for rescanning.
2015-11-25 16:18:53,108 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478 is already queued for rescanning.
2015-11-25 16:20:30,334 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479 is already queued for rescanning.
2015-11-25 16:22:07,309 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480 is already queued for rescanning.
2015-11-25 16:23:44,158 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481 is already queued for rescanning.
2015-11-25 16:25:21,391 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482 is already queued for rescanning.
2015-11-25 16:26:58,598 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483 is already queued for rescanning.
2015-11-25 16:28:34,951 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484 is already queued for rescanning.
2015-11-25 16:30:11,337 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485 is already queued for rescanning.
2015-11-25 16:31:47,298 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486 is already queued for rescanning.
2015-11-25 16:33:23,410 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487 is already queued for rescanning.
2015-11-25 16:34:59,460 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488 is already queued for rescanning.
2015-11-25 16:36:35,767 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489 is already queued for rescanning.
2015-11-25 16:38:11,830 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490 is already queued for rescanning.
2015-11-25 16:39:47,860 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491 is already queued for rescanning.
2015-11-25 16:41:23,837 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492 is already queued for rescanning.
2015-11-25 16:42:59,794 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493 is already queued for rescanning.
2015-11-25 16:44:36,126 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494 is already queued for rescanning.
2015-11-25 16:46:12,153 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495 is already queued for rescanning.
2015-11-25 16:47:48,337 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496 is already queued for rescanning.
2015-11-25 16:49:24,320 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497 is already queued for rescanning.
2015-11-25 16:51:00,606 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498 is already queued for rescanning.
2015-11-25 16:52:36,802 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 is already queued for rescanning.
2015-11-25 16:53:00,851 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 is already queued for rescanning.
2015-11-25 16:55:50,037 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501 is already queued for rescanning.
2015-11-25 16:57:26,211 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502 is already queued for rescanning.
2015-11-25 16:59:02,439 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503 is already queued for rescanning.
2015-11-25 17:00:38,657 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504 is already queued for rescanning.
2015-11-25 17:02:15,002 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505 is already queued for rescanning.
2015-11-25 17:03:51,290 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506 is already queued for rescanning.
2015-11-25 17:05:27,546 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507 is already queued for rescanning.
2015-11-25 17:07:03,827 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508 is already queued for rescanning.
2015-11-25 17:08:39,665 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509 is already queued for rescanning.
2015-11-25 17:10:16,072 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 is already queued for rescanning.
2015-11-25 17:11:52,522 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511 is already queued for rescanning.
2015-11-25 17:13:28,576 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512 is already queued for rescanning.
2015-11-25 17:15:04,910 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513 is already queued for rescanning.
2015-11-25 17:16:41,262 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514 is already queued for rescanning.
2015-11-25 17:18:17,192 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515 is already queued for rescanning.
2015-11-25 17:19:53,636 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 is already queued for rescanning.
2015-11-25 17:21:30,043 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517 is already queued for rescanning.
2015-11-25 17:23:06,471 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518 is already queued for rescanning.
2015-11-25 17:24:43,301 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519 is already queued for rescanning.
2015-11-25 17:26:19,945 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520 is already queued for rescanning.
2015-11-25 17:27:56,355 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521 is already queued for rescanning.
2015-11-25 17:29:32,557 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522 is already queued for rescanning.
2015-11-25 17:31:08,931 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 is already queued for rescanning.
2015-11-25 17:32:45,053 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 is already queued for rescanning.
2015-11-25 17:34:21,441 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 is already queued for rescanning.
2015-11-25 17:35:57,850 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526 is already queued for rescanning.
2015-11-25 17:48:24,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2848dd5ed502,  containing 1 storage report(s), of which we sent 1. The reports had 265 total blocks and used 1 RPC(s). This took 1 msec to generate and 7 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-25 17:48:24,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 17:48:27,880 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742373_1550 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/rbw/blk_1073742373 for deletion
2015-11-25 17:48:27,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742374_1551 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/rbw/blk_1073742374 for deletion
2015-11-25 17:48:27,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742375_1552 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/rbw/blk_1073742375 for deletion
2015-11-25 17:48:27,883 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742373_1550 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/rbw/blk_1073742373
2015-11-25 17:48:27,884 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742374_1551 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/rbw/blk_1073742374
2015-11-25 17:48:27,884 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742375_1552 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/rbw/blk_1073742375
2015-11-25 19:57:27,875 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-25 19:57:30,172 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-25 19:57:30,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-25 19:59:22,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-25 19:59:22,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-25 19:59:23,294 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-25 19:59:23,358 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-25 19:59:23,358 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-25 19:59:23,363 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-25 19:59:23,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-25 19:59:23,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-25 19:59:23,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-25 19:59:23,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-25 19:59:23,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-25 19:59:23,483 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-25 19:59:23,491 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-25 19:59:23,497 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-25 19:59:23,502 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-25 19:59:23,504 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-25 19:59:23,504 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-25 19:59:23,504 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-25 19:59:23,514 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37390
2015-11-25 19:59:23,514 INFO org.mortbay.log: jetty-6.1.26
2015-11-25 19:59:23,665 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37390
2015-11-25 19:59:23,867 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-25 19:59:23,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-25 19:59:23,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-25 19:59:23,912 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-25 19:59:23,924 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-25 19:59:23,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-25 19:59:23,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-25 19:59:23,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-25 19:59:24,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-25 19:59:24,008 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-25 19:59:24,009 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-25 19:59:24,280 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 7599@rushikesh1
2015-11-25 19:59:24,378 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-25 19:59:24,378 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-25 19:59:24,379 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-25 19:59:24,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-25 19:59:24,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-25 19:59:24,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-25 19:59:24,489 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-25 19:59:24,489 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 19:59:24,490 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 19:59:24,518 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35143790592
2015-11-25 19:59:24,520 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 29ms
2015-11-25 19:59:24,520 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 31ms
2015-11-25 19:59:24,520 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 19:59:24,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 35ms
2015-11-25 19:59:24,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 35ms
2015-11-25 19:59:24,814 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1089243564 ms.
2015-11-25 19:59:24,816 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448474736816 with interval 21600000
2015-11-25 19:59:24,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-25 19:59:24,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-25 19:59:24,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-25 19:59:24,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1984
2015-11-25 19:59:24,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-25 19:59:24,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2f6ee9975236,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 3 msec to generate and 44 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-25 19:59:24,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 20:05:24,001 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-25 20:05:28,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-25 20:05:28,329 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-25 20:05:28,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-25 20:07:30,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-25 20:07:30,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-25 20:07:31,139 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-25 20:07:31,204 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-25 20:07:31,204 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-25 20:07:31,209 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-25 20:07:31,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-25 20:07:31,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-25 20:07:31,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-25 20:07:31,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-25 20:07:31,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-25 20:07:31,332 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-25 20:07:31,339 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-25 20:07:31,344 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-25 20:07:31,349 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-25 20:07:31,352 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-25 20:07:31,352 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-25 20:07:31,352 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-25 20:07:31,362 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 49919
2015-11-25 20:07:31,362 INFO org.mortbay.log: jetty-6.1.26
2015-11-25 20:07:31,520 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:49919
2015-11-25 20:07:31,602 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-25 20:07:31,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-25 20:07:31,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-25 20:07:31,643 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-25 20:07:31,654 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-25 20:07:31,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-25 20:07:31,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-25 20:07:31,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-25 20:07:31,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-25 20:07:31,735 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-25 20:07:31,735 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-25 20:07:32,035 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 9975@rushikesh1
2015-11-25 20:07:32,142 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-25 20:07:32,142 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-25 20:07:32,143 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-25 20:07:32,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-25 20:07:32,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-25 20:07:32,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-25 20:07:32,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-25 20:07:32,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 20:07:32,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 20:07:32,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35143790592
2015-11-25 20:07:32,267 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-25 20:07:32,267 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-25 20:07:32,267 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 20:07:32,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 32ms
2015-11-25 20:07:32,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2015-11-25 20:07:32,463 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1088755915 ms.
2015-11-25 20:07:32,465 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448479289465 with interval 21600000
2015-11-25 20:07:32,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-25 20:07:32,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-25 20:07:32,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-25 20:07:32,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1987
2015-11-25 20:07:32,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-25 20:07:32,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2fe0772d716e,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 13 msec to generate and 56 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-25 20:07:32,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 20:09:16,730 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-25 20:09:20,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-25 20:09:21,314 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-25 20:09:21,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-25 20:21:24,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-25 20:21:24,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-25 20:21:24,683 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-25 20:21:24,746 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-25 20:21:24,746 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-25 20:21:24,751 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-25 20:21:24,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-25 20:21:24,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-25 20:21:24,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-25 20:21:24,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-25 20:21:24,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-25 20:21:24,873 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-25 20:21:24,881 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-25 20:21:24,886 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-25 20:21:24,891 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-25 20:21:24,894 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-25 20:21:24,894 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-25 20:21:24,894 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-25 20:21:24,904 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43623
2015-11-25 20:21:24,904 INFO org.mortbay.log: jetty-6.1.26
2015-11-25 20:21:25,051 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43623
2015-11-25 20:21:25,139 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-25 20:21:25,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-25 20:21:25,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-25 20:21:25,178 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-25 20:21:25,189 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-25 20:21:25,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-25 20:21:25,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-25 20:21:25,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-25 20:21:25,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-25 20:21:25,268 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-25 20:21:25,268 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-25 20:21:25,602 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 13404@rushikesh1
2015-11-25 20:21:25,691 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-25 20:21:25,691 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-25 20:21:25,691 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-25 20:21:25,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-25 20:21:25,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-25 20:21:25,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-25 20:21:25,807 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-25 20:21:25,807 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 20:21:25,808 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 20:21:25,824 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 16ms
2015-11-25 20:21:25,824 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 17ms
2015-11-25 20:21:25,825 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 20:21:25,876 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 50ms
2015-11-25 20:21:25,876 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 52ms
2015-11-25 20:21:26,052 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1087922326 ms.
2015-11-25 20:21:26,054 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448483873054 with interval 21600000
2015-11-25 20:21:26,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-25 20:21:26,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-25 20:21:26,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-25 20:21:26,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1990
2015-11-25 20:21:26,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-25 20:21:26,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x30a28c597d63,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 4 msec to generate and 66 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-25 20:21:26,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 20:22:07,046 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 for rescanning.
2015-11-25 20:22:07,047 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-11-25 20:22:07,048 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting.
2015-11-25 20:22:13,754 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 for rescanning.
2015-11-25 20:22:19,839 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 for rescanning.
2015-11-25 20:22:26,135 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 for rescanning.
2015-11-25 20:22:33,032 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 for rescanning.
2015-11-25 20:22:39,190 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 for rescanning.
2015-11-25 20:22:46,780 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 for rescanning.
2015-11-25 20:22:53,101 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 for rescanning.
2015-11-25 20:22:59,246 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 for rescanning.
2015-11-25 20:23:06,725 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 for rescanning.
2015-11-25 20:23:12,963 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 for rescanning.
2015-11-25 20:23:18,997 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 for rescanning.
2015-11-25 20:23:26,142 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 for rescanning.
2015-11-25 20:23:32,349 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 for rescanning.
2015-11-25 20:23:38,792 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 for rescanning.
2015-11-25 20:23:46,627 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 for rescanning.
2015-11-25 20:23:53,511 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 for rescanning.
2015-11-25 20:24:00,333 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 for rescanning.
2015-11-25 20:24:06,414 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 for rescanning.
2015-11-25 20:24:13,524 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 for rescanning.
2015-11-25 20:24:21,626 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 for rescanning.
2015-11-25 20:24:27,595 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 for rescanning.
2015-11-25 20:24:34,281 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 for rescanning.
2015-11-25 20:24:40,635 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 for rescanning.
2015-11-25 20:24:47,911 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 for rescanning.
2015-11-25 20:24:54,770 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 for rescanning.
2015-11-25 20:25:00,926 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 for rescanning.
2015-11-25 20:25:07,704 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 for rescanning.
2015-11-25 20:25:15,029 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 for rescanning.
2015-11-25 20:25:21,167 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 for rescanning.
2015-11-25 20:25:27,571 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 for rescanning.
2015-11-25 20:25:33,733 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 for rescanning.
2015-11-25 20:25:41,052 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 for rescanning.
2015-11-25 20:25:48,040 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 for rescanning.
2015-11-25 20:25:54,249 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 for rescanning.
2015-11-25 20:26:01,871 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 for rescanning.
2015-11-25 20:26:08,315 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 for rescanning.
2015-11-25 20:26:14,303 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 for rescanning.
2015-11-25 20:26:21,592 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 for rescanning.
2015-11-25 20:26:27,531 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 for rescanning.
2015-11-25 20:26:34,386 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 for rescanning.
2015-11-25 20:26:42,238 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 for rescanning.
2015-11-25 20:26:48,242 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 for rescanning.
2015-11-25 20:26:54,614 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 for rescanning.
2015-11-25 20:27:01,789 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442 for rescanning.
2015-11-25 20:27:07,853 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443 for rescanning.
2015-11-25 20:27:15,196 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 for rescanning.
2015-11-25 20:27:22,259 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 for rescanning.
2015-11-25 20:27:30,100 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446 for rescanning.
2015-11-25 20:27:37,743 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447 for rescanning.
2015-11-25 20:27:43,921 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448 for rescanning.
2015-11-25 20:27:50,397 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449 for rescanning.
2015-11-25 20:27:57,799 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450 for rescanning.
2015-11-25 20:28:03,933 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451 for rescanning.
2015-11-25 20:28:10,876 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452 for rescanning.
2015-11-25 20:28:17,553 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453 for rescanning.
2015-11-25 20:28:24,396 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454 for rescanning.
2015-11-25 20:28:31,781 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455 for rescanning.
2015-11-25 20:28:38,067 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456 for rescanning.
2015-11-25 20:28:44,419 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457 for rescanning.
2015-11-25 20:28:51,447 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458 for rescanning.
2015-11-25 20:28:57,997 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459 for rescanning.
2015-11-25 20:29:04,519 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460 for rescanning.
2015-11-25 20:29:11,220 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461 for rescanning.
2015-11-25 20:29:19,290 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462 for rescanning.
2015-11-25 20:29:26,029 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463 for rescanning.
2015-11-25 20:29:33,041 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464 for rescanning.
2015-11-25 20:29:41,937 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465 for rescanning.
2015-11-25 20:29:48,051 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466 for rescanning.
2015-11-25 20:29:54,383 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467 for rescanning.
2015-11-25 20:30:02,825 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468 for rescanning.
2015-11-25 20:30:09,121 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469 for rescanning.
2015-11-25 20:30:17,419 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470 for rescanning.
2015-11-25 20:30:23,985 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471 for rescanning.
2015-11-25 20:30:30,124 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472 for rescanning.
2015-11-25 20:30:38,846 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473 for rescanning.
2015-11-25 20:30:45,845 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474 for rescanning.
2015-11-25 20:30:52,146 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475 for rescanning.
2015-11-25 20:31:00,329 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476 for rescanning.
2015-11-25 20:31:06,469 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477 for rescanning.
2015-11-25 20:31:13,547 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478 for rescanning.
2015-11-25 20:31:21,180 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479 for rescanning.
2015-11-25 20:31:27,320 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480 for rescanning.
2015-11-25 20:31:34,096 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481 for rescanning.
2015-11-25 20:31:41,499 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482 for rescanning.
2015-11-25 20:31:47,531 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483 for rescanning.
2015-11-25 20:31:53,960 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484 for rescanning.
2015-11-25 20:32:01,497 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485 for rescanning.
2015-11-25 20:32:07,816 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486 for rescanning.
2015-11-25 20:32:14,515 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487 for rescanning.
2015-11-25 20:32:22,009 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488 for rescanning.
2015-11-25 20:32:28,345 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489 for rescanning.
2015-11-25 20:32:34,859 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490 for rescanning.
2015-11-25 20:32:42,456 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491 for rescanning.
2015-11-25 20:32:48,873 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492 for rescanning.
2015-11-25 20:32:54,996 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493 for rescanning.
2015-11-25 20:33:02,625 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494 for rescanning.
2015-11-25 20:33:08,594 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495 for rescanning.
2015-11-25 20:33:15,302 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496 for rescanning.
2015-11-25 20:33:22,793 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497 for rescanning.
2015-11-25 20:33:29,097 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498 for rescanning.
2015-11-25 20:33:35,326 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 for rescanning.
2015-11-25 20:33:36,725 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 for rescanning, because we rescanned it recently.
2015-11-25 20:34:13,543 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 for rescanning, because we rescanned it recently.
2015-11-25 20:34:19,691 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502 for rescanning.
2015-11-25 20:34:25,906 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503 for rescanning.
2015-11-25 20:34:32,519 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504 for rescanning.
2015-11-25 20:34:38,772 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505 for rescanning.
2015-11-25 20:34:46,081 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506 for rescanning.
2015-11-25 20:34:52,626 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507 for rescanning.
2015-11-25 20:34:58,706 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508 for rescanning.
2015-11-25 20:35:06,258 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509 for rescanning.
2015-11-25 20:35:12,579 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 for rescanning.
2015-11-25 20:35:16,207 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 for rescanning, because we rescanned it recently.
2015-11-25 20:35:23,801 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 for rescanning, because we rescanned it recently.
2015-11-25 20:35:29,916 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512 for rescanning.
2015-11-25 20:35:37,693 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513 for rescanning.
2015-11-25 20:35:43,760 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514 for rescanning.
2015-11-25 20:35:50,024 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515 for rescanning.
2015-11-25 20:35:57,206 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 for rescanning.
2015-11-25 20:36:03,372 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517 for rescanning.
2015-11-25 20:36:11,004 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518 for rescanning.
2015-11-25 20:36:17,134 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519 for rescanning.
2015-11-25 20:36:24,409 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520 for rescanning.
2015-11-25 20:36:31,735 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521 for rescanning.
2015-11-25 20:36:38,040 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522 for rescanning.
2015-11-25 20:36:44,973 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 for rescanning.
2015-11-25 20:36:52,015 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 for rescanning.
2015-11-25 20:36:58,193 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 for rescanning.
2015-11-25 20:37:05,414 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526 for rescanning.
2015-11-25 23:02:13,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3968b0afc493,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 1 msec to generate and 8 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-25 23:02:13,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-26 02:07:53,117 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 262, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-26 05:02:13,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4d0dd50c284d,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 1 msec to generate and 7 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-26 05:02:13,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-26 08:07:53,099 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 262, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-26 11:02:13,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x60b2f953caff,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 1 msec to generate and 7 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-26 11:02:13,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-26 12:32:01,262 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-26 12:32:05,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-26 12:32:06,133 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-26 12:32:06,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-26 17:56:01,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-26 17:56:01,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-26 17:56:01,712 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-26 17:56:01,774 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-26 17:56:01,774 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-26 17:56:01,779 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-26 17:56:01,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-26 17:56:01,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-26 17:56:01,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-26 17:56:01,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-26 17:56:01,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-26 17:56:01,941 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-26 17:56:01,949 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-26 17:56:01,954 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-26 17:56:01,959 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-26 17:56:01,961 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-26 17:56:01,961 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-26 17:56:01,961 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-26 17:56:01,971 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 46205
2015-11-26 17:56:01,971 INFO org.mortbay.log: jetty-6.1.26
2015-11-26 17:56:02,130 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46205
2015-11-26 17:56:02,312 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-26 17:56:02,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-26 17:56:02,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-26 17:56:02,376 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-26 17:56:02,387 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-26 17:56:02,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-26 17:56:02,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-26 17:56:02,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-26 17:56:02,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-26 17:56:02,505 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-26 17:56:02,506 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-26 17:56:03,075 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 25178@rushikesh1
2015-11-26 17:56:03,163 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-26 17:56:03,163 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-26 17:56:03,164 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-26 17:56:03,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-26 17:56:03,331 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-26 17:56:03,331 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-26 17:56:03,369 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-26 17:56:03,369 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-26 17:56:03,370 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-26 17:56:03,421 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 51ms
2015-11-26 17:56:03,422 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 52ms
2015-11-26 17:56:03,422 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-26 17:56:03,477 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 55ms
2015-11-26 17:56:03,477 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 56ms
2015-11-26 17:56:03,752 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 1010244626 ms.
2015-11-26 17:56:03,754 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448560077754 with interval 21600000
2015-11-26 17:56:03,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-26 17:56:03,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-26 17:56:03,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-26 17:56:03,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2027
2015-11-26 17:56:03,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-26 17:56:04,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x112408fdb9d1,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 7 msec to generate and 79 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-26 17:56:04,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-26 18:10:15,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742377_1554 src: /192.168.6.248:51598 dest: /192.168.6.248:50010
2015-11-26 18:10:15,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:51598, dest: /192.168.6.248:50010, bytes: 287206, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1428792081_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742377_1554, duration: 86290861
2015-11-26 18:10:15,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742377_1554, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-26 18:10:44,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742377_1554 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742377 for deletion
2015-11-26 18:10:44,468 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742377_1554 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742377
2015-11-26 18:27:26,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x12da5932e418,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 1 msec to generate and 10 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-26 18:27:26,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-26 19:58:53,465 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-26 19:58:57,328 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-26 19:58:57,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-27 12:39:07,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 12:39:08,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 12:39:08,745 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 12:39:08,808 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 12:39:08,808 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 12:39:08,813 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 12:39:08,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-27 12:39:08,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 12:39:08,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 12:39:08,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 12:39:08,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 12:39:08,985 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 12:39:08,992 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 12:39:08,997 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 12:39:09,002 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 12:39:09,005 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 12:39:09,005 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 12:39:09,005 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 12:39:09,015 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 58188
2015-11-27 12:39:09,015 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 12:39:09,223 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:58188
2015-11-27 12:39:09,350 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 12:39:09,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 12:39:09,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 12:39:09,391 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 12:39:09,403 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 12:39:09,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 12:39:09,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 12:39:09,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 12:39:09,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 12:39:09,507 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 12:39:09,507 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 12:39:09,940 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3989@rushikesh1
2015-11-27 12:39:10,026 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 12:39:10,026 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 12:39:10,027 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 12:39:10,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-27 12:39:10,155 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-27 12:39:10,155 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 12:39:10,193 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 12:39:10,193 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 12:39:10,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 12:39:10,240 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 46ms
2015-11-27 12:39:10,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 47ms
2015-11-27 12:39:10,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 12:39:10,296 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 54ms
2015-11-27 12:39:10,296 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 55ms
2015-11-27 12:39:10,539 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 942857839 ms.
2015-11-27 12:39:10,541 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448611124541 with interval 21600000
2015-11-27 12:39:10,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 12:39:10,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 12:39:10,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 12:39:10,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2053
2015-11-27 12:39:10,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-27 12:39:10,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa82a01eb07,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 7 msec to generate and 82 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 12:39:10,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 12:41:06,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc31db1653d,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 1 msec to generate and 12 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 12:41:06,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 13:07:01,797 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh1:50010:DataXceiver error processing unknown operation  src: /192.168.6.254:38046 dst: /192.168.6.248:50010
java.io.IOException: Version Mismatch (Expected: 28, Received: 18245 )
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:60)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:227)
	at java.lang.Thread.run(Thread.java:745)
2015-11-27 13:28:44,615 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 262, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-27 14:05:06,476 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-27 14:05:10,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-27 14:05:11,014 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 14:05:11,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-27 14:10:33,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 14:10:33,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 14:10:34,193 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 14:10:34,256 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 14:10:34,256 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 14:10:34,261 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 14:10:34,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-27 14:10:34,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 14:10:34,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 14:10:34,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 14:10:34,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 14:10:34,379 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 14:10:34,387 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 14:10:34,392 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 14:10:34,397 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 14:10:34,400 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 14:10:34,400 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 14:10:34,400 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 14:10:34,410 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 60226
2015-11-27 14:10:34,410 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 14:10:34,558 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:60226
2015-11-27 14:10:34,649 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 14:10:34,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 14:10:34,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 14:10:34,689 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 14:10:34,700 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 14:10:34,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 14:10:34,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 14:10:34,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 14:10:34,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 14:10:34,779 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 14:10:34,779 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 14:10:35,123 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 9870@rushikesh1
2015-11-27 14:10:35,219 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:10:35,219 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:10:35,220 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 14:10:35,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-27 14:10:35,294 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-27 14:10:35,294 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 14:10:35,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 14:10:35,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:10:35,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 14:10:35,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35143790592
2015-11-27 14:10:35,335 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-27 14:10:35,335 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-11-27 14:10:35,335 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 14:10:35,367 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 32ms
2015-11-27 14:10:35,368 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2015-11-27 14:10:35,538 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 937372841 ms.
2015-11-27 14:10:35,540 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448616494540 with interval 21600000
2015-11-27 14:10:35,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 14:10:35,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 14:10:35,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 14:10:35,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2064
2015-11-27 14:10:35,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-27 14:10:35,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5a539209fd7,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 4 msec to generate and 68 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 14:10:35,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:11:07,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 to 192.168.6.238:50010 
2015-11-27 14:11:07,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 to 192.168.6.238:50010 
2015-11-27 14:11:42,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 (numBytes=134217728) to /192.168.6.238:50010
2015-11-27 14:11:43,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 to 192.168.6.238:50010 
2015-11-27 14:11:44,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 (numBytes=134217728) to /192.168.6.238:50010
2015-11-27 14:11:46,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 to 192.168.6.238:50010 
2015-11-27 14:12:22,773 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-27 14:12:26,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-27 14:12:27,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 (numBytes=134217728) to /192.168.6.238:50010
2015-11-27 14:12:27,275 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 14:12:27,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-27 14:15:21,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 14:15:21,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 14:15:22,197 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 14:15:22,260 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 14:15:22,260 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 14:15:22,265 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 14:15:22,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-27 14:15:22,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 14:15:22,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 14:15:22,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 14:15:22,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 14:15:22,386 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 14:15:22,394 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 14:15:22,399 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 14:15:22,404 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 14:15:22,406 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 14:15:22,406 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 14:15:22,406 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 14:15:22,416 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 60199
2015-11-27 14:15:22,416 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 14:15:22,569 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:60199
2015-11-27 14:15:22,652 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 14:15:22,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 14:15:22,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 14:15:22,693 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 14:15:22,704 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 14:15:22,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 14:15:22,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 14:15:22,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 14:15:22,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 14:15:22,783 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 14:15:22,783 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 14:15:23,146 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 11849@rushikesh1
2015-11-27 14:15:23,243 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:15:23,243 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:15:23,244 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 14:15:23,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-27 14:15:23,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-27 14:15:23,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 14:15:23,354 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 14:15:23,355 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:15:23,355 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 14:15:23,362 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35143790592
2015-11-27 14:15:23,363 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 7ms
2015-11-27 14:15:23,363 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-11-27 14:15:23,363 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 14:15:23,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 32ms
2015-11-27 14:15:23,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2015-11-27 14:15:23,558 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 937084820 ms.
2015-11-27 14:15:23,560 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448615049560 with interval 21600000
2015-11-27 14:15:23,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 14:15:23,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 14:15:23,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 14:15:23,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2067
2015-11-27 14:15:23,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-27 14:15:23,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5e8494fd2f8,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 5 msec to generate and 67 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 14:15:23,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:23:19,778 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-27 14:23:23,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-27 14:23:24,197 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 14:23:24,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-27 14:27:19,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 14:27:19,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 14:27:19,703 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 14:27:19,766 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 14:27:19,766 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 14:27:19,771 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 14:27:19,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-27 14:27:19,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 14:27:19,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 14:27:19,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 14:27:19,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 14:27:19,889 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 14:27:19,897 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 14:27:19,902 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 14:27:19,907 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 14:27:19,909 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 14:27:19,910 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 14:27:19,910 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 14:27:19,920 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41862
2015-11-27 14:27:19,920 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 14:27:20,072 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41862
2015-11-27 14:27:20,154 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 14:27:20,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 14:27:20,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 14:27:20,194 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 14:27:20,205 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 14:27:20,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 14:27:20,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 14:27:20,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 14:27:20,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 14:27:20,286 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 14:27:20,286 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 14:27:20,609 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 14108@rushikesh1
2015-11-27 14:27:20,705 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:27:20,705 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:27:20,706 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 14:27:20,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-27 14:27:20,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-27 14:27:20,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 14:27:20,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 14:27:20,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:27:20,814 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 14:27:20,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35143790592
2015-11-27 14:27:20,822 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-27 14:27:20,822 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-27 14:27:20,822 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 14:27:20,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 32ms
2015-11-27 14:27:20,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2015-11-27 14:27:21,025 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 936367353 ms.
2015-11-27 14:27:21,027 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448632081027 with interval 21600000
2015-11-27 14:27:21,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 14:27:21,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 14:27:21,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 14:27:21,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2070
2015-11-27 14:27:21,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-27 14:27:21,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x68f54cc46aa,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 3 msec to generate and 65 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 14:27:21,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:27:53,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004 to 192.168.6.238:50010 
2015-11-27 14:27:53,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 to 192.168.6.238:50010 
2015-11-27 14:27:54,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004 (numBytes=4045946) to /192.168.6.238:50010
2015-11-27 14:27:56,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 to 192.168.6.238:50010 
2015-11-27 14:28:36,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 (numBytes=134217728) to /192.168.6.238:50010
2015-11-27 14:28:39,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 (numBytes=134217728) to /192.168.6.238:50010
2015-11-27 14:28:46,789 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-27 14:28:46,980 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 14:28:46,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-27 14:30:08,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 14:30:08,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 14:30:09,216 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 14:30:09,279 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 14:30:09,279 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 14:30:09,284 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 14:30:09,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-27 14:30:09,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 14:30:09,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 14:30:09,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 14:30:09,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 14:30:09,401 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 14:30:09,409 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 14:30:09,414 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 14:30:09,419 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 14:30:09,421 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 14:30:09,421 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 14:30:09,422 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 14:30:09,431 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 32875
2015-11-27 14:30:09,431 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 14:30:09,578 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:32875
2015-11-27 14:30:09,666 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 14:30:09,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 14:30:09,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 14:30:09,706 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 14:30:09,717 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 14:30:09,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 14:30:09,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 14:30:09,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 14:30:09,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 14:30:09,796 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 14:30:09,797 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 14:30:10,134 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 16037@rushikesh1
2015-11-27 14:30:10,237 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:30:10,237 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:30:10,238 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 14:30:10,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-27 14:30:10,320 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-27 14:30:10,320 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 14:30:10,345 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 14:30:10,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:30:10,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 14:30:10,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35143790592
2015-11-27 14:30:10,354 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 7ms
2015-11-27 14:30:10,354 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-11-27 14:30:10,354 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 14:30:10,386 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 32ms
2015-11-27 14:30:10,386 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2015-11-27 14:30:10,546 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 936197832 ms.
2015-11-27 14:30:10,548 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448616160548 with interval 21600000
2015-11-27 14:30:10,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 14:30:10,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 14:30:10,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 14:30:10,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2073
2015-11-27 14:30:10,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-27 14:30:10,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6b6cdab3c09,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 5 msec to generate and 75 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 14:30:10,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 14:31:24,791 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-27 14:31:28,067 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 14:31:28,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-27 15:00:13,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 15:00:13,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 15:00:14,381 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 15:00:14,444 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 15:00:14,444 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 15:00:14,450 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 15:00:14,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-27 15:00:14,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 15:00:14,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 15:00:14,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 15:00:14,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 15:00:14,568 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 15:00:14,576 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 15:00:14,581 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 15:00:14,586 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 15:00:14,588 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 15:00:14,588 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 15:00:14,588 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 15:00:14,598 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 56464
2015-11-27 15:00:14,598 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 15:00:14,750 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:56464
2015-11-27 15:00:14,833 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 15:00:14,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 15:00:14,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 15:00:14,872 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 15:00:14,883 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 15:00:14,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 15:00:14,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 15:00:14,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 15:00:14,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 15:00:14,969 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 15:00:14,969 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 15:00:15,302 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 18916@rushikesh1
2015-11-27 15:00:15,398 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 15:00:15,398 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 15:00:15,398 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 15:00:15,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-27 15:00:15,481 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-27 15:00:15,481 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 15:00:15,508 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 15:00:15,508 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 15:00:15,509 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 15:00:15,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 12ms
2015-11-27 15:00:15,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 13ms
2015-11-27 15:00:15,522 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 15:00:15,559 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 38ms
2015-11-27 15:00:15,559 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 39ms
2015-11-27 15:00:15,720 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 934392658 ms.
2015-11-27 15:00:15,722 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448623491722 with interval 21600000
2015-11-27 15:00:15,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 15:00:15,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 15:00:15,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 15:00:15,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2074
2015-11-27 15:00:15,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-27 15:00:15,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x85b19644b34,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 4 msec to generate and 67 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 15:00:15,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 15:14:44,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-27 15:14:47,821 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 15:14:47,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-27 15:55:44,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 15:55:44,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 15:55:44,634 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 15:55:44,696 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 15:55:44,696 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 15:55:44,701 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 15:55:44,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-27 15:55:44,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 15:55:44,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 15:55:44,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 15:55:44,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 15:55:44,819 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 15:55:44,826 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 15:55:44,832 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 15:55:44,837 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 15:55:44,839 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 15:55:44,839 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 15:55:44,839 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 15:55:44,849 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38824
2015-11-27 15:55:44,849 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 15:55:44,996 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38824
2015-11-27 15:55:45,083 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 15:55:45,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 15:55:45,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 15:55:45,122 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 15:55:45,133 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 15:55:45,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 15:55:45,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 15:55:45,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 15:55:45,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 15:55:45,213 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 15:55:45,213 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 15:55:45,571 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 23481@rushikesh1
2015-11-27 15:55:45,666 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 15:55:45,666 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 15:55:45,667 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 15:55:45,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-27 15:55:45,750 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-27 15:55:45,750 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 15:55:45,775 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 15:55:45,776 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 15:55:45,776 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 15:55:45,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 10ms
2015-11-27 15:55:45,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 11ms
2015-11-27 15:55:45,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 15:55:45,820 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 32ms
2015-11-27 15:55:45,820 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2015-11-27 15:55:45,979 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 931062399 ms.
2015-11-27 15:55:45,981 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448627898981 with interval 21600000
2015-11-27 15:55:45,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 15:55:46,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 15:55:46,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 15:55:46,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2081
2015-11-27 15:55:46,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-27 15:55:46,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb627d1bcc48,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 13 msec to generate and 54 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 15:55:46,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 16:01:36,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742378_1555 src: /192.168.6.248:37490 dest: /192.168.6.248:50010
2015-11-27 16:01:36,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37490, dest: /192.168.6.248:50010, bytes: 3363967, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-192053129_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742378_1555, duration: 353901205
2015-11-27 16:01:36,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742378_1555, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:14,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742379_1556 src: /192.168.6.248:37539 dest: /192.168.6.248:50010
2015-11-27 16:04:15,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37539, dest: /192.168.6.248:50010, bytes: 2239882, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1419366260_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742379_1556, duration: 252427089
2015-11-27 16:04:15,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742379_1556, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:17,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742380_1557 src: /192.168.6.248:37542 dest: /192.168.6.248:50010
2015-11-27 16:04:17,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37542, dest: /192.168.6.248:50010, bytes: 2254636, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_769393575_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742380_1557, duration: 251973533
2015-11-27 16:04:17,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742380_1557, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:19,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742381_1558 src: /192.168.6.248:37545 dest: /192.168.6.248:50010
2015-11-27 16:04:20,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37545, dest: /192.168.6.248:50010, bytes: 2255615, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2100031452_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742381_1558, duration: 249239669
2015-11-27 16:04:20,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742381_1558, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:22,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742382_1559 src: /192.168.6.248:37548 dest: /192.168.6.248:50010
2015-11-27 16:04:22,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37548, dest: /192.168.6.248:50010, bytes: 2245077, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1091588624_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742382_1559, duration: 244947768
2015-11-27 16:04:22,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742382_1559, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:24,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742383_1560 src: /192.168.6.248:37551 dest: /192.168.6.248:50010
2015-11-27 16:04:24,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37551, dest: /192.168.6.248:50010, bytes: 2244091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-188399430_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742383_1560, duration: 245266975
2015-11-27 16:04:24,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742383_1560, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:26,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742384_1561 src: /192.168.6.248:37554 dest: /192.168.6.248:50010
2015-11-27 16:04:27,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37554, dest: /192.168.6.248:50010, bytes: 2239839, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1089394258_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742384_1561, duration: 248182476
2015-11-27 16:04:27,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742384_1561, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:29,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742385_1562 src: /192.168.6.248:37557 dest: /192.168.6.248:50010
2015-11-27 16:04:29,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37557, dest: /192.168.6.248:50010, bytes: 2254636, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_708916130_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742385_1562, duration: 245736728
2015-11-27 16:04:29,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742385_1562, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:31,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742386_1563 src: /192.168.6.248:37563 dest: /192.168.6.248:50010
2015-11-27 16:04:31,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37563, dest: /192.168.6.248:50010, bytes: 2255615, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1652519470_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742386_1563, duration: 247694058
2015-11-27 16:04:31,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742386_1563, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:34,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742387_1564 src: /192.168.6.248:37569 dest: /192.168.6.248:50010
2015-11-27 16:04:34,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37569, dest: /192.168.6.248:50010, bytes: 2245077, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1681932291_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742387_1564, duration: 244563133
2015-11-27 16:04:34,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742387_1564, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:36,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742388_1565 src: /192.168.6.248:37572 dest: /192.168.6.248:50010
2015-11-27 16:04:36,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37572, dest: /192.168.6.248:50010, bytes: 2244091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1496412515_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742388_1565, duration: 251887715
2015-11-27 16:04:36,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742388_1565, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:38,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742389_1566 src: /192.168.6.248:37575 dest: /192.168.6.248:50010
2015-11-27 16:04:38,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37575, dest: /192.168.6.248:50010, bytes: 2268436, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1057581316_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742389_1566, duration: 248895721
2015-11-27 16:04:38,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742389_1566, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:41,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742390_1567 src: /192.168.6.248:37578 dest: /192.168.6.248:50010
2015-11-27 16:04:41,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37578, dest: /192.168.6.248:50010, bytes: 2266617, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_77722342_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742390_1567, duration: 245942387
2015-11-27 16:04:41,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742390_1567, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:43,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742391_1568 src: /192.168.6.248:37581 dest: /192.168.6.248:50010
2015-11-27 16:04:43,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37581, dest: /192.168.6.248:50010, bytes: 2269853, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1787146598_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742391_1568, duration: 250783550
2015-11-27 16:04:43,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742391_1568, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:46,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742392_1569 src: /192.168.6.248:37584 dest: /192.168.6.248:50010
2015-11-27 16:04:46,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37584, dest: /192.168.6.248:50010, bytes: 2272502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_754321113_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742392_1569, duration: 255190568
2015-11-27 16:04:46,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742392_1569, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:48,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742393_1570 src: /192.168.6.248:37587 dest: /192.168.6.248:50010
2015-11-27 16:04:48,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37587, dest: /192.168.6.248:50010, bytes: 2389070, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1892248561_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742393_1570, duration: 266346256
2015-11-27 16:04:48,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742393_1570, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:50,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742394_1571 src: /192.168.6.248:37590 dest: /192.168.6.248:50010
2015-11-27 16:04:51,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37590, dest: /192.168.6.248:50010, bytes: 2261764, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1085163711_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742394_1571, duration: 246058712
2015-11-27 16:04:51,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742394_1571, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:53,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742395_1572 src: /192.168.6.248:37593 dest: /192.168.6.248:50010
2015-11-27 16:04:53,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37593, dest: /192.168.6.248:50010, bytes: 2269724, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1935583376_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742395_1572, duration: 246096056
2015-11-27 16:04:53,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742395_1572, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:55,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742396_1573 src: /192.168.6.248:37596 dest: /192.168.6.248:50010
2015-11-27 16:04:55,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37596, dest: /192.168.6.248:50010, bytes: 2267336, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_83448357_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742396_1573, duration: 247519924
2015-11-27 16:04:55,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742396_1573, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:04:57,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742397_1574 src: /192.168.6.248:37599 dest: /192.168.6.248:50010
2015-11-27 16:04:58,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37599, dest: /192.168.6.248:50010, bytes: 2272974, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-482657314_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742397_1574, duration: 247259654
2015-11-27 16:04:58,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742397_1574, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:00,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742398_1575 src: /192.168.6.248:37603 dest: /192.168.6.248:50010
2015-11-27 16:05:00,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37603, dest: /192.168.6.248:50010, bytes: 2390295, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-375788784_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742398_1575, duration: 256626530
2015-11-27 16:05:00,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742398_1575, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:02,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742399_1576 src: /192.168.6.248:37611 dest: /192.168.6.248:50010
2015-11-27 16:05:03,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37611, dest: /192.168.6.248:50010, bytes: 2271852, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2079544779_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742399_1576, duration: 246840717
2015-11-27 16:05:03,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742399_1576, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:05,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742400_1577 src: /192.168.6.248:37615 dest: /192.168.6.248:50010
2015-11-27 16:05:05,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37615, dest: /192.168.6.248:50010, bytes: 2273589, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_360155184_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742400_1577, duration: 245712372
2015-11-27 16:05:05,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742400_1577, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:07,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742401_1578 src: /192.168.6.248:37618 dest: /192.168.6.248:50010
2015-11-27 16:05:07,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37618, dest: /192.168.6.248:50010, bytes: 2267236, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-73119185_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742401_1578, duration: 246771192
2015-11-27 16:05:07,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742401_1578, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:09,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742402_1579 src: /192.168.6.248:37621 dest: /192.168.6.248:50010
2015-11-27 16:05:10,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37621, dest: /192.168.6.248:50010, bytes: 2267993, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1063814990_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742402_1579, duration: 245934084
2015-11-27 16:05:10,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742402_1579, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:12,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742403_1580 src: /192.168.6.248:37624 dest: /192.168.6.248:50010
2015-11-27 16:05:12,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37624, dest: /192.168.6.248:50010, bytes: 2387080, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1712759748_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742403_1580, duration: 257160447
2015-11-27 16:05:12,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742403_1580, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:14,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742404_1581 src: /192.168.6.248:37627 dest: /192.168.6.248:50010
2015-11-27 16:05:14,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37627, dest: /192.168.6.248:50010, bytes: 2273506, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_874066476_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742404_1581, duration: 245791440
2015-11-27 16:05:14,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742404_1581, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:16,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742405_1582 src: /192.168.6.248:37630 dest: /192.168.6.248:50010
2015-11-27 16:05:17,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37630, dest: /192.168.6.248:50010, bytes: 2269452, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1546805183_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742405_1582, duration: 245768587
2015-11-27 16:05:17,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742405_1582, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:19,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742406_1583 src: /192.168.6.248:37633 dest: /192.168.6.248:50010
2015-11-27 16:05:19,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37633, dest: /192.168.6.248:50010, bytes: 2274079, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1313171196_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742406_1583, duration: 246459525
2015-11-27 16:05:19,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742406_1583, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:21,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742407_1584 src: /192.168.6.248:37636 dest: /192.168.6.248:50010
2015-11-27 16:05:21,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37636, dest: /192.168.6.248:50010, bytes: 2267787, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1491561210_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742407_1584, duration: 245788360
2015-11-27 16:05:21,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742407_1584, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:23,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742408_1585 src: /192.168.6.248:37639 dest: /192.168.6.248:50010
2015-11-27 16:05:24,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37639, dest: /192.168.6.248:50010, bytes: 2385603, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2122912683_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742408_1585, duration: 256522601
2015-11-27 16:05:24,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742408_1585, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:26,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742409_1586 src: /192.168.6.248:37642 dest: /192.168.6.248:50010
2015-11-27 16:05:26,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37642, dest: /192.168.6.248:50010, bytes: 2273388, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-994562952_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742409_1586, duration: 246400135
2015-11-27 16:05:26,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742409_1586, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:28,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742410_1587 src: /192.168.6.248:37645 dest: /192.168.6.248:50010
2015-11-27 16:05:28,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37645, dest: /192.168.6.248:50010, bytes: 2272899, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-12583390_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742410_1587, duration: 248417210
2015-11-27 16:05:28,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742410_1587, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:30,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742411_1588 src: /192.168.6.248:37648 dest: /192.168.6.248:50010
2015-11-27 16:05:31,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37648, dest: /192.168.6.248:50010, bytes: 2270531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1978097543_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742411_1588, duration: 246484060
2015-11-27 16:05:31,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742411_1588, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:33,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742412_1589 src: /192.168.6.248:37657 dest: /192.168.6.248:50010
2015-11-27 16:05:33,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37657, dest: /192.168.6.248:50010, bytes: 2270886, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-433820038_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742412_1589, duration: 247860438
2015-11-27 16:05:33,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742412_1589, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:35,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742413_1590 src: /192.168.6.248:37660 dest: /192.168.6.248:50010
2015-11-27 16:05:36,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37660, dest: /192.168.6.248:50010, bytes: 2395800, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_444249520_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742413_1590, duration: 257881435
2015-11-27 16:05:36,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742413_1590, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:38,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742414_1591 src: /192.168.6.248:37663 dest: /192.168.6.248:50010
2015-11-27 16:05:38,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37663, dest: /192.168.6.248:50010, bytes: 2269527, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2107828724_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742414_1591, duration: 246606139
2015-11-27 16:05:38,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742414_1591, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:40,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742415_1592 src: /192.168.6.248:37666 dest: /192.168.6.248:50010
2015-11-27 16:05:40,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37666, dest: /192.168.6.248:50010, bytes: 2269234, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_954871241_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742415_1592, duration: 250679069
2015-11-27 16:05:40,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742415_1592, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:42,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742416_1593 src: /192.168.6.248:37669 dest: /192.168.6.248:50010
2015-11-27 16:05:43,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37669, dest: /192.168.6.248:50010, bytes: 2268521, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1652322610_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742416_1593, duration: 246319212
2015-11-27 16:05:43,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742416_1593, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:45,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742417_1594 src: /192.168.6.248:37672 dest: /192.168.6.248:50010
2015-11-27 16:05:45,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37672, dest: /192.168.6.248:50010, bytes: 2264843, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1322734136_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742417_1594, duration: 245786387
2015-11-27 16:05:45,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742417_1594, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:47,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742418_1595 src: /192.168.6.248:37675 dest: /192.168.6.248:50010
2015-11-27 16:05:48,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37675, dest: /192.168.6.248:50010, bytes: 2389217, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2040685034_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742418_1595, duration: 257862251
2015-11-27 16:05:48,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742418_1595, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:50,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742419_1596 src: /192.168.6.248:37678 dest: /192.168.6.248:50010
2015-11-27 16:05:50,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37678, dest: /192.168.6.248:50010, bytes: 2275038, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_963768916_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742419_1596, duration: 245682559
2015-11-27 16:05:50,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742419_1596, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:52,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742420_1597 src: /192.168.6.248:37681 dest: /192.168.6.248:50010
2015-11-27 16:05:52,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37681, dest: /192.168.6.248:50010, bytes: 2271002, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-898202297_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742420_1597, duration: 246234263
2015-11-27 16:05:52,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742420_1597, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:55,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742421_1598 src: /192.168.6.248:37684 dest: /192.168.6.248:50010
2015-11-27 16:05:55,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37684, dest: /192.168.6.248:50010, bytes: 2273574, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1877017368_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742421_1598, duration: 247268471
2015-11-27 16:05:55,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742421_1598, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:57,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742422_1599 src: /192.168.6.248:37687 dest: /192.168.6.248:50010
2015-11-27 16:05:57,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37687, dest: /192.168.6.248:50010, bytes: 2272153, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_779290810_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742422_1599, duration: 245667052
2015-11-27 16:05:57,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742422_1599, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:05:59,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742423_1600 src: /192.168.6.248:37691 dest: /192.168.6.248:50010
2015-11-27 16:05:59,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37691, dest: /192.168.6.248:50010, bytes: 2386911, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_76222914_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742423_1600, duration: 256043393
2015-11-27 16:05:59,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742423_1600, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:02,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742424_1601 src: /192.168.6.248:37697 dest: /192.168.6.248:50010
2015-11-27 16:06:02,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37697, dest: /192.168.6.248:50010, bytes: 2272985, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1840817464_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742424_1601, duration: 246155689
2015-11-27 16:06:02,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742424_1601, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:04,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742425_1602 src: /192.168.6.248:37703 dest: /192.168.6.248:50010
2015-11-27 16:06:04,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37703, dest: /192.168.6.248:50010, bytes: 2275600, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_507254154_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742425_1602, duration: 246813865
2015-11-27 16:06:04,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742425_1602, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:06,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742426_1603 src: /192.168.6.248:37706 dest: /192.168.6.248:50010
2015-11-27 16:06:06,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37706, dest: /192.168.6.248:50010, bytes: 2276131, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1237103931_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742426_1603, duration: 246854499
2015-11-27 16:06:06,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742426_1603, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:09,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742427_1604 src: /192.168.6.248:37709 dest: /192.168.6.248:50010
2015-11-27 16:06:09,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37709, dest: /192.168.6.248:50010, bytes: 2267440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-690563768_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742427_1604, duration: 249040795
2015-11-27 16:06:09,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742427_1604, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:11,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742428_1605 src: /192.168.6.248:37712 dest: /192.168.6.248:50010
2015-11-27 16:06:11,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37712, dest: /192.168.6.248:50010, bytes: 2390920, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-74694203_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742428_1605, duration: 257349711
2015-11-27 16:06:11,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742428_1605, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:13,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742429_1606 src: /192.168.6.248:37715 dest: /192.168.6.248:50010
2015-11-27 16:06:14,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37715, dest: /192.168.6.248:50010, bytes: 2270807, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1020296189_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742429_1606, duration: 245926037
2015-11-27 16:06:14,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742429_1606, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:16,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742430_1607 src: /192.168.6.248:37718 dest: /192.168.6.248:50010
2015-11-27 16:06:16,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37718, dest: /192.168.6.248:50010, bytes: 2276051, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_631562882_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742430_1607, duration: 254242483
2015-11-27 16:06:16,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742430_1607, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:18,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742431_1608 src: /192.168.6.248:37721 dest: /192.168.6.248:50010
2015-11-27 16:06:18,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37721, dest: /192.168.6.248:50010, bytes: 2274516, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-10474375_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742431_1608, duration: 255315498
2015-11-27 16:06:18,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742431_1608, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:21,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742432_1609 src: /192.168.6.248:37724 dest: /192.168.6.248:50010
2015-11-27 16:06:21,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37724, dest: /192.168.6.248:50010, bytes: 2273165, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_644104075_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742432_1609, duration: 251350431
2015-11-27 16:06:21,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742432_1609, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:23,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742433_1610 src: /192.168.6.248:37727 dest: /192.168.6.248:50010
2015-11-27 16:06:23,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37727, dest: /192.168.6.248:50010, bytes: 2389961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1672726040_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742433_1610, duration: 256283223
2015-11-27 16:06:23,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742433_1610, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:25,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742434_1611 src: /192.168.6.248:37730 dest: /192.168.6.248:50010
2015-11-27 16:06:26,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37730, dest: /192.168.6.248:50010, bytes: 2289411, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1242412575_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742434_1611, duration: 250974791
2015-11-27 16:06:26,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742434_1611, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:28,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742435_1612 src: /192.168.6.248:37733 dest: /192.168.6.248:50010
2015-11-27 16:06:28,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37733, dest: /192.168.6.248:50010, bytes: 2284941, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_912214448_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742435_1612, duration: 261320124
2015-11-27 16:06:28,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742435_1612, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:30,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742436_1613 src: /192.168.6.248:37736 dest: /192.168.6.248:50010
2015-11-27 16:06:30,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37736, dest: /192.168.6.248:50010, bytes: 2290782, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-943589859_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742436_1613, duration: 250059043
2015-11-27 16:06:30,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742436_1613, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:33,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742437_1614 src: /192.168.6.248:37744 dest: /192.168.6.248:50010
2015-11-27 16:06:33,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37744, dest: /192.168.6.248:50010, bytes: 2292847, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660110734_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742437_1614, duration: 249515077
2015-11-27 16:06:33,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742437_1614, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:35,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742438_1615 src: /192.168.6.248:37748 dest: /192.168.6.248:50010
2015-11-27 16:06:35,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37748, dest: /192.168.6.248:50010, bytes: 2417443, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1905048515_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742438_1615, duration: 259239623
2015-11-27 16:06:35,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742438_1615, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:37,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742439_1616 src: /192.168.6.248:37751 dest: /192.168.6.248:50010
2015-11-27 16:06:37,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37751, dest: /192.168.6.248:50010, bytes: 2295666, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2069650804_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742439_1616, duration: 249595542
2015-11-27 16:06:37,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742439_1616, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:40,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742440_1617 src: /192.168.6.248:37754 dest: /192.168.6.248:50010
2015-11-27 16:06:40,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37754, dest: /192.168.6.248:50010, bytes: 2289573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1424057413_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742440_1617, duration: 247941285
2015-11-27 16:06:40,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742440_1617, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:42,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742441_1618 src: /192.168.6.248:37757 dest: /192.168.6.248:50010
2015-11-27 16:06:42,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37757, dest: /192.168.6.248:50010, bytes: 2293427, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1893650872_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742441_1618, duration: 248900863
2015-11-27 16:06:42,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742441_1618, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:44,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742442_1619 src: /192.168.6.248:37760 dest: /192.168.6.248:50010
2015-11-27 16:06:45,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37760, dest: /192.168.6.248:50010, bytes: 2288666, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1067645573_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742442_1619, duration: 247855414
2015-11-27 16:06:45,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742442_1619, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:47,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742443_1620 src: /192.168.6.248:37763 dest: /192.168.6.248:50010
2015-11-27 16:06:47,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37763, dest: /192.168.6.248:50010, bytes: 2411194, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2011779315_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742443_1620, duration: 257724494
2015-11-27 16:06:47,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742443_1620, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:49,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742444_1621 src: /192.168.6.248:37766 dest: /192.168.6.248:50010
2015-11-27 16:06:49,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37766, dest: /192.168.6.248:50010, bytes: 2285174, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-869177167_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742444_1621, duration: 246653311
2015-11-27 16:06:49,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742444_1621, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:51,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742445_1622 src: /192.168.6.248:37769 dest: /192.168.6.248:50010
2015-11-27 16:06:52,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37769, dest: /192.168.6.248:50010, bytes: 2292583, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_584425715_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742445_1622, duration: 247714214
2015-11-27 16:06:52,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742445_1622, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:54,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742446_1623 src: /192.168.6.248:37772 dest: /192.168.6.248:50010
2015-11-27 16:06:54,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37772, dest: /192.168.6.248:50010, bytes: 2294051, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-857097577_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742446_1623, duration: 248166509
2015-11-27 16:06:54,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742446_1623, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:56,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742447_1624 src: /192.168.6.248:37775 dest: /192.168.6.248:50010
2015-11-27 16:06:56,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37775, dest: /192.168.6.248:50010, bytes: 2292867, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1063317200_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742447_1624, duration: 247969826
2015-11-27 16:06:56,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742447_1624, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:06:58,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742448_1625 src: /192.168.6.248:37778 dest: /192.168.6.248:50010
2015-11-27 16:06:59,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37778, dest: /192.168.6.248:50010, bytes: 2405524, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_143882097_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742448_1625, duration: 259116351
2015-11-27 16:06:59,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742448_1625, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:01,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742449_1626 src: /192.168.6.248:37782 dest: /192.168.6.248:50010
2015-11-27 16:07:01,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37782, dest: /192.168.6.248:50010, bytes: 2289633, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1766073522_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742449_1626, duration: 246990403
2015-11-27 16:07:01,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742449_1626, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:03,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742450_1627 src: /192.168.6.248:37791 dest: /192.168.6.248:50010
2015-11-27 16:07:03,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37791, dest: /192.168.6.248:50010, bytes: 2293870, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1929397648_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742450_1627, duration: 247994959
2015-11-27 16:07:03,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742450_1627, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:05,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742451_1628 src: /192.168.6.248:37794 dest: /192.168.6.248:50010
2015-11-27 16:07:06,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37794, dest: /192.168.6.248:50010, bytes: 2294634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-59698746_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742451_1628, duration: 247583036
2015-11-27 16:07:06,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742451_1628, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:08,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742452_1629 src: /192.168.6.248:37797 dest: /192.168.6.248:50010
2015-11-27 16:07:08,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37797, dest: /192.168.6.248:50010, bytes: 2291622, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_880480733_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742452_1629, duration: 247935150
2015-11-27 16:07:08,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742452_1629, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:10,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742453_1630 src: /192.168.6.248:37800 dest: /192.168.6.248:50010
2015-11-27 16:07:10,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37800, dest: /192.168.6.248:50010, bytes: 2409832, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-946460339_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742453_1630, duration: 258745158
2015-11-27 16:07:10,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742453_1630, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:12,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742454_1631 src: /192.168.6.248:37803 dest: /192.168.6.248:50010
2015-11-27 16:07:13,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37803, dest: /192.168.6.248:50010, bytes: 2285973, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1404823623_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742454_1631, duration: 247838888
2015-11-27 16:07:13,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742454_1631, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:15,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742455_1632 src: /192.168.6.248:37806 dest: /192.168.6.248:50010
2015-11-27 16:07:15,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37806, dest: /192.168.6.248:50010, bytes: 2288342, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1222421418_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742455_1632, duration: 246694565
2015-11-27 16:07:15,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742455_1632, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:17,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742456_1633 src: /192.168.6.248:37809 dest: /192.168.6.248:50010
2015-11-27 16:07:17,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37809, dest: /192.168.6.248:50010, bytes: 2281696, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66515687_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742456_1633, duration: 248965433
2015-11-27 16:07:17,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742456_1633, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:20,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742457_1634 src: /192.168.6.248:37812 dest: /192.168.6.248:50010
2015-11-27 16:07:20,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37812, dest: /192.168.6.248:50010, bytes: 2292596, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_532562631_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742457_1634, duration: 249844896
2015-11-27 16:07:20,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742457_1634, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:22,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742458_1635 src: /192.168.6.248:37815 dest: /192.168.6.248:50010
2015-11-27 16:07:22,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37815, dest: /192.168.6.248:50010, bytes: 2412679, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1344285286_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742458_1635, duration: 260659772
2015-11-27 16:07:22,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742458_1635, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:24,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742459_1636 src: /192.168.6.248:37818 dest: /192.168.6.248:50010
2015-11-27 16:07:25,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37818, dest: /192.168.6.248:50010, bytes: 2293989, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1437073586_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742459_1636, duration: 247980932
2015-11-27 16:07:25,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742459_1636, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:27,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742460_1637 src: /192.168.6.248:37821 dest: /192.168.6.248:50010
2015-11-27 16:07:27,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37821, dest: /192.168.6.248:50010, bytes: 2289798, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-759902944_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742460_1637, duration: 246984743
2015-11-27 16:07:27,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742460_1637, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:29,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742461_1638 src: /192.168.6.248:37824 dest: /192.168.6.248:50010
2015-11-27 16:07:29,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37824, dest: /192.168.6.248:50010, bytes: 2282382, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1270462564_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742461_1638, duration: 247312065
2015-11-27 16:07:29,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742461_1638, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:31,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742462_1639 src: /192.168.6.248:37830 dest: /192.168.6.248:50010
2015-11-27 16:07:32,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37830, dest: /192.168.6.248:50010, bytes: 2294380, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1119755314_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742462_1639, duration: 248276561
2015-11-27 16:07:32,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742462_1639, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:34,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742463_1640 src: /192.168.6.248:37836 dest: /192.168.6.248:50010
2015-11-27 16:07:34,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37836, dest: /192.168.6.248:50010, bytes: 2411515, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-683929355_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742463_1640, duration: 257649758
2015-11-27 16:07:34,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742463_1640, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:36,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742464_1641 src: /192.168.6.248:37839 dest: /192.168.6.248:50010
2015-11-27 16:07:36,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37839, dest: /192.168.6.248:50010, bytes: 2290091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_587936907_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742464_1641, duration: 246731058
2015-11-27 16:07:36,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742464_1641, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:38,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742465_1642 src: /192.168.6.248:37842 dest: /192.168.6.248:50010
2015-11-27 16:07:39,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37842, dest: /192.168.6.248:50010, bytes: 2294522, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-673816849_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742465_1642, duration: 250188051
2015-11-27 16:07:39,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742465_1642, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:41,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742466_1643 src: /192.168.6.248:37845 dest: /192.168.6.248:50010
2015-11-27 16:07:41,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37845, dest: /192.168.6.248:50010, bytes: 2288555, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-406867141_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742466_1643, duration: 247387315
2015-11-27 16:07:41,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742466_1643, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:43,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742467_1644 src: /192.168.6.248:37848 dest: /192.168.6.248:50010
2015-11-27 16:07:43,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37848, dest: /192.168.6.248:50010, bytes: 2287397, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1563458683_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742467_1644, duration: 247069348
2015-11-27 16:07:43,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742467_1644, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:45,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742468_1645 src: /192.168.6.248:37851 dest: /192.168.6.248:50010
2015-11-27 16:07:46,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37851, dest: /192.168.6.248:50010, bytes: 2406340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548125119_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742468_1645, duration: 258560227
2015-11-27 16:07:46,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742468_1645, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:48,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742469_1646 src: /192.168.6.248:37854 dest: /192.168.6.248:50010
2015-11-27 16:07:48,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37854, dest: /192.168.6.248:50010, bytes: 2287959, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1670137118_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742469_1646, duration: 247461389
2015-11-27 16:07:48,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742469_1646, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:50,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742470_1647 src: /192.168.6.248:37857 dest: /192.168.6.248:50010
2015-11-27 16:07:51,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37857, dest: /192.168.6.248:50010, bytes: 2290934, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1673511722_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742470_1647, duration: 250359558
2015-11-27 16:07:51,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742470_1647, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:53,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742471_1648 src: /192.168.6.248:37860 dest: /192.168.6.248:50010
2015-11-27 16:07:53,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37860, dest: /192.168.6.248:50010, bytes: 2294640, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-231086332_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742471_1648, duration: 248578374
2015-11-27 16:07:53,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742471_1648, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:55,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742472_1649 src: /192.168.6.248:37863 dest: /192.168.6.248:50010
2015-11-27 16:07:55,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37863, dest: /192.168.6.248:50010, bytes: 2290170, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_588758593_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742472_1649, duration: 247060288
2015-11-27 16:07:55,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742472_1649, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:07:57,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742473_1650 src: /192.168.6.248:37866 dest: /192.168.6.248:50010
2015-11-27 16:07:58,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37866, dest: /192.168.6.248:50010, bytes: 2405103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1288854922_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742473_1650, duration: 257653686
2015-11-27 16:07:58,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742473_1650, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:00,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742474_1651 src: /192.168.6.248:37870 dest: /192.168.6.248:50010
2015-11-27 16:08:00,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37870, dest: /192.168.6.248:50010, bytes: 2289531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1204476918_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742474_1651, duration: 246800613
2015-11-27 16:08:00,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742474_1651, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:02,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742475_1652 src: /192.168.6.248:37876 dest: /192.168.6.248:50010
2015-11-27 16:08:02,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37876, dest: /192.168.6.248:50010, bytes: 2287622, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1060909720_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742475_1652, duration: 247245648
2015-11-27 16:08:02,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742475_1652, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:04,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742476_1653 src: /192.168.6.248:37882 dest: /192.168.6.248:50010
2015-11-27 16:08:05,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37882, dest: /192.168.6.248:50010, bytes: 2293699, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1738006446_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742476_1653, duration: 248217055
2015-11-27 16:08:05,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742476_1653, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:07,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742477_1654 src: /192.168.6.248:37885 dest: /192.168.6.248:50010
2015-11-27 16:08:07,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37885, dest: /192.168.6.248:50010, bytes: 2287442, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1883509908_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742477_1654, duration: 247411010
2015-11-27 16:08:07,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742477_1654, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:09,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742478_1655 src: /192.168.6.248:37888 dest: /192.168.6.248:50010
2015-11-27 16:08:09,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37888, dest: /192.168.6.248:50010, bytes: 2410600, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-490365154_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742478_1655, duration: 257953437
2015-11-27 16:08:09,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742478_1655, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:11,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742479_1656 src: /192.168.6.248:37891 dest: /192.168.6.248:50010
2015-11-27 16:08:12,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37891, dest: /192.168.6.248:50010, bytes: 2291553, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-670621669_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742479_1656, duration: 246786256
2015-11-27 16:08:12,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742479_1656, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:14,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742480_1657 src: /192.168.6.248:37894 dest: /192.168.6.248:50010
2015-11-27 16:08:14,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37894, dest: /192.168.6.248:50010, bytes: 2290058, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1631830163_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742480_1657, duration: 247442511
2015-11-27 16:08:14,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742480_1657, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:16,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742481_1658 src: /192.168.6.248:37897 dest: /192.168.6.248:50010
2015-11-27 16:08:16,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37897, dest: /192.168.6.248:50010, bytes: 2297579, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_112176155_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742481_1658, duration: 248115641
2015-11-27 16:08:16,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742481_1658, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:18,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742482_1659 src: /192.168.6.248:37900 dest: /192.168.6.248:50010
2015-11-27 16:08:19,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37900, dest: /192.168.6.248:50010, bytes: 2288728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1539988541_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742482_1659, duration: 248598443
2015-11-27 16:08:19,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742482_1659, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:21,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742483_1660 src: /192.168.6.248:37903 dest: /192.168.6.248:50010
2015-11-27 16:08:21,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37903, dest: /192.168.6.248:50010, bytes: 2417254, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2144051889_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742483_1660, duration: 258822694
2015-11-27 16:08:21,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742483_1660, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:23,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742484_1661 src: /192.168.6.248:37906 dest: /192.168.6.248:50010
2015-11-27 16:08:23,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37906, dest: /192.168.6.248:50010, bytes: 2290716, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1143030396_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742484_1661, duration: 247332097
2015-11-27 16:08:23,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742484_1661, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:25,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742485_1662 src: /192.168.6.248:37909 dest: /192.168.6.248:50010
2015-11-27 16:08:26,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37909, dest: /192.168.6.248:50010, bytes: 2295070, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-595900108_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742485_1662, duration: 248571041
2015-11-27 16:08:26,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742485_1662, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:28,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742486_1663 src: /192.168.6.248:37912 dest: /192.168.6.248:50010
2015-11-27 16:08:28,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37912, dest: /192.168.6.248:50010, bytes: 2290339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1581369795_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742486_1663, duration: 246645668
2015-11-27 16:08:28,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742486_1663, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:30,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742487_1664 src: /192.168.6.248:37915 dest: /192.168.6.248:50010
2015-11-27 16:08:30,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37915, dest: /192.168.6.248:50010, bytes: 2290579, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1004951633_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742487_1664, duration: 247716790
2015-11-27 16:08:30,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742487_1664, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:32,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742488_1665 src: /192.168.6.248:37923 dest: /192.168.6.248:50010
2015-11-27 16:08:33,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37923, dest: /192.168.6.248:50010, bytes: 2290451, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2020991002_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742488_1665, duration: 247227694
2015-11-27 16:08:33,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742488_1665, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:35,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742489_1666 src: /192.168.6.248:37927 dest: /192.168.6.248:50010
2015-11-27 16:08:35,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37927, dest: /192.168.6.248:50010, bytes: 2291575, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1286189244_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742489_1666, duration: 249827814
2015-11-27 16:08:35,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742489_1666, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:37,648 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742490_1667 src: /192.168.6.248:37930 dest: /192.168.6.248:50010
2015-11-27 16:08:37,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37930, dest: /192.168.6.248:50010, bytes: 2293784, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_349574531_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742490_1667, duration: 247531053
2015-11-27 16:08:37,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742490_1667, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:39,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742491_1668 src: /192.168.6.248:37933 dest: /192.168.6.248:50010
2015-11-27 16:08:40,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37933, dest: /192.168.6.248:50010, bytes: 2291618, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_264554633_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742491_1668, duration: 247969990
2015-11-27 16:08:40,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742491_1668, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:42,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742492_1669 src: /192.168.6.248:37936 dest: /192.168.6.248:50010
2015-11-27 16:08:42,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37936, dest: /192.168.6.248:50010, bytes: 2292379, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_347390061_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742492_1669, duration: 247685667
2015-11-27 16:08:42,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742492_1669, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:44,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742493_1670 src: /192.168.6.248:37939 dest: /192.168.6.248:50010
2015-11-27 16:08:44,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37939, dest: /192.168.6.248:50010, bytes: 2413436, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1372984698_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742493_1670, duration: 257319842
2015-11-27 16:08:44,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742493_1670, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:47,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742494_1671 src: /192.168.6.248:37942 dest: /192.168.6.248:50010
2015-11-27 16:08:47,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37942, dest: /192.168.6.248:50010, bytes: 2288146, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1098553655_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742494_1671, duration: 247364212
2015-11-27 16:08:47,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742494_1671, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:49,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742495_1672 src: /192.168.6.248:37945 dest: /192.168.6.248:50010
2015-11-27 16:08:49,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37945, dest: /192.168.6.248:50010, bytes: 2291028, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_731875983_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742495_1672, duration: 246860628
2015-11-27 16:08:49,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742495_1672, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:51,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742496_1673 src: /192.168.6.248:37948 dest: /192.168.6.248:50010
2015-11-27 16:08:52,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37948, dest: /192.168.6.248:50010, bytes: 2290339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1833442713_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742496_1673, duration: 247305953
2015-11-27 16:08:52,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742496_1673, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:54,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742497_1674 src: /192.168.6.248:37951 dest: /192.168.6.248:50010
2015-11-27 16:08:54,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37951, dest: /192.168.6.248:50010, bytes: 2296792, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1031105257_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742497_1674, duration: 247988784
2015-11-27 16:08:54,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742497_1674, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:56,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742498_1675 src: /192.168.6.248:37954 dest: /192.168.6.248:50010
2015-11-27 16:08:56,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37954, dest: /192.168.6.248:50010, bytes: 2414274, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_811459237_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742498_1675, duration: 258572139
2015-11-27 16:08:56,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742498_1675, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:08:58,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742499_1676 src: /192.168.6.248:37957 dest: /192.168.6.248:50010
2015-11-27 16:08:59,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37957, dest: /192.168.6.248:50010, bytes: 2288407, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_801677910_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742499_1676, duration: 247258015
2015-11-27 16:08:59,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742499_1676, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:01,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742500_1677 src: /192.168.6.248:37961 dest: /192.168.6.248:50010
2015-11-27 16:09:01,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37961, dest: /192.168.6.248:50010, bytes: 2287770, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_140349992_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742500_1677, duration: 248257413
2015-11-27 16:09:01,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742500_1677, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:03,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742501_1678 src: /192.168.6.248:37970 dest: /192.168.6.248:50010
2015-11-27 16:09:03,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37970, dest: /192.168.6.248:50010, bytes: 2281341, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1247573846_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742501_1678, duration: 247089285
2015-11-27 16:09:03,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742501_1678, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:05,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742502_1679 src: /192.168.6.248:37973 dest: /192.168.6.248:50010
2015-11-27 16:09:06,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37973, dest: /192.168.6.248:50010, bytes: 2293076, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2035452382_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742502_1679, duration: 247687110
2015-11-27 16:09:06,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742502_1679, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:08,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742503_1680 src: /192.168.6.248:37976 dest: /192.168.6.248:50010
2015-11-27 16:09:08,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37976, dest: /192.168.6.248:50010, bytes: 2409139, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1818995334_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742503_1680, duration: 257458017
2015-11-27 16:09:08,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742503_1680, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:10,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742504_1681 src: /192.168.6.248:37979 dest: /192.168.6.248:50010
2015-11-27 16:09:10,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37979, dest: /192.168.6.248:50010, bytes: 2293932, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1575312172_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742504_1681, duration: 247265007
2015-11-27 16:09:10,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742504_1681, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:12,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742505_1682 src: /192.168.6.248:37982 dest: /192.168.6.248:50010
2015-11-27 16:09:13,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37982, dest: /192.168.6.248:50010, bytes: 2288022, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1052252575_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742505_1682, duration: 246780742
2015-11-27 16:09:13,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742505_1682, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:15,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742506_1683 src: /192.168.6.248:37985 dest: /192.168.6.248:50010
2015-11-27 16:09:15,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37985, dest: /192.168.6.248:50010, bytes: 2285595, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-604575427_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742506_1683, duration: 247644073
2015-11-27 16:09:15,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742506_1683, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:17,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742507_1684 src: /192.168.6.248:37988 dest: /192.168.6.248:50010
2015-11-27 16:09:17,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37988, dest: /192.168.6.248:50010, bytes: 2286577, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1975174841_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742507_1684, duration: 247336501
2015-11-27 16:09:17,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742507_1684, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:19,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742508_1685 src: /192.168.6.248:37991 dest: /192.168.6.248:50010
2015-11-27 16:09:20,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37991, dest: /192.168.6.248:50010, bytes: 2410555, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-242709526_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742508_1685, duration: 258506395
2015-11-27 16:09:20,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742508_1685, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:22,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742509_1686 src: /192.168.6.248:37994 dest: /192.168.6.248:50010
2015-11-27 16:09:22,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37994, dest: /192.168.6.248:50010, bytes: 2289188, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_382709483_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742509_1686, duration: 249878322
2015-11-27 16:09:22,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742509_1686, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:24,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742510_1687 src: /192.168.6.248:37997 dest: /192.168.6.248:50010
2015-11-27 16:09:24,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:37997, dest: /192.168.6.248:50010, bytes: 2294779, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1204064585_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742510_1687, duration: 247862927
2015-11-27 16:09:24,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742510_1687, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:26,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742511_1688 src: /192.168.6.248:38000 dest: /192.168.6.248:50010
2015-11-27 16:09:27,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38000, dest: /192.168.6.248:50010, bytes: 2291583, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_756168246_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742511_1688, duration: 247092447
2015-11-27 16:09:27,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742511_1688, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:29,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742512_1689 src: /192.168.6.248:38003 dest: /192.168.6.248:50010
2015-11-27 16:09:29,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38003, dest: /192.168.6.248:50010, bytes: 2290985, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_477747833_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742512_1689, duration: 248047619
2015-11-27 16:09:29,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742512_1689, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:31,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742513_1690 src: /192.168.6.248:38007 dest: /192.168.6.248:50010
2015-11-27 16:09:31,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38007, dest: /192.168.6.248:50010, bytes: 2404324, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2140082222_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742513_1690, duration: 257726839
2015-11-27 16:09:31,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742513_1690, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:33,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742514_1691 src: /192.168.6.248:38015 dest: /192.168.6.248:50010
2015-11-27 16:09:34,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38015, dest: /192.168.6.248:50010, bytes: 2291599, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1585879922_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742514_1691, duration: 247449950
2015-11-27 16:09:34,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742514_1691, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:36,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742515_1692 src: /192.168.6.248:38018 dest: /192.168.6.248:50010
2015-11-27 16:09:36,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38018, dest: /192.168.6.248:50010, bytes: 2289176, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_981701545_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742515_1692, duration: 248125110
2015-11-27 16:09:36,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742515_1692, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:38,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742516_1693 src: /192.168.6.248:38021 dest: /192.168.6.248:50010
2015-11-27 16:09:39,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38021, dest: /192.168.6.248:50010, bytes: 2288806, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2039814022_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742516_1693, duration: 247631409
2015-11-27 16:09:39,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742516_1693, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:41,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742517_1694 src: /192.168.6.248:38024 dest: /192.168.6.248:50010
2015-11-27 16:09:41,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38024, dest: /192.168.6.248:50010, bytes: 2294713, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_345499742_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742517_1694, duration: 247569268
2015-11-27 16:09:41,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742517_1694, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:43,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742518_1695 src: /192.168.6.248:38027 dest: /192.168.6.248:50010
2015-11-27 16:09:43,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38027, dest: /192.168.6.248:50010, bytes: 2409715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_665135682_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742518_1695, duration: 258729764
2015-11-27 16:09:43,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742518_1695, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:45,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742519_1696 src: /192.168.6.248:38030 dest: /192.168.6.248:50010
2015-11-27 16:09:46,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38030, dest: /192.168.6.248:50010, bytes: 2291136, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1469125467_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742519_1696, duration: 247902450
2015-11-27 16:09:46,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742519_1696, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:48,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742520_1697 src: /192.168.6.248:38033 dest: /192.168.6.248:50010
2015-11-27 16:09:48,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38033, dest: /192.168.6.248:50010, bytes: 2284262, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_466223941_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742520_1697, duration: 247182507
2015-11-27 16:09:48,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742520_1697, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:50,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742521_1698 src: /192.168.6.248:38036 dest: /192.168.6.248:50010
2015-11-27 16:09:50,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38036, dest: /192.168.6.248:50010, bytes: 2280566, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_448519385_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742521_1698, duration: 246897373
2015-11-27 16:09:50,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742521_1698, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:52,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742522_1699 src: /192.168.6.248:38039 dest: /192.168.6.248:50010
2015-11-27 16:09:53,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38039, dest: /192.168.6.248:50010, bytes: 2287500, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2087922707_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742522_1699, duration: 247497944
2015-11-27 16:09:53,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742522_1699, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:55,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742523_1700 src: /192.168.6.248:38042 dest: /192.168.6.248:50010
2015-11-27 16:09:55,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38042, dest: /192.168.6.248:50010, bytes: 2415405, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1507235687_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742523_1700, duration: 258541821
2015-11-27 16:09:55,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742523_1700, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:57,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742524_1701 src: /192.168.6.248:38045 dest: /192.168.6.248:50010
2015-11-27 16:09:57,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38045, dest: /192.168.6.248:50010, bytes: 2287709, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327612690_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742524_1701, duration: 246300922
2015-11-27 16:09:57,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742524_1701, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:09:59,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742525_1702 src: /192.168.6.248:38049 dest: /192.168.6.248:50010
2015-11-27 16:10:00,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38049, dest: /192.168.6.248:50010, bytes: 2291512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_46829782_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742525_1702, duration: 247501850
2015-11-27 16:10:00,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742525_1702, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:02,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742526_1703 src: /192.168.6.248:38055 dest: /192.168.6.248:50010
2015-11-27 16:10:02,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38055, dest: /192.168.6.248:50010, bytes: 2285659, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-528799370_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742526_1703, duration: 246539812
2015-11-27 16:10:02,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742526_1703, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:04,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742527_1704 src: /192.168.6.248:38061 dest: /192.168.6.248:50010
2015-11-27 16:10:04,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38061, dest: /192.168.6.248:50010, bytes: 2286916, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1991565011_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742527_1704, duration: 247702477
2015-11-27 16:10:04,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742527_1704, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:06,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742528_1705 src: /192.168.6.248:38064 dest: /192.168.6.248:50010
2015-11-27 16:10:07,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38064, dest: /192.168.6.248:50010, bytes: 2411555, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_753683436_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742528_1705, duration: 257700333
2015-11-27 16:10:07,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742528_1705, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:09,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742529_1706 src: /192.168.6.248:38067 dest: /192.168.6.248:50010
2015-11-27 16:10:09,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38067, dest: /192.168.6.248:50010, bytes: 2295497, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-895026993_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742529_1706, duration: 247370972
2015-11-27 16:10:09,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742529_1706, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:11,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742530_1707 src: /192.168.6.248:38070 dest: /192.168.6.248:50010
2015-11-27 16:10:11,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38070, dest: /192.168.6.248:50010, bytes: 2295421, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1988921987_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742530_1707, duration: 248062609
2015-11-27 16:10:11,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742530_1707, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:13,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742531_1708 src: /192.168.6.248:38073 dest: /192.168.6.248:50010
2015-11-27 16:10:14,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38073, dest: /192.168.6.248:50010, bytes: 2290787, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1730978907_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742531_1708, duration: 246415844
2015-11-27 16:10:14,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742531_1708, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:16,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742532_1709 src: /192.168.6.248:38076 dest: /192.168.6.248:50010
2015-11-27 16:10:16,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38076, dest: /192.168.6.248:50010, bytes: 2290510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2114056552_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742532_1709, duration: 247640628
2015-11-27 16:10:16,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742532_1709, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:18,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742533_1710 src: /192.168.6.248:38079 dest: /192.168.6.248:50010
2015-11-27 16:10:18,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38079, dest: /192.168.6.248:50010, bytes: 2409806, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1356289429_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742533_1710, duration: 257447362
2015-11-27 16:10:18,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742533_1710, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:21,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742534_1711 src: /192.168.6.248:38082 dest: /192.168.6.248:50010
2015-11-27 16:10:21,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38082, dest: /192.168.6.248:50010, bytes: 2286193, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-380614335_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742534_1711, duration: 246966111
2015-11-27 16:10:21,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742534_1711, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:23,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742535_1712 src: /192.168.6.248:38085 dest: /192.168.6.248:50010
2015-11-27 16:10:23,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38085, dest: /192.168.6.248:50010, bytes: 2294986, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-353357989_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742535_1712, duration: 247991800
2015-11-27 16:10:23,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742535_1712, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:25,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742536_1713 src: /192.168.6.248:38088 dest: /192.168.6.248:50010
2015-11-27 16:10:25,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38088, dest: /192.168.6.248:50010, bytes: 2293569, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66887566_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742536_1713, duration: 248029083
2015-11-27 16:10:25,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742536_1713, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:28,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742537_1714 src: /192.168.6.248:38091 dest: /192.168.6.248:50010
2015-11-27 16:10:28,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38091, dest: /192.168.6.248:50010, bytes: 2292344, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2140712317_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742537_1714, duration: 260594360
2015-11-27 16:10:28,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742537_1714, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:30,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742538_1715 src: /192.168.6.248:38094 dest: /192.168.6.248:50010
2015-11-27 16:10:30,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38094, dest: /192.168.6.248:50010, bytes: 2412010, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_534199632_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742538_1715, duration: 258417538
2015-11-27 16:10:30,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742538_1715, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:32,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742539_1716 src: /192.168.6.248:38100 dest: /192.168.6.248:50010
2015-11-27 16:10:33,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38100, dest: /192.168.6.248:50010, bytes: 2294494, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-388834464_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742539_1716, duration: 247518742
2015-11-27 16:10:33,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742539_1716, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:35,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742540_1717 src: /192.168.6.248:38106 dest: /192.168.6.248:50010
2015-11-27 16:10:35,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38106, dest: /192.168.6.248:50010, bytes: 2290858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1867781145_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742540_1717, duration: 247503343
2015-11-27 16:10:35,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742540_1717, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:37,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742541_1718 src: /192.168.6.248:38109 dest: /192.168.6.248:50010
2015-11-27 16:10:37,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38109, dest: /192.168.6.248:50010, bytes: 2285050, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1403339187_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742541_1718, duration: 247291886
2015-11-27 16:10:37,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742541_1718, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:39,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742542_1719 src: /192.168.6.248:38112 dest: /192.168.6.248:50010
2015-11-27 16:10:39,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38112, dest: /192.168.6.248:50010, bytes: 2290669, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-141531338_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742542_1719, duration: 247661079
2015-11-27 16:10:39,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742542_1719, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:42,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742543_1720 src: /192.168.6.248:38115 dest: /192.168.6.248:50010
2015-11-27 16:10:42,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38115, dest: /192.168.6.248:50010, bytes: 2405136, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_797389139_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742543_1720, duration: 257170372
2015-11-27 16:10:42,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742543_1720, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:44,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742544_1721 src: /192.168.6.248:38118 dest: /192.168.6.248:50010
2015-11-27 16:10:44,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38118, dest: /192.168.6.248:50010, bytes: 2291803, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_434157951_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742544_1721, duration: 249269449
2015-11-27 16:10:44,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742544_1721, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:46,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742545_1722 src: /192.168.6.248:38121 dest: /192.168.6.248:50010
2015-11-27 16:10:46,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38121, dest: /192.168.6.248:50010, bytes: 2293676, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1373058181_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742545_1722, duration: 248411637
2015-11-27 16:10:46,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742545_1722, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:49,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742546_1723 src: /192.168.6.248:38124 dest: /192.168.6.248:50010
2015-11-27 16:10:49,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38124, dest: /192.168.6.248:50010, bytes: 2287033, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1288091342_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742546_1723, duration: 246912001
2015-11-27 16:10:49,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742546_1723, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:51,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742547_1724 src: /192.168.6.248:38127 dest: /192.168.6.248:50010
2015-11-27 16:10:51,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38127, dest: /192.168.6.248:50010, bytes: 2284640, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1471436280_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742547_1724, duration: 247858658
2015-11-27 16:10:51,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742547_1724, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:53,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742548_1725 src: /192.168.6.248:38130 dest: /192.168.6.248:50010
2015-11-27 16:10:54,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38130, dest: /192.168.6.248:50010, bytes: 2409768, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1333765134_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742548_1725, duration: 258644798
2015-11-27 16:10:54,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742548_1725, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:56,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742549_1726 src: /192.168.6.248:38133 dest: /192.168.6.248:50010
2015-11-27 16:10:56,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38133, dest: /192.168.6.248:50010, bytes: 2292936, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1596964327_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742549_1726, duration: 247303808
2015-11-27 16:10:56,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742549_1726, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:10:58,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742550_1727 src: /192.168.6.248:38136 dest: /192.168.6.248:50010
2015-11-27 16:10:58,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38136, dest: /192.168.6.248:50010, bytes: 2287164, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1478570429_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742550_1727, duration: 247627229
2015-11-27 16:10:58,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742550_1727, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:00,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742551_1728 src: /192.168.6.248:38140 dest: /192.168.6.248:50010
2015-11-27 16:11:01,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38140, dest: /192.168.6.248:50010, bytes: 2295193, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1064300927_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742551_1728, duration: 247342826
2015-11-27 16:11:01,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742551_1728, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:03,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742552_1729 src: /192.168.6.248:38148 dest: /192.168.6.248:50010
2015-11-27 16:11:03,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38148, dest: /192.168.6.248:50010, bytes: 2295340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1063432796_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742552_1729, duration: 248654201
2015-11-27 16:11:03,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742552_1729, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:05,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742553_1730 src: /192.168.6.248:38152 dest: /192.168.6.248:50010
2015-11-27 16:11:05,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38152, dest: /192.168.6.248:50010, bytes: 2411079, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_54619902_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742553_1730, duration: 257975832
2015-11-27 16:11:05,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742553_1730, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:07,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742554_1731 src: /192.168.6.248:38155 dest: /192.168.6.248:50010
2015-11-27 16:11:08,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38155, dest: /192.168.6.248:50010, bytes: 2288391, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1409549092_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742554_1731, duration: 247465864
2015-11-27 16:11:08,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742554_1731, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:10,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742555_1732 src: /192.168.6.248:38158 dest: /192.168.6.248:50010
2015-11-27 16:11:10,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38158, dest: /192.168.6.248:50010, bytes: 2295191, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-334157337_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742555_1732, duration: 247924895
2015-11-27 16:11:10,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742555_1732, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:12,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742556_1733 src: /192.168.6.248:38161 dest: /192.168.6.248:50010
2015-11-27 16:11:12,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38161, dest: /192.168.6.248:50010, bytes: 2291457, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_918250182_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742556_1733, duration: 247357113
2015-11-27 16:11:12,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742556_1733, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:14,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742557_1734 src: /192.168.6.248:38164 dest: /192.168.6.248:50010
2015-11-27 16:11:15,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38164, dest: /192.168.6.248:50010, bytes: 2296212, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1444621994_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742557_1734, duration: 248601177
2015-11-27 16:11:15,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742557_1734, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:17,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742558_1735 src: /192.168.6.248:38167 dest: /192.168.6.248:50010
2015-11-27 16:11:17,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38167, dest: /192.168.6.248:50010, bytes: 2410532, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1817662981_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742558_1735, duration: 258290592
2015-11-27 16:11:17,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742558_1735, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:19,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742559_1736 src: /192.168.6.248:38170 dest: /192.168.6.248:50010
2015-11-27 16:11:19,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38170, dest: /192.168.6.248:50010, bytes: 2285163, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_363775666_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742559_1736, duration: 247265395
2015-11-27 16:11:19,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742559_1736, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:21,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742560_1737 src: /192.168.6.248:38173 dest: /192.168.6.248:50010
2015-11-27 16:11:22,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38173, dest: /192.168.6.248:50010, bytes: 2285120, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1287098855_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742560_1737, duration: 247745830
2015-11-27 16:11:22,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742560_1737, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:24,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742561_1738 src: /192.168.6.248:38176 dest: /192.168.6.248:50010
2015-11-27 16:11:24,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38176, dest: /192.168.6.248:50010, bytes: 2289274, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_38346166_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742561_1738, duration: 250084396
2015-11-27 16:11:24,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742561_1738, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:26,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742562_1739 src: /192.168.6.248:38179 dest: /192.168.6.248:50010
2015-11-27 16:11:26,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38179, dest: /192.168.6.248:50010, bytes: 2292194, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_482988911_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742562_1739, duration: 247859420
2015-11-27 16:11:26,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742562_1739, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:29,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742563_1740 src: /192.168.6.248:38182 dest: /192.168.6.248:50010
2015-11-27 16:11:29,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38182, dest: /192.168.6.248:50010, bytes: 2406688, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1458725692_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742563_1740, duration: 258388497
2015-11-27 16:11:29,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742563_1740, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:31,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742564_1741 src: /192.168.6.248:38185 dest: /192.168.6.248:50010
2015-11-27 16:11:31,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38185, dest: /192.168.6.248:50010, bytes: 2291891, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_453490172_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742564_1741, duration: 247964255
2015-11-27 16:11:31,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742564_1741, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:33,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742565_1742 src: /192.168.6.248:38194 dest: /192.168.6.248:50010
2015-11-27 16:11:33,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38194, dest: /192.168.6.248:50010, bytes: 2291798, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629623637_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742565_1742, duration: 247598339
2015-11-27 16:11:33,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742565_1742, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:35,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742566_1743 src: /192.168.6.248:38197 dest: /192.168.6.248:50010
2015-11-27 16:11:36,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38197, dest: /192.168.6.248:50010, bytes: 2284708, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_634115115_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742566_1743, duration: 247001511
2015-11-27 16:11:36,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742566_1743, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:38,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742567_1744 src: /192.168.6.248:38200 dest: /192.168.6.248:50010
2015-11-27 16:11:38,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38200, dest: /192.168.6.248:50010, bytes: 2290727, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-297124875_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742567_1744, duration: 247388277
2015-11-27 16:11:38,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742567_1744, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:40,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742568_1745 src: /192.168.6.248:38203 dest: /192.168.6.248:50010
2015-11-27 16:11:40,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38203, dest: /192.168.6.248:50010, bytes: 2411475, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-823563717_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742568_1745, duration: 257875723
2015-11-27 16:11:40,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742568_1745, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:42,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742569_1746 src: /192.168.6.248:38206 dest: /192.168.6.248:50010
2015-11-27 16:11:43,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38206, dest: /192.168.6.248:50010, bytes: 2289378, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1659635363_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742569_1746, duration: 246933580
2015-11-27 16:11:43,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742569_1746, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:45,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742570_1747 src: /192.168.6.248:38209 dest: /192.168.6.248:50010
2015-11-27 16:11:45,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38209, dest: /192.168.6.248:50010, bytes: 2290641, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-325135083_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742570_1747, duration: 247874902
2015-11-27 16:11:45,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742570_1747, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:47,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742571_1748 src: /192.168.6.248:38212 dest: /192.168.6.248:50010
2015-11-27 16:11:47,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38212, dest: /192.168.6.248:50010, bytes: 2289353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2100375598_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742571_1748, duration: 247856692
2015-11-27 16:11:47,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742571_1748, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:49,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742572_1749 src: /192.168.6.248:38215 dest: /192.168.6.248:50010
2015-11-27 16:11:50,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38215, dest: /192.168.6.248:50010, bytes: 2284789, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-801357073_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742572_1749, duration: 246130735
2015-11-27 16:11:50,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742572_1749, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:52,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742573_1750 src: /192.168.6.248:38218 dest: /192.168.6.248:50010
2015-11-27 16:11:52,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38218, dest: /192.168.6.248:50010, bytes: 2405465, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_883669965_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742573_1750, duration: 260100145
2015-11-27 16:11:52,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742573_1750, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:54,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742574_1751 src: /192.168.6.248:38221 dest: /192.168.6.248:50010
2015-11-27 16:11:54,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38221, dest: /192.168.6.248:50010, bytes: 2290537, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_20797540_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742574_1751, duration: 247925556
2015-11-27 16:11:54,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742574_1751, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:56,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742575_1752 src: /192.168.6.248:38225 dest: /192.168.6.248:50010
2015-11-27 16:11:57,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38225, dest: /192.168.6.248:50010, bytes: 2294357, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1526895203_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742575_1752, duration: 248232585
2015-11-27 16:11:57,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742575_1752, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:11:59,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742576_1753 src: /192.168.6.248:38229 dest: /192.168.6.248:50010
2015-11-27 16:11:59,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38229, dest: /192.168.6.248:50010, bytes: 2286718, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1778170340_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742576_1753, duration: 248303724
2015-11-27 16:11:59,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742576_1753, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:01,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742577_1754 src: /192.168.6.248:38233 dest: /192.168.6.248:50010
2015-11-27 16:12:01,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38233, dest: /192.168.6.248:50010, bytes: 2291085, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-466440332_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742577_1754, duration: 247477618
2015-11-27 16:12:01,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742577_1754, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:04,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742578_1755 src: /192.168.6.248:38247 dest: /192.168.6.248:50010
2015-11-27 16:12:04,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38247, dest: /192.168.6.248:50010, bytes: 2406971, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-983385689_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742578_1755, duration: 257922717
2015-11-27 16:12:04,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742578_1755, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:06,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742579_1756 src: /192.168.6.248:38250 dest: /192.168.6.248:50010
2015-11-27 16:12:06,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38250, dest: /192.168.6.248:50010, bytes: 2284507, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_428444440_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742579_1756, duration: 288992818
2015-11-27 16:12:06,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742579_1756, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:08,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742580_1757 src: /192.168.6.248:38253 dest: /192.168.6.248:50010
2015-11-27 16:12:09,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38253, dest: /192.168.6.248:50010, bytes: 2290973, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-256401522_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742580_1757, duration: 251982841
2015-11-27 16:12:09,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742580_1757, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:11,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742581_1758 src: /192.168.6.248:38256 dest: /192.168.6.248:50010
2015-11-27 16:12:11,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38256, dest: /192.168.6.248:50010, bytes: 2286440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1600554755_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742581_1758, duration: 246636684
2015-11-27 16:12:11,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742581_1758, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:13,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742582_1759 src: /192.168.6.248:38259 dest: /192.168.6.248:50010
2015-11-27 16:12:13,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38259, dest: /192.168.6.248:50010, bytes: 2297078, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1096585532_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742582_1759, duration: 248626957
2015-11-27 16:12:13,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742582_1759, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:15,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742583_1760 src: /192.168.6.248:38262 dest: /192.168.6.248:50010
2015-11-27 16:12:16,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38262, dest: /192.168.6.248:50010, bytes: 2411844, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_270814248_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742583_1760, duration: 257772966
2015-11-27 16:12:16,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742583_1760, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:18,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742584_1761 src: /192.168.6.248:38265 dest: /192.168.6.248:50010
2015-11-27 16:12:18,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38265, dest: /192.168.6.248:50010, bytes: 2287077, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1351436074_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742584_1761, duration: 247044387
2015-11-27 16:12:18,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742584_1761, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:20,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742585_1762 src: /192.168.6.248:38268 dest: /192.168.6.248:50010
2015-11-27 16:12:20,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38268, dest: /192.168.6.248:50010, bytes: 2289374, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1224349465_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742585_1762, duration: 246737698
2015-11-27 16:12:20,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742585_1762, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:22,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742586_1763 src: /192.168.6.248:38271 dest: /192.168.6.248:50010
2015-11-27 16:12:23,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38271, dest: /192.168.6.248:50010, bytes: 2291717, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1988533966_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742586_1763, duration: 248024486
2015-11-27 16:12:23,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742586_1763, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:25,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742587_1764 src: /192.168.6.248:38274 dest: /192.168.6.248:50010
2015-11-27 16:12:25,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38274, dest: /192.168.6.248:50010, bytes: 2292447, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-909237142_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742587_1764, duration: 247168658
2015-11-27 16:12:25,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742587_1764, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:27,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742588_1765 src: /192.168.6.248:38277 dest: /192.168.6.248:50010
2015-11-27 16:12:27,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38277, dest: /192.168.6.248:50010, bytes: 2408066, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_229439443_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742588_1765, duration: 257688577
2015-11-27 16:12:27,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742588_1765, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:29,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742589_1766 src: /192.168.6.248:38286 dest: /192.168.6.248:50010
2015-11-27 16:12:30,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38286, dest: /192.168.6.248:50010, bytes: 2292458, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1423866499_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742589_1766, duration: 247843428
2015-11-27 16:12:30,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742589_1766, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:32,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742590_1767 src: /192.168.6.248:38292 dest: /192.168.6.248:50010
2015-11-27 16:12:32,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38292, dest: /192.168.6.248:50010, bytes: 2296975, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1693992949_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742590_1767, duration: 256504532
2015-11-27 16:12:32,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742590_1767, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:34,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742591_1768 src: /192.168.6.248:38298 dest: /192.168.6.248:50010
2015-11-27 16:12:34,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38298, dest: /192.168.6.248:50010, bytes: 2277585, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_680410185_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742591_1768, duration: 247021393
2015-11-27 16:12:34,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742591_1768, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:36,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742592_1769 src: /192.168.6.248:38301 dest: /192.168.6.248:50010
2015-11-27 16:12:37,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38301, dest: /192.168.6.248:50010, bytes: 2288012, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1175118199_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742592_1769, duration: 247769261
2015-11-27 16:12:37,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742592_1769, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:39,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742593_1770 src: /192.168.6.248:38304 dest: /192.168.6.248:50010
2015-11-27 16:12:39,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38304, dest: /192.168.6.248:50010, bytes: 2402830, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-581942997_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742593_1770, duration: 256582706
2015-11-27 16:12:39,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742593_1770, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:41,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742594_1771 src: /192.168.6.248:38307 dest: /192.168.6.248:50010
2015-11-27 16:12:42,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38307, dest: /192.168.6.248:50010, bytes: 2298836, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_803795117_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742594_1771, duration: 248157957
2015-11-27 16:12:42,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742594_1771, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:44,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742595_1772 src: /192.168.6.248:38310 dest: /192.168.6.248:50010
2015-11-27 16:12:44,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38310, dest: /192.168.6.248:50010, bytes: 2291904, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1729750434_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742595_1772, duration: 247015906
2015-11-27 16:12:44,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742595_1772, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:46,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742596_1773 src: /192.168.6.248:38313 dest: /192.168.6.248:50010
2015-11-27 16:12:46,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38313, dest: /192.168.6.248:50010, bytes: 2288912, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_341225210_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742596_1773, duration: 246367128
2015-11-27 16:12:46,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742596_1773, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:49,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742597_1774 src: /192.168.6.248:38316 dest: /192.168.6.248:50010
2015-11-27 16:12:49,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38316, dest: /192.168.6.248:50010, bytes: 2289796, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1134970997_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742597_1774, duration: 250054211
2015-11-27 16:12:49,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742597_1774, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:12:51,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742598_1775 src: /192.168.6.248:38319 dest: /192.168.6.248:50010
2015-11-27 16:12:51,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:38319, dest: /192.168.6.248:50010, bytes: 2275376, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-30769806_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742598_1775, duration: 245435406
2015-11-27 16:12:51,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742598_1775, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 16:21:07,985 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 16:21:07,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-30 16:38:04,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-30 16:38:04,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-30 16:38:05,324 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-30 16:38:05,386 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-30 16:38:05,387 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-30 16:38:05,392 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-30 16:38:05,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-30 16:38:05,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-30 16:38:05,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-30 16:38:05,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-30 16:38:05,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-30 16:38:05,552 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-30 16:38:05,560 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-30 16:38:05,565 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-30 16:38:05,570 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-30 16:38:05,572 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-30 16:38:05,572 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-30 16:38:05,572 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-30 16:38:05,582 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 32896
2015-11-30 16:38:05,582 INFO org.mortbay.log: jetty-6.1.26
2015-11-30 16:38:05,736 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:32896
2015-11-30 16:38:05,889 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-30 16:38:05,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-30 16:38:05,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-30 16:38:05,970 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-30 16:38:05,988 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-30 16:38:06,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-30 16:38:06,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-30 16:38:06,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-30 16:38:06,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-30 16:38:06,112 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-30 16:38:06,113 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-30 16:38:06,532 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 13170@rushikesh1
2015-11-30 16:38:06,638 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-30 16:38:06,639 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-30 16:38:06,639 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-30 16:38:06,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-30 16:38:06,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-30 16:38:06,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-30 16:38:06,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-30 16:38:06,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-30 16:38:06,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-30 16:38:06,875 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 77ms
2015-11-30 16:38:06,875 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 78ms
2015-11-30 16:38:06,875 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-30 16:38:06,940 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 64ms
2015-11-30 16:38:06,940 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 65ms
2015-11-30 16:38:07,247 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 669321131 ms.
2015-11-30 16:38:07,249 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448893331249 with interval 21600000
2015-11-30 16:38:07,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-30 16:38:07,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-30 16:38:07,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-30 16:38:07,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3638
2015-11-30 16:38:07,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-30 16:38:07,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc66dd801a0f,  containing 1 storage report(s), of which we sent 1. The reports had 483 total blocks and used 1 RPC(s). This took 9 msec to generate and 116 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-30 16:38:07,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-30 16:38:42,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 to 192.168.6.238:50010 
2015-11-30 16:38:42,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742175_1351 to 192.168.6.238:50010 
2015-11-30 16:39:06,408 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742175_1351 for rescanning.
2015-11-30 16:39:06,409 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-11-30 16:39:06,409 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0):Failed to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742175_1351 to 192.168.6.238:50010 got 
java.net.SocketException: Original Exception : java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
	at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:434)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:566)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:579)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:759)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:706)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2126)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	... 9 more
2015-11-30 16:39:06,411 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting.
2015-11-30 16:39:06,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting CheckDiskError Thread
2015-11-30 16:39:09,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742164_1340 to 192.168.6.238:50010 
2015-11-30 16:39:24,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 (numBytes=134217728) to /192.168.6.238:50010
2015-11-30 16:39:27,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742162_1338 to 192.168.6.238:50010 
2015-11-30 16:39:45,081 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-30 16:39:47,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742164_1340 (numBytes=134217728) to /192.168.6.238:50010
2015-11-30 16:39:49,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-30 16:39:49,366 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-30 16:39:49,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-11-30 16:41:01,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-30 16:41:01,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-30 16:41:01,655 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-30 16:41:01,718 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-30 16:41:01,718 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-30 16:41:01,723 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-30 16:41:01,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-11-30 16:41:01,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-30 16:41:01,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-30 16:41:01,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-30 16:41:01,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-30 16:41:01,842 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-30 16:41:01,849 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-30 16:41:01,855 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-30 16:41:01,860 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-30 16:41:01,862 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-30 16:41:01,862 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-30 16:41:01,862 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-30 16:41:01,872 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 56519
2015-11-30 16:41:01,872 INFO org.mortbay.log: jetty-6.1.26
2015-11-30 16:41:02,026 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:56519
2015-11-30 16:41:02,105 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-30 16:41:02,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-30 16:41:02,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-30 16:41:02,145 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-30 16:41:02,156 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-30 16:41:02,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-30 16:41:02,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-30 16:41:02,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-30 16:41:02,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-30 16:41:02,235 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-30 16:41:02,235 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-30 16:41:02,571 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 15087@rushikesh1
2015-11-30 16:41:02,673 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-30 16:41:02,673 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-30 16:41:02,674 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-30 16:41:02,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-11-30 16:41:02,769 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-11-30 16:41:02,769 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-30 16:41:02,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-30 16:41:02,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-30 16:41:02,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-30 16:41:02,814 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35659497472
2015-11-30 16:41:02,815 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 14ms
2015-11-30 16:41:02,815 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 15ms
2015-11-30 16:41:02,816 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-30 16:41:02,883 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 66ms
2015-11-30 16:41:02,883 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 67ms
2015-11-30 16:41:03,089 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 669145289 ms.
2015-11-30 16:41:03,091 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448900869091 with interval 21600000
2015-11-30 16:41:03,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-30 16:41:03,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-30 16:41:03,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-30 16:41:03,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3641
2015-11-30 16:41:03,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-11-30 16:41:03,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc8fc89f7b70,  containing 1 storage report(s), of which we sent 1. The reports had 483 total blocks and used 1 RPC(s). This took 4 msec to generate and 40 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-30 16:41:03,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-30 16:51:35,557 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 for rescanning.
2015-11-30 16:51:35,558 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-11-30 16:51:35,559 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8) exiting.
2015-11-30 16:51:35,624 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 for rescanning.
2015-11-30 16:51:36,140 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 for rescanning.
2015-11-30 16:51:36,202 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 for rescanning.
2015-11-30 16:51:36,711 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 for rescanning.
2015-11-30 16:51:36,738 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 for rescanning.
2015-11-30 16:51:37,156 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 for rescanning.
2015-11-30 16:51:37,224 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 for rescanning.
2015-11-30 16:51:37,775 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 for rescanning.
2015-11-30 16:51:37,836 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 for rescanning.
2015-11-30 16:51:38,180 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 for rescanning.
2015-11-30 16:51:38,248 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 for rescanning.
2015-11-30 16:51:38,646 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 for rescanning.
2015-11-30 16:51:38,691 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 for rescanning.
2015-11-30 16:51:39,086 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 for rescanning.
2015-11-30 16:51:39,138 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 for rescanning.
2015-11-30 16:51:39,578 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 for rescanning.
2015-11-30 16:51:39,629 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 for rescanning.
2015-11-30 16:51:40,003 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 for rescanning.
2015-11-30 16:51:40,074 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 for rescanning.
2015-11-30 16:51:40,425 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 for rescanning.
2015-11-30 16:51:40,458 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 for rescanning.
2015-11-30 16:51:40,837 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 for rescanning.
2015-11-30 16:51:40,909 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 for rescanning.
2015-11-30 16:51:41,347 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 for rescanning.
2015-11-30 16:51:41,408 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 for rescanning.
2015-11-30 16:51:41,788 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 for rescanning.
2015-11-30 16:51:41,833 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 for rescanning.
2015-11-30 16:51:42,224 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 for rescanning.
2015-11-30 16:51:42,251 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 for rescanning.
2015-11-30 16:51:42,632 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 for rescanning.
2015-11-30 16:51:42,696 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 for rescanning.
2015-11-30 16:51:43,095 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 for rescanning.
2015-11-30 16:51:43,161 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 for rescanning.
2015-11-30 16:51:43,515 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 for rescanning.
2015-11-30 16:51:43,574 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 for rescanning.
2015-11-30 16:51:43,942 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 for rescanning.
2015-11-30 16:51:44,021 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 for rescanning.
2015-11-30 16:51:44,416 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 for rescanning.
2015-11-30 16:51:44,486 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 for rescanning.
2015-11-30 16:51:44,921 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 for rescanning.
2015-11-30 16:51:44,977 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 for rescanning.
2015-11-30 16:51:45,333 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 for rescanning.
2015-11-30 16:51:45,386 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 for rescanning.
2015-11-30 16:51:45,721 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 for rescanning.
2015-11-30 16:51:45,782 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442 for rescanning.
2015-11-30 16:51:46,124 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443 for rescanning.
2015-11-30 16:51:46,149 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 for rescanning.
2015-11-30 16:51:46,558 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 for rescanning.
2015-11-30 16:51:46,597 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446 for rescanning.
2015-11-30 16:51:46,917 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447 for rescanning.
2015-11-30 16:51:46,964 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448 for rescanning.
2015-11-30 16:51:47,342 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449 for rescanning.
2015-11-30 16:51:47,400 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450 for rescanning.
2015-11-30 16:51:47,785 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451 for rescanning.
2015-11-30 16:51:47,844 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452 for rescanning.
2015-11-30 16:51:48,282 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453 for rescanning.
2015-11-30 16:51:48,303 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454 for rescanning.
2015-11-30 16:51:48,633 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455 for rescanning.
2015-11-30 16:51:48,697 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456 for rescanning.
2015-11-30 16:51:49,050 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457 for rescanning.
2015-11-30 16:51:49,128 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458 for rescanning.
2015-11-30 16:51:49,593 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459 for rescanning.
2015-11-30 16:51:49,645 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460 for rescanning.
2015-11-30 16:51:50,077 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461 for rescanning.
2015-11-30 16:51:50,124 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462 for rescanning.
2015-11-30 16:51:50,508 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463 for rescanning.
2015-11-30 16:51:50,545 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464 for rescanning.
2015-11-30 16:51:50,861 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465 for rescanning.
2015-11-30 16:51:50,897 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466 for rescanning.
2015-11-30 16:51:51,237 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467 for rescanning.
2015-11-30 16:51:51,268 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468 for rescanning.
2015-11-30 16:51:51,689 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469 for rescanning.
2015-11-30 16:51:51,722 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470 for rescanning.
2015-11-30 16:51:52,042 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471 for rescanning.
2015-11-30 16:51:52,087 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472 for rescanning.
2015-11-30 16:51:52,429 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473 for rescanning.
2015-11-30 16:51:52,481 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474 for rescanning.
2015-11-30 16:51:52,847 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475 for rescanning.
2015-11-30 16:51:52,881 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476 for rescanning.
2015-11-30 16:51:53,304 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477 for rescanning.
2015-11-30 16:51:53,368 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478 for rescanning.
2015-11-30 16:51:53,739 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479 for rescanning.
2015-11-30 16:51:53,766 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480 for rescanning.
2015-11-30 16:51:54,130 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481 for rescanning.
2015-11-30 16:51:54,246 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482 for rescanning.
2015-11-30 16:51:54,650 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483 for rescanning.
2015-11-30 16:51:54,686 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484 for rescanning.
2015-11-30 16:51:55,142 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485 for rescanning.
2015-11-30 16:51:55,201 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486 for rescanning.
2015-11-30 16:51:55,543 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487 for rescanning.
2015-11-30 16:51:55,600 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488 for rescanning.
2015-11-30 16:51:56,019 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489 for rescanning.
2015-11-30 16:51:56,082 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490 for rescanning.
2015-11-30 16:51:56,464 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491 for rescanning.
2015-11-30 16:51:56,526 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492 for rescanning.
2015-11-30 16:51:56,967 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493 for rescanning.
2015-11-30 16:51:57,009 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494 for rescanning.
2015-11-30 16:51:57,345 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495 for rescanning.
2015-11-30 16:51:57,408 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496 for rescanning.
2015-11-30 16:51:57,794 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497 for rescanning.
2015-11-30 16:51:57,853 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498 for rescanning.
2015-11-30 16:51:58,243 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 for rescanning.
2015-11-30 16:51:58,301 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500 for rescanning.
2015-11-30 16:51:58,724 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501 for rescanning.
2015-11-30 16:51:58,775 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502 for rescanning.
2015-11-30 16:51:59,165 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503 for rescanning.
2015-11-30 16:51:59,261 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504 for rescanning.
2015-11-30 16:51:59,611 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505 for rescanning.
2015-11-30 16:51:59,662 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506 for rescanning.
2015-11-30 16:52:00,047 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507 for rescanning.
2015-11-30 16:52:00,115 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508 for rescanning.
2015-11-30 16:52:00,530 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509 for rescanning.
2015-11-30 16:52:00,588 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 for rescanning.
2015-11-30 16:52:00,988 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511 for rescanning.
2015-11-30 16:52:01,050 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512 for rescanning.
2015-11-30 16:52:01,418 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513 for rescanning.
2015-11-30 16:52:01,494 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514 for rescanning.
2015-11-30 16:52:01,861 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515 for rescanning.
2015-11-30 16:52:01,924 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 for rescanning.
2015-11-30 16:52:02,393 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517 for rescanning.
2015-11-30 16:52:02,428 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518 for rescanning.
2015-11-30 16:52:02,798 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519 for rescanning.
2015-11-30 16:52:02,848 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520 for rescanning.
2015-11-30 16:52:03,226 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521 for rescanning.
2015-11-30 16:52:03,284 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522 for rescanning.
2015-11-30 16:52:03,703 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 for rescanning.
2015-11-30 16:52:04,208 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 for rescanning.
2015-11-30 16:52:04,734 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 for rescanning.
2015-11-30 16:52:04,787 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526 for rescanning.
2015-11-30 16:52:05,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742599_1776 src: /192.168.6.248:49427 dest: /192.168.6.248:50010
2015-11-30 16:52:05,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:49427, dest: /192.168.6.248:50010, bytes: 87071, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1479103996_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742599_1776, duration: 61865830
2015-11-30 16:52:05,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742599_1776, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-30 16:52:06,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742600_1777 src: /192.168.6.248:49435 dest: /192.168.6.248:50010
2015-11-30 16:52:06,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:49435, dest: /192.168.6.248:50010, bytes: 87071, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1479103996_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742600_1777, duration: 44054877
2015-11-30 16:52:06,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742600_1777, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-30 16:52:08,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742599_1776 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir3/blk_1073742599 for deletion
2015-11-30 16:52:08,234 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742599_1776 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir3/blk_1073742599
2015-11-30 16:53:46,841 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 for rescanning, because we rescanned it recently.
2015-11-30 16:53:46,845 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 for rescanning, because we rescanned it recently.
2015-11-30 16:53:46,992 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 for rescanning, because we rescanned it recently.
2015-11-30 16:53:46,997 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 for rescanning, because we rescanned it recently.
2015-11-30 16:53:47,230 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 for rescanning, because we rescanned it recently.
2015-11-30 16:53:47,233 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 for rescanning, because we rescanned it recently.
2015-11-30 16:53:47,362 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 for rescanning, because we rescanned it recently.
2015-11-30 16:53:47,366 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 for rescanning, because we rescanned it recently.
2015-11-30 16:53:47,510 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 for rescanning, because we rescanned it recently.
2015-11-30 16:53:47,514 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 for rescanning, because we rescanned it recently.
2015-11-30 16:53:47,655 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 for rescanning, because we rescanned it recently.
2015-11-30 16:53:47,663 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 for rescanning, because we rescanned it recently.
2015-11-30 16:53:47,886 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 for rescanning, because we rescanned it recently.
2015-11-30 16:53:47,891 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,032 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,037 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,175 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,181 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,322 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,328 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,547 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,561 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,700 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,706 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,840 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,845 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,985 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 for rescanning, because we rescanned it recently.
2015-11-30 16:53:48,991 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 for rescanning, because we rescanned it recently.
2015-11-30 16:53:49,207 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 for rescanning, because we rescanned it recently.
2015-11-30 16:53:49,212 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 for rescanning, because we rescanned it recently.
2015-11-30 16:53:49,343 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 for rescanning, because we rescanned it recently.
2015-11-30 16:53:49,350 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 for rescanning, because we rescanned it recently.
2015-11-30 16:53:49,502 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 for rescanning, because we rescanned it recently.
2015-11-30 16:53:49,508 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 for rescanning, because we rescanned it recently.
2015-11-30 16:53:49,648 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 for rescanning, because we rescanned it recently.
2015-11-30 16:53:49,653 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 for rescanning, because we rescanned it recently.
2015-11-30 16:53:49,869 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 for rescanning, because we rescanned it recently.
2015-11-30 16:53:49,876 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,026 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,041 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,179 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,184 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,325 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,330 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,535 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,540 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,672 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,677 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,816 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,820 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,947 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447 for rescanning, because we rescanned it recently.
2015-11-30 16:53:50,953 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,171 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,176 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,304 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,310 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,444 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,450 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,590 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,595 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,810 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,815 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,959 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459 for rescanning, because we rescanned it recently.
2015-11-30 16:53:51,965 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,096 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,101 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,224 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,234 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,443 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,448 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,609 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,614 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,757 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,763 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,916 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471 for rescanning, because we rescanned it recently.
2015-11-30 16:53:52,920 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,126 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,130 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,259 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,266 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,425 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,429 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,564 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,570 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,786 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,792 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,926 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483 for rescanning, because we rescanned it recently.
2015-11-30 16:53:53,932 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,055 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,060 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,194 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,199 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,403 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,408 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,536 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,542 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,682 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,687 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,823 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495 for rescanning, because we rescanned it recently.
2015-11-30 16:53:54,829 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,038 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,043 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,187 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,192 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,336 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,340 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,469 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,474 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,678 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,683 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,832 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,838 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,990 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509 for rescanning, because we rescanned it recently.
2015-11-30 16:53:55,994 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,125 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,130 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,330 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,335 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,465 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,470 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,593 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,598 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,722 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,728 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,925 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521 for rescanning, because we rescanned it recently.
2015-11-30 16:53:56,929 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522 for rescanning, because we rescanned it recently.
2015-11-30 16:53:57,061 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 for rescanning, because we rescanned it recently.
2015-11-30 16:53:57,066 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 for rescanning, because we rescanned it recently.
2015-11-30 16:53:57,227 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 for rescanning, because we rescanned it recently.
2015-11-30 16:53:57,231 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526 for rescanning, because we rescanned it recently.
2015-11-30 16:53:57,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742601_1778 src: /192.168.6.248:49595 dest: /192.168.6.248:50010
2015-11-30 16:53:57,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:49595, dest: /192.168.6.248:50010, bytes: 98305, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1479103996_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742601_1778, duration: 77435262
2015-11-30 16:53:57,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742601_1778, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-30 16:53:57,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742602_1779 src: /192.168.6.248:49598 dest: /192.168.6.248:50010
2015-11-30 16:53:57,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:49598, dest: /192.168.6.248:50010, bytes: 98305, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1479103996_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742602_1779, duration: 26382025
2015-11-30 16:53:57,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742602_1779, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-30 16:54:02,228 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742601_1778 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir3/blk_1073742601 for deletion
2015-11-30 16:54:02,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742601_1778 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir3/blk_1073742601
2015-11-30 16:54:04,306 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 for rescanning, because we rescanned it recently.
2015-11-30 16:54:04,319 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 for rescanning, because we rescanned it recently.
2015-11-30 16:54:04,537 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 for rescanning, because we rescanned it recently.
2015-11-30 16:54:04,542 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 for rescanning, because we rescanned it recently.
2015-11-30 16:54:04,820 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 for rescanning, because we rescanned it recently.
2015-11-30 16:54:04,823 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 for rescanning, because we rescanned it recently.
2015-11-30 16:54:05,146 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 for rescanning, because we rescanned it recently.
2015-11-30 16:54:05,150 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 for rescanning, because we rescanned it recently.
2015-11-30 16:54:05,418 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 for rescanning, because we rescanned it recently.
2015-11-30 16:54:05,421 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 for rescanning, because we rescanned it recently.
2015-11-30 16:54:05,704 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 for rescanning, because we rescanned it recently.
2015-11-30 16:54:05,709 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 for rescanning, because we rescanned it recently.
2015-11-30 16:54:05,954 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 for rescanning, because we rescanned it recently.
2015-11-30 16:54:05,958 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 for rescanning, because we rescanned it recently.
2015-11-30 16:54:06,283 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 for rescanning, because we rescanned it recently.
2015-11-30 16:54:06,317 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 for rescanning, because we rescanned it recently.
2015-11-30 16:54:06,531 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 for rescanning, because we rescanned it recently.
2015-11-30 16:54:06,535 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 for rescanning, because we rescanned it recently.
2015-11-30 16:54:06,779 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 for rescanning, because we rescanned it recently.
2015-11-30 16:54:06,785 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 for rescanning, because we rescanned it recently.
2015-11-30 16:54:07,033 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 for rescanning, because we rescanned it recently.
2015-11-30 16:54:07,038 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 for rescanning, because we rescanned it recently.
2015-11-30 16:54:07,359 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 for rescanning, because we rescanned it recently.
2015-11-30 16:54:07,364 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 for rescanning, because we rescanned it recently.
2015-11-30 16:54:07,629 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 for rescanning, because we rescanned it recently.
2015-11-30 16:54:07,633 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 for rescanning, because we rescanned it recently.
2015-11-30 16:54:07,868 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 for rescanning, because we rescanned it recently.
2015-11-30 16:54:07,873 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 for rescanning, because we rescanned it recently.
2015-11-30 16:54:08,101 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 for rescanning, because we rescanned it recently.
2015-11-30 16:54:08,105 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 for rescanning, because we rescanned it recently.
2015-11-30 16:54:08,425 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 for rescanning, because we rescanned it recently.
2015-11-30 16:54:08,430 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 for rescanning, because we rescanned it recently.
2015-11-30 16:54:08,664 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 for rescanning, because we rescanned it recently.
2015-11-30 16:54:08,671 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 for rescanning, because we rescanned it recently.
2015-11-30 16:54:08,916 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 for rescanning, because we rescanned it recently.
2015-11-30 16:54:08,921 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 for rescanning, because we rescanned it recently.
2015-11-30 16:54:09,180 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 for rescanning, because we rescanned it recently.
2015-11-30 16:54:09,185 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 for rescanning, because we rescanned it recently.
2015-11-30 16:54:09,492 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 for rescanning, because we rescanned it recently.
2015-11-30 16:54:09,497 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 for rescanning, because we rescanned it recently.
2015-11-30 16:54:09,763 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 for rescanning, because we rescanned it recently.
2015-11-30 16:54:09,769 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 for rescanning, because we rescanned it recently.
2015-11-30 16:54:10,042 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 for rescanning, because we rescanned it recently.
2015-11-30 16:54:10,047 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 for rescanning, because we rescanned it recently.
2015-11-30 16:54:10,277 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 for rescanning, because we rescanned it recently.
2015-11-30 16:54:10,281 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442 for rescanning, because we rescanned it recently.
2015-11-30 16:54:10,613 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443 for rescanning, because we rescanned it recently.
2015-11-30 16:54:10,622 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 for rescanning, because we rescanned it recently.
2015-11-30 16:54:10,864 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 for rescanning, because we rescanned it recently.
2015-11-30 16:54:10,868 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446 for rescanning, because we rescanned it recently.
2015-11-30 16:54:11,149 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447 for rescanning, because we rescanned it recently.
2015-11-30 16:54:11,153 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448 for rescanning, because we rescanned it recently.
2015-11-30 16:54:11,385 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449 for rescanning, because we rescanned it recently.
2015-11-30 16:54:11,390 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450 for rescanning, because we rescanned it recently.
2015-11-30 16:54:11,710 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451 for rescanning, because we rescanned it recently.
2015-11-30 16:54:11,716 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452 for rescanning, because we rescanned it recently.
2015-11-30 16:54:11,976 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453 for rescanning, because we rescanned it recently.
2015-11-30 16:54:11,981 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454 for rescanning, because we rescanned it recently.
2015-11-30 16:54:12,260 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455 for rescanning, because we rescanned it recently.
2015-11-30 16:54:12,265 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456 for rescanning, because we rescanned it recently.
2015-11-30 16:54:12,509 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457 for rescanning, because we rescanned it recently.
2015-11-30 16:54:12,514 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458 for rescanning, because we rescanned it recently.
2015-11-30 16:54:12,823 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459 for rescanning, because we rescanned it recently.
2015-11-30 16:54:12,827 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460 for rescanning, because we rescanned it recently.
2015-11-30 16:54:13,102 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461 for rescanning, because we rescanned it recently.
2015-11-30 16:54:13,107 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462 for rescanning, because we rescanned it recently.
2015-11-30 16:54:13,368 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463 for rescanning, because we rescanned it recently.
2015-11-30 16:54:13,372 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464 for rescanning, because we rescanned it recently.
2015-11-30 16:54:13,641 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465 for rescanning, because we rescanned it recently.
2015-11-30 16:54:13,647 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466 for rescanning, because we rescanned it recently.
2015-11-30 16:54:13,965 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467 for rescanning, because we rescanned it recently.
2015-11-30 16:54:13,970 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468 for rescanning, because we rescanned it recently.
2015-11-30 16:54:14,216 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469 for rescanning, because we rescanned it recently.
2015-11-30 16:54:14,221 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470 for rescanning, because we rescanned it recently.
2015-11-30 16:54:14,480 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471 for rescanning, because we rescanned it recently.
2015-11-30 16:54:14,484 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472 for rescanning, because we rescanned it recently.
2015-11-30 16:54:14,737 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473 for rescanning, because we rescanned it recently.
2015-11-30 16:54:14,741 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474 for rescanning, because we rescanned it recently.
2015-11-30 16:54:15,056 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475 for rescanning, because we rescanned it recently.
2015-11-30 16:54:15,060 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476 for rescanning, because we rescanned it recently.
2015-11-30 16:54:15,307 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477 for rescanning, because we rescanned it recently.
2015-11-30 16:54:15,312 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478 for rescanning, because we rescanned it recently.
2015-11-30 16:54:15,552 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479 for rescanning, because we rescanned it recently.
2015-11-30 16:54:15,557 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480 for rescanning, because we rescanned it recently.
2015-11-30 16:54:15,838 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481 for rescanning, because we rescanned it recently.
2015-11-30 16:54:15,842 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482 for rescanning, because we rescanned it recently.
2015-11-30 16:54:16,169 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483 for rescanning, because we rescanned it recently.
2015-11-30 16:54:16,173 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484 for rescanning, because we rescanned it recently.
2015-11-30 16:54:16,425 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485 for rescanning, because we rescanned it recently.
2015-11-30 16:54:16,431 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486 for rescanning, because we rescanned it recently.
2015-11-30 16:54:16,684 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487 for rescanning, because we rescanned it recently.
2015-11-30 16:54:16,689 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488 for rescanning, because we rescanned it recently.
2015-11-30 16:54:16,924 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489 for rescanning, because we rescanned it recently.
2015-11-30 16:54:16,928 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490 for rescanning, because we rescanned it recently.
2015-11-30 16:54:17,245 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491 for rescanning, because we rescanned it recently.
2015-11-30 16:54:17,251 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492 for rescanning, because we rescanned it recently.
2015-11-30 16:54:17,495 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493 for rescanning, because we rescanned it recently.
2015-11-30 16:54:17,500 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494 for rescanning, because we rescanned it recently.
2015-11-30 16:54:17,733 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495 for rescanning, because we rescanned it recently.
2015-11-30 16:54:17,738 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496 for rescanning, because we rescanned it recently.
2015-11-30 16:54:18,011 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497 for rescanning, because we rescanned it recently.
2015-11-30 16:54:18,015 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498 for rescanning, because we rescanned it recently.
2015-11-30 16:54:18,344 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 for rescanning, because we rescanned it recently.
2015-11-30 16:54:18,348 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500 for rescanning, because we rescanned it recently.
2015-11-30 16:54:18,605 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501 for rescanning, because we rescanned it recently.
2015-11-30 16:54:18,609 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502 for rescanning, because we rescanned it recently.
2015-11-30 16:54:18,836 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503 for rescanning, because we rescanned it recently.
2015-11-30 16:54:18,840 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504 for rescanning, because we rescanned it recently.
2015-11-30 16:54:19,084 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505 for rescanning, because we rescanned it recently.
2015-11-30 16:54:19,088 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506 for rescanning, because we rescanned it recently.
2015-11-30 16:54:19,399 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507 for rescanning, because we rescanned it recently.
2015-11-30 16:54:19,405 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508 for rescanning, because we rescanned it recently.
2015-11-30 16:54:19,658 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509 for rescanning, because we rescanned it recently.
2015-11-30 16:54:19,664 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 for rescanning, because we rescanned it recently.
2015-11-30 16:54:19,905 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511 for rescanning, because we rescanned it recently.
2015-11-30 16:54:19,909 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512 for rescanning, because we rescanned it recently.
2015-11-30 16:54:20,156 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513 for rescanning, because we rescanned it recently.
2015-11-30 16:54:20,161 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514 for rescanning, because we rescanned it recently.
2015-11-30 16:54:20,503 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515 for rescanning, because we rescanned it recently.
2015-11-30 16:54:20,508 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 for rescanning, because we rescanned it recently.
2015-11-30 16:54:20,772 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517 for rescanning, because we rescanned it recently.
2015-11-30 16:54:20,776 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518 for rescanning, because we rescanned it recently.
2015-11-30 16:54:21,050 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519 for rescanning, because we rescanned it recently.
2015-11-30 16:54:21,055 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520 for rescanning, because we rescanned it recently.
2015-11-30 16:54:21,304 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521 for rescanning, because we rescanned it recently.
2015-11-30 16:54:21,308 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522 for rescanning, because we rescanned it recently.
2015-11-30 16:54:21,627 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 for rescanning, because we rescanned it recently.
2015-11-30 16:54:21,631 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 for rescanning, because we rescanned it recently.
2015-11-30 16:54:21,875 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 for rescanning, because we rescanned it recently.
2015-11-30 16:54:21,879 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): Not scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526 for rescanning, because we rescanned it recently.
2015-11-30 16:54:22,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742603_1780 src: /192.168.6.248:49737 dest: /192.168.6.248:50010
2015-11-30 16:54:22,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:49737, dest: /192.168.6.248:50010, bytes: 105074, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1479103996_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742603_1780, duration: 102749078
2015-11-30 16:54:22,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742603_1780, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-30 16:54:22,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742604_1781 src: /192.168.6.248:49740 dest: /192.168.6.248:50010
2015-11-30 16:54:22,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:49740, dest: /192.168.6.248:50010, bytes: 105074, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1479103996_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742604_1781, duration: 27401192
2015-11-30 16:54:22,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742604_1781, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-30 16:54:26,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742603_1780 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir3/blk_1073742603 for deletion
2015-11-30 16:54:26,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742603_1780 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir3/blk_1073742603
2015-11-30 16:59:50,229 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-30 16:59:54,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-30 16:59:54,763 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-30 16:59:54,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-12-01 16:11:44,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-12-01 16:11:44,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-01 16:11:44,786 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-01 16:11:44,849 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-01 16:11:44,849 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-01 16:11:44,854 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-12-01 16:11:44,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-12-01 16:11:44,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-01 16:11:44,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-01 16:11:44,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-01 16:11:44,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-01 16:11:45,019 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-01 16:11:45,027 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-01 16:11:45,032 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-01 16:11:45,037 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-01 16:11:45,040 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-01 16:11:45,040 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-01 16:11:45,040 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-01 16:11:45,050 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 52667
2015-12-01 16:11:45,050 INFO org.mortbay.log: jetty-6.1.26
2015-12-01 16:11:45,206 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:52667
2015-12-01 16:11:45,353 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-12-01 16:11:45,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-12-01 16:11:45,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-01 16:11:45,439 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-01 16:11:45,457 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-01 16:11:45,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-01 16:11:45,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-01 16:11:45,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-01 16:11:45,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-12-01 16:11:45,581 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-01 16:11:45,581 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-01 16:11:46,061 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 4252@rushikesh1
2015-12-01 16:11:46,174 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:11:46,175 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:11:46,175 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-01 16:11:46,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-12-01 16:11:46,299 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-12-01 16:11:46,299 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-01 16:11:46,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-01 16:11:46,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:11:46,338 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 16:11:46,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 64ms
2015-12-01 16:11:46,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 65ms
2015-12-01 16:11:46,402 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 16:11:46,474 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 73ms
2015-12-01 16:11:46,474 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 73ms
2015-12-01 16:11:46,752 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 584501626 ms.
2015-12-01 16:11:46,754 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448968370754 with interval 21600000
2015-12-01 16:11:46,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-12-01 16:11:46,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-12-01 16:11:46,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-01 16:11:46,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3708
2015-12-01 16:11:46,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-12-01 16:11:47,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2045f4828e5c,  containing 1 storage report(s), of which we sent 1. The reports had 486 total blocks and used 1 RPC(s). This took 8 msec to generate and 119 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-12-01 16:11:47,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:12:21,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742593_1770 to 192.168.6.237:50010 
2015-12-01 16:12:21,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742382_1559 to 192.168.6.237:50010 
2015-12-01 16:12:22,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742382_1559 (numBytes=2245077) to /192.168.6.237:50010
2015-12-01 16:12:22,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742593_1770 (numBytes=2402830) to /192.168.6.237:50010
2015-12-01 16:12:24,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742387_1564 to 192.168.6.237:50010 
2015-12-01 16:12:24,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742388_1565 to 192.168.6.237:50010 
2015-12-01 16:12:24,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742387_1564 (numBytes=2245077) to /192.168.6.237:50010
2015-12-01 16:12:25,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742388_1565 (numBytes=2244091) to /192.168.6.237:50010
2015-12-01 16:12:27,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742397_1574 to 192.168.6.237:50010 
2015-12-01 16:12:27,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742398_1575 to 192.168.6.237:50010 
2015-12-01 16:12:27,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742397_1574 (numBytes=2272974) to /192.168.6.237:50010
2015-12-01 16:12:27,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742398_1575 (numBytes=2390295) to /192.168.6.237:50010
2015-12-01 16:12:30,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742412_1589 to 192.168.6.237:50010 
2015-12-01 16:12:30,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742414_1591 to 192.168.6.237:50010 
2015-12-01 16:12:30,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742412_1589 (numBytes=2270886) to /192.168.6.237:50010
2015-12-01 16:12:30,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742414_1591 (numBytes=2269527) to /192.168.6.237:50010
2015-12-01 16:12:33,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742426_1603 to 192.168.6.237:50010 
2015-12-01 16:12:33,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742427_1604 to 192.168.6.237:50010 
2015-12-01 16:12:33,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742426_1603 (numBytes=2276131) to /192.168.6.237:50010
2015-12-01 16:12:33,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742427_1604 (numBytes=2267440) to /192.168.6.237:50010
2015-12-01 16:12:36,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742440_1617 to 192.168.6.237:50010 
2015-12-01 16:12:36,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742442_1619 to 192.168.6.237:50010 
2015-12-01 16:12:36,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742440_1617 (numBytes=2289573) to /192.168.6.237:50010
2015-12-01 16:12:36,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742442_1619 (numBytes=2288666) to /192.168.6.237:50010
2015-12-01 16:12:39,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742455_1632 to 192.168.6.237:50010 
2015-12-01 16:12:39,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742461_1638 to 192.168.6.237:50010 
2015-12-01 16:12:39,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742455_1632 (numBytes=2288342) to /192.168.6.237:50010
2015-12-01 16:12:39,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742461_1638 (numBytes=2282382) to /192.168.6.237:50010
2015-12-01 16:12:42,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742478_1655 to 192.168.6.237:50010 
2015-12-01 16:12:42,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742481_1658 to 192.168.6.237:50010 
2015-12-01 16:12:42,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742478_1655 (numBytes=2410600) to /192.168.6.237:50010
2015-12-01 16:12:42,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742481_1658 (numBytes=2297579) to /192.168.6.237:50010
2015-12-01 16:12:45,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742489_1666 to 192.168.6.237:50010 
2015-12-01 16:12:45,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742492_1669 to 192.168.6.237:50010 
2015-12-01 16:12:45,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742489_1666 (numBytes=2291575) to /192.168.6.237:50010
2015-12-01 16:12:45,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742492_1669 (numBytes=2292379) to /192.168.6.237:50010
2015-12-01 16:12:48,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742502_1679 to 192.168.6.237:50010 
2015-12-01 16:12:48,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742503_1680 to 192.168.6.237:50010 
2015-12-01 16:12:48,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742502_1679 (numBytes=2293076) to /192.168.6.237:50010
2015-12-01 16:12:48,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742503_1680 (numBytes=2409139) to /192.168.6.237:50010
2015-12-01 16:12:51,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742516_1693 to 192.168.6.237:50010 
2015-12-01 16:12:51,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742518_1695 to 192.168.6.237:50010 
2015-12-01 16:12:51,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742516_1693 (numBytes=2288806) to /192.168.6.237:50010
2015-12-01 16:12:51,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742518_1695 (numBytes=2409715) to /192.168.6.237:50010
2015-12-01 16:12:54,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742541_1718 to 192.168.6.237:50010 
2015-12-01 16:12:54,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742551_1728 to 192.168.6.237:50010 
2015-12-01 16:12:54,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742541_1718 (numBytes=2285050) to /192.168.6.237:50010
2015-12-01 16:12:54,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742551_1728 (numBytes=2295193) to /192.168.6.237:50010
2015-12-01 16:12:57,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742560_1737 to 192.168.6.237:50010 
2015-12-01 16:12:57,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742561_1738 to 192.168.6.237:50010 
2015-12-01 16:12:57,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742560_1737 (numBytes=2285120) to /192.168.6.237:50010
2015-12-01 16:12:57,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742561_1738 (numBytes=2289274) to /192.168.6.237:50010
2015-12-01 16:13:00,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742571_1748 to 192.168.6.237:50010 
2015-12-01 16:13:00,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742572_1749 to 192.168.6.237:50010 
2015-12-01 16:13:00,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742571_1748 (numBytes=2289353) to /192.168.6.237:50010
2015-12-01 16:13:00,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742572_1749 (numBytes=2284789) to /192.168.6.237:50010
2015-12-01 16:13:03,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742583_1760 to 192.168.6.237:50010 
2015-12-01 16:13:03,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742588_1765 to 192.168.6.237:50010 
2015-12-01 16:13:03,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742583_1760 (numBytes=2411844) to /192.168.6.237:50010
2015-12-01 16:13:03,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742588_1765 (numBytes=2408066) to /192.168.6.237:50010
2015-12-01 16:13:06,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742598_1775 to 192.168.6.237:50010 
2015-12-01 16:13:06,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742600_1777 to 192.168.6.237:50010 
2015-12-01 16:13:06,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742600_1777 (numBytes=87071) to /192.168.6.237:50010
2015-12-01 16:13:06,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742598_1775 (numBytes=2275376) to /192.168.6.237:50010
2015-12-01 16:13:57,550 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-12-01 16:14:01,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:14:01,903 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-01 16:14:01,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-12-01 16:14:34,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-12-01 16:14:34,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-01 16:14:35,077 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-01 16:14:35,139 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-01 16:14:35,139 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-01 16:14:35,144 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-12-01 16:14:35,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-12-01 16:14:35,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-01 16:14:35,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-01 16:14:35,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-01 16:14:35,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-01 16:14:35,263 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-01 16:14:35,271 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-01 16:14:35,276 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-01 16:14:35,281 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-01 16:14:35,283 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-01 16:14:35,283 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-01 16:14:35,283 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-01 16:14:35,293 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 52152
2015-12-01 16:14:35,293 INFO org.mortbay.log: jetty-6.1.26
2015-12-01 16:14:35,444 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:52152
2015-12-01 16:14:35,525 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-12-01 16:14:35,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-12-01 16:14:35,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-01 16:14:35,565 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-01 16:14:35,575 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-01 16:14:35,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-01 16:14:35,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-01 16:14:35,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-01 16:14:35,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-12-01 16:14:35,654 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-01 16:14:35,654 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-01 16:14:36,023 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 6165@rushikesh1
2015-12-01 16:14:36,111 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:14:36,111 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:14:36,112 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-01 16:14:36,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-12-01 16:14:36,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-12-01 16:14:36,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-01 16:14:36,223 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-01 16:14:36,223 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:14:36,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 16:14:36,231 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35659808768
2015-12-01 16:14:36,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-12-01 16:14:36,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-12-01 16:14:36,233 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 16:14:36,281 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 48ms
2015-12-01 16:14:36,281 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 49ms
2015-12-01 16:14:36,452 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 584331926 ms.
2015-12-01 16:14:36,454 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448981733454 with interval 21600000
2015-12-01 16:14:36,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-12-01 16:14:36,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-12-01 16:14:36,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-01 16:14:36,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3711
2015-12-01 16:14:36,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-12-01 16:14:36,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x206d7356642f,  containing 1 storage report(s), of which we sent 1. The reports had 486 total blocks and used 1 RPC(s). This took 5 msec to generate and 63 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-12-01 16:14:36,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:15:08,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742435_1612 to 192.168.6.249:50010 
2015-12-01 16:15:08,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742439_1616 to 192.168.6.249:50010 
2015-12-01 16:15:09,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742439_1616 (numBytes=2295666) to /192.168.6.249:50010
2015-12-01 16:15:09,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742435_1612 (numBytes=2284941) to /192.168.6.249:50010
2015-12-01 16:15:14,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742421_1598 to 192.168.6.249:50010 
2015-12-01 16:15:14,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742423_1600 to 192.168.6.249:50010 
2015-12-01 16:15:15,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742421_1598 (numBytes=2273574) to /192.168.6.249:50010
2015-12-01 16:15:15,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742423_1600 (numBytes=2386911) to /192.168.6.249:50010
2015-12-01 16:15:20,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742574_1751 to 192.168.6.249:50010 
2015-12-01 16:15:20,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742582_1759 to 192.168.6.249:50010 
2015-12-01 16:15:21,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742582_1759 (numBytes=2297078) to /192.168.6.249:50010
2015-12-01 16:15:21,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742574_1751 (numBytes=2290537) to /192.168.6.249:50010
2015-12-01 16:15:26,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742550_1727 to 192.168.6.249:50010 
2015-12-01 16:15:26,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742553_1730 to 192.168.6.249:50010 
2015-12-01 16:15:27,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742553_1730 (numBytes=2411079) to /192.168.6.249:50010
2015-12-01 16:15:27,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742550_1727 (numBytes=2287164) to /192.168.6.249:50010
2015-12-01 16:15:29,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742517_1694 to 192.168.6.249:50010 
2015-12-01 16:15:29,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742524_1701 to 192.168.6.249:50010 
2015-12-01 16:15:30,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742517_1694 (numBytes=2294713) to /192.168.6.249:50010
2015-12-01 16:15:30,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742524_1701 (numBytes=2287709) to /192.168.6.249:50010
2015-12-01 16:15:35,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742484_1661 to 192.168.6.249:50010 
2015-12-01 16:15:35,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742596_1773 to 192.168.6.249:50010 
2015-12-01 16:15:36,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742484_1661 (numBytes=2290716) to /192.168.6.249:50010
2015-12-01 16:15:36,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742596_1773 (numBytes=2288912) to /192.168.6.249:50010
2015-12-01 16:15:38,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742433_1610 to 192.168.6.237:50010 
2015-12-01 16:15:38,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742436_1613 to 192.168.6.237:50010 
2015-12-01 16:15:39,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742433_1610 (numBytes=2389961) to /192.168.6.237:50010
2015-12-01 16:15:39,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742436_1613 (numBytes=2290782) to /192.168.6.237:50010
2015-12-01 16:15:41,648 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742408_1585 to 192.168.6.237:50010 
2015-12-01 16:15:41,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742400_1577 to 192.168.6.237:50010 
2015-12-01 16:15:42,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742408_1585 (numBytes=2385603) to /192.168.6.237:50010
2015-12-01 16:15:42,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742400_1577 (numBytes=2273589) to /192.168.6.237:50010
2015-12-01 16:15:44,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742418_1595 to 192.168.6.237:50010 
2015-12-01 16:15:44,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742420_1597 to 192.168.6.237:50010 
2015-12-01 16:15:45,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742418_1595 (numBytes=2389217) to /192.168.6.237:50010
2015-12-01 16:15:45,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742420_1597 (numBytes=2271002) to /192.168.6.237:50010
2015-12-01 16:15:47,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742569_1746 to 192.168.6.237:50010 
2015-12-01 16:15:47,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742573_1750 to 192.168.6.237:50010 
2015-12-01 16:15:48,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742569_1746 (numBytes=2289378) to /192.168.6.237:50010
2015-12-01 16:15:48,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742573_1750 (numBytes=2405465) to /192.168.6.237:50010
2015-12-01 16:15:50,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742532_1709 to 192.168.6.237:50010 
2015-12-01 16:15:50,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742538_1715 to 192.168.6.237:50010 
2015-12-01 16:15:50,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742532_1709 (numBytes=2290510) to /192.168.6.237:50010
2015-12-01 16:15:51,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742538_1715 (numBytes=2412010) to /192.168.6.237:50010
2015-12-01 16:15:53,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742557_1734 to 192.168.6.237:50010 
2015-12-01 16:15:53,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742558_1735 to 192.168.6.237:50010 
2015-12-01 16:15:54,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742557_1734 (numBytes=2296212) to /192.168.6.237:50010
2015-12-01 16:15:54,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742558_1735 (numBytes=2410532) to /192.168.6.237:50010
2015-12-01 16:15:56,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742512_1689 to 192.168.6.237:50010 
2015-12-01 16:15:56,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742513_1690 to 192.168.6.237:50010 
2015-12-01 16:15:57,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742512_1689 (numBytes=2290985) to /192.168.6.237:50010
2015-12-01 16:15:57,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742513_1690 (numBytes=2404324) to /192.168.6.237:50010
2015-12-01 16:15:59,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742472_1649 to 192.168.6.237:50010 
2015-12-01 16:15:59,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742486_1663 to 192.168.6.237:50010 
2015-12-01 16:16:00,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742472_1649 (numBytes=2290170) to /192.168.6.237:50010
2015-12-01 16:16:00,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742486_1663 (numBytes=2290339) to /192.168.6.237:50010
2015-12-01 16:16:02,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742604_1781 to 192.168.6.237:50010 
2015-12-01 16:16:02,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742602_1779 to 192.168.6.237:50010 
2015-12-01 16:16:02,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742604_1781 (numBytes=105074) to /192.168.6.237:50010
2015-12-01 16:16:02,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742602_1779 (numBytes=98305) to /192.168.6.237:50010
2015-12-01 16:16:08,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742458_1635 to 192.168.6.249:50010 
2015-12-01 16:16:08,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742460_1637 to 192.168.6.249:50010 
2015-12-01 16:16:09,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742458_1635 (numBytes=2412679) to /192.168.6.249:50010
2015-12-01 16:16:09,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742460_1637 (numBytes=2289798) to /192.168.6.249:50010
2015-12-01 16:16:14,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742390_1567 to 192.168.6.249:50010 
2015-12-01 16:16:14,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742563_1740 to 192.168.6.249:50010 
2015-12-01 16:16:15,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742563_1740 (numBytes=2406688) to /192.168.6.249:50010
2015-12-01 16:16:15,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742390_1567 (numBytes=2266617) to /192.168.6.249:50010
2015-12-01 16:16:20,649 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-12-01 16:16:23,706 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-01 16:16:23,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-12-01 16:16:57,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-12-01 16:16:57,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-01 16:16:57,693 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-01 16:16:57,755 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-01 16:16:57,755 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-01 16:16:57,760 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-12-01 16:16:57,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-12-01 16:16:57,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-01 16:16:57,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-01 16:16:57,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-01 16:16:57,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-01 16:16:57,880 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-01 16:16:57,888 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-01 16:16:57,893 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-01 16:16:57,898 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-01 16:16:57,900 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-01 16:16:57,900 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-01 16:16:57,900 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-01 16:16:57,910 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38961
2015-12-01 16:16:57,910 INFO org.mortbay.log: jetty-6.1.26
2015-12-01 16:16:58,061 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38961
2015-12-01 16:16:58,142 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-12-01 16:16:58,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-12-01 16:16:58,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-01 16:16:58,181 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-01 16:16:58,192 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-01 16:16:58,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-01 16:16:58,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-01 16:16:58,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-01 16:16:58,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-12-01 16:16:58,271 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-01 16:16:58,271 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-01 16:16:58,660 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 8072@rushikesh1
2015-12-01 16:16:58,748 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:16:58,748 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:16:58,748 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-01 16:16:58,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-12-01 16:16:58,823 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-12-01 16:16:58,823 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-01 16:16:58,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-01 16:16:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:16:58,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 16:16:58,861 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35659808768
2015-12-01 16:16:58,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 10ms
2015-12-01 16:16:58,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 11ms
2015-12-01 16:16:58,863 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 16:16:58,917 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 54ms
2015-12-01 16:16:58,917 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 54ms
2015-12-01 16:16:59,119 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 584189259 ms.
2015-12-01 16:16:59,121 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448987048121 with interval 21600000
2015-12-01 16:16:59,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-12-01 16:16:59,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-12-01 16:16:59,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-01 16:16:59,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3714
2015-12-01 16:16:59,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-12-01 16:16:59,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x208ea83d06b0,  containing 1 storage report(s), of which we sent 1. The reports had 486 total blocks and used 1 RPC(s). This took 5 msec to generate and 61 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-12-01 16:16:59,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:17:31,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742448_1625 to 192.168.6.237:50010 
2015-12-01 16:17:31,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742452_1629 to 192.168.6.237:50010 
2015-12-01 16:17:32,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742452_1629 (numBytes=2291622) to /192.168.6.237:50010
2015-12-01 16:17:32,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742448_1625 (numBytes=2405524) to /192.168.6.237:50010
2015-12-01 16:17:34,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742579_1756 to 192.168.6.237:50010 
2015-12-01 16:17:34,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742576_1753 to 192.168.6.237:50010 
2015-12-01 16:17:34,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742579_1756 (numBytes=2284507) to /192.168.6.237:50010
2015-12-01 16:17:34,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742576_1753 (numBytes=2286718) to /192.168.6.237:50010
2015-12-01 16:17:40,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742429_1606 to 192.168.6.249:50010 
2015-12-01 16:17:40,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742430_1607 to 192.168.6.249:50010 
2015-12-01 16:17:40,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742429_1606 (numBytes=2270807) to /192.168.6.249:50010
2015-12-01 16:17:40,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742430_1607 (numBytes=2276051) to /192.168.6.249:50010
2015-12-01 16:17:43,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742399_1576 to 192.168.6.249:50010 
2015-12-01 16:17:43,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742528_1705 to 192.168.6.249:50010 
2015-12-01 16:17:43,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742399_1576 (numBytes=2271852) to /192.168.6.249:50010
2015-12-01 16:17:44,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742528_1705 (numBytes=2411555) to /192.168.6.249:50010
2015-12-01 16:17:46,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742545_1722 to 192.168.6.249:50010 
2015-12-01 16:17:46,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742547_1724 to 192.168.6.249:50010 
2015-12-01 16:17:46,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742545_1722 (numBytes=2293676) to /192.168.6.249:50010
2015-12-01 16:17:47,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742547_1724 (numBytes=2284640) to /192.168.6.249:50010
2015-12-01 16:17:49,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742525_1702 to 192.168.6.249:50010 
2015-12-01 16:17:49,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742465_1642 to 192.168.6.249:50010 
2015-12-01 16:17:49,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742525_1702 (numBytes=2291512) to /192.168.6.249:50010
2015-12-01 16:17:49,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742465_1642 (numBytes=2294522) to /192.168.6.249:50010
2015-12-01 16:17:55,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742391_1568 to 192.168.6.237:50010 
2015-12-01 16:17:55,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742567_1744 to 192.168.6.237:50010 
2015-12-01 16:17:55,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742391_1568 (numBytes=2269853) to /192.168.6.237:50010
2015-12-01 16:17:55,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742567_1744 (numBytes=2290727) to /192.168.6.237:50010
2015-12-01 16:18:01,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742535_1712 to 192.168.6.249:50010 
2015-12-01 16:18:01,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742543_1720 to 192.168.6.249:50010 
2015-12-01 16:18:01,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742543_1720 (numBytes=2405136) to /192.168.6.249:50010
2015-12-01 16:18:02,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742535_1712 (numBytes=2294986) to /192.168.6.249:50010
2015-12-01 16:18:07,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742473_1650 to 192.168.6.237:50010 
2015-12-01 16:18:07,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.248:50010, datanodeUuid=d629bce3-4072-426c-a3ff-71fefbd485b4, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742493_1670 to 192.168.6.237:50010 
2015-12-01 16:18:07,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742473_1650 (numBytes=2405103) to /192.168.6.237:50010
2015-12-01 16:18:07,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742493_1670 (numBytes=2413436) to /192.168.6.237:50010
2015-12-01 16:18:52,265 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-12-01 16:18:55,331 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-01 16:18:55,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-12-01 16:21:00,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-12-01 16:21:00,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-01 16:21:01,354 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-01 16:21:01,418 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-01 16:21:01,418 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-01 16:21:01,423 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-12-01 16:21:01,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-12-01 16:21:01,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-01 16:21:01,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-01 16:21:01,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-01 16:21:01,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-01 16:21:01,572 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-01 16:21:01,587 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-01 16:21:01,592 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-01 16:21:01,597 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-01 16:21:01,599 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-01 16:21:01,600 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-01 16:21:01,600 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-01 16:21:01,609 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34485
2015-12-01 16:21:01,610 INFO org.mortbay.log: jetty-6.1.26
2015-12-01 16:21:01,766 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34485
2015-12-01 16:21:01,848 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-12-01 16:21:01,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-12-01 16:21:01,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-01 16:21:01,888 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-01 16:21:01,899 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-01 16:21:01,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-01 16:21:01,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-01 16:21:01,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-01 16:21:01,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-12-01 16:21:01,979 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-01 16:21:01,980 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-01 16:21:02,311 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 10011@rushikesh1
2015-12-01 16:21:02,407 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:21:02,407 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:21:02,408 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-01 16:21:02,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-12-01 16:21:02,475 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-12-01 16:21:02,475 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-01 16:21:02,507 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-01 16:21:02,507 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:21:02,508 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 16:21:02,514 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35659808768
2015-12-01 16:21:02,515 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 7ms
2015-12-01 16:21:02,515 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-12-01 16:21:02,519 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 16:21:02,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 48ms
2015-12-01 16:21:02,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 52ms
2015-12-01 16:21:02,737 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 583945642 ms.
2015-12-01 16:21:02,739 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448977435739 with interval 21600000
2015-12-01 16:21:02,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-12-01 16:21:02,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-12-01 16:21:02,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-01 16:21:02,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3717
2015-12-01 16:21:02,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-12-01 16:21:02,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x20c7639e085a,  containing 1 storage report(s), of which we sent 1. The reports had 486 total blocks and used 1 RPC(s). This took 4 msec to generate and 64 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-12-01 16:21:02,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:30:55,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742378_1555 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742378 for deletion
2015-12-01 16:30:56,007 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742378_1555 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742378
2015-12-01 16:36:20,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742605_1782 src: /192.168.6.248:33578 dest: /192.168.6.248:50010
2015-12-01 16:36:20,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33578, dest: /192.168.6.248:50010, bytes: 3363967, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2052990752_1, offset: 0, srvID: d629bce3-4072-426c-a3ff-71fefbd485b4, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742605_1782, duration: 362445728
2015-12-01 16:36:20,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742605_1782, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-12-01 16:43:55,973 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-12-01 16:43:59,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:44:00,797 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-01 16:44:00,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
2015-12-01 16:44:54,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh1/192.168.6.248
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-12-01 16:44:54,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-01 16:44:55,388 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-01 16:44:55,451 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-01 16:44:55,451 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-01 16:44:55,456 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-12-01 16:44:55,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh1
2015-12-01 16:44:55,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-01 16:44:55,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-01 16:44:55,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-01 16:44:55,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-01 16:44:55,595 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-01 16:44:55,603 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-01 16:44:55,608 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-01 16:44:55,613 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-01 16:44:55,615 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-01 16:44:55,615 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-01 16:44:55,616 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-01 16:44:55,625 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 58644
2015-12-01 16:44:55,625 INFO org.mortbay.log: jetty-6.1.26
2015-12-01 16:44:55,779 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:58644
2015-12-01 16:44:55,861 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-12-01 16:44:55,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-12-01 16:44:55,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-01 16:44:55,901 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-01 16:44:55,912 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-01 16:44:55,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-01 16:44:55,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-01 16:44:55,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-01 16:44:55,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-12-01 16:44:55,991 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-01 16:44:55,992 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-01 16:44:56,320 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 25582@rushikesh1
2015-12-01 16:44:56,408 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:44:56,408 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:44:56,409 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-01 16:44:56,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=d629bce3-4072-426c-a3ff-71fefbd485b4
2015-12-01 16:44:56,483 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ee91df04-2c9e-46e7-9206-23b25b9587e8
2015-12-01 16:44:56,483 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-01 16:44:56,513 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-01 16:44:56,513 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:44:56,514 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 16:44:56,520 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35659808768
2015-12-01 16:44:56,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-12-01 16:44:56,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-12-01 16:44:56,522 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 16:44:56,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 47ms
2015-12-01 16:44:56,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 48ms
2015-12-01 16:44:56,730 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-ee91df04-2c9e-46e7-9206-23b25b9587e8): no suitable block pools found to scan.  Waiting 582511649 ms.
2015-12-01 16:44:56,731 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448986172731 with interval 21600000
2015-12-01 16:44:56,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-12-01 16:44:56,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-12-01 16:44:56,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-01 16:44:56,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3727
2015-12-01 16:44:56,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid d629bce3-4072-426c-a3ff-71fefbd485b4) service to rushikesh1/192.168.6.248:54310
2015-12-01 16:44:56,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x22154555f9f7,  containing 1 storage report(s), of which we sent 1. The reports had 486 total blocks and used 1 RPC(s). This took 8 msec to generate and 63 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-12-01 16:44:56,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 16:45:52,985 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh1/192.168.6.248"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-12-01 16:45:56,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:45:57,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:45:58,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:45:59,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:00,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:01,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:02,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:03,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:04,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:05,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:05,992 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh1/192.168.6.248 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-12-01 16:46:06,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:07,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:08,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:09,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:10,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:11,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:12,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:13,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 16:46:14,485 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-01 16:46:14,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh1/192.168.6.248
************************************************************/
