2015-09-30 05:44:29,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-09-30 05:44:29,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-09-30 05:44:30,171 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /app/hadoop/tmp/dfs/data : 
java.io.FileNotFoundException: File file:/app/hadoop/tmp/dfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:606)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:819)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:596)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:139)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:156)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2344)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2386)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2368)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2260)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2307)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2484)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2508)
2015-09-30 05:44:30,173 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/app/hadoop/tmp/dfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2395)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2368)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2260)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2307)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2484)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2508)
2015-09-30 05:44:30,174 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-09-30 05:44:30,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-09-30 06:15:40,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-09-30 06:15:40,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-09-30 06:15:41,070 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /app/hadoop/tmp/dfs/data : 
java.io.FileNotFoundException: File file:/app/hadoop/tmp/dfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:606)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:819)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:596)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:139)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:156)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2344)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2386)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2368)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2260)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2307)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2484)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2508)
2015-09-30 06:15:41,072 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/app/hadoop/tmp/dfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2395)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2368)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2260)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2307)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2484)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2508)
2015-09-30 06:15:41,073 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-09-30 06:15:41,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-09-30 06:32:13,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-09-30 06:32:13,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-09-30 06:32:13,777 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-09-30 06:32:13,843 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-09-30 06:32:13,843 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-09-30 06:32:13,849 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-09-30 06:32:13,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-09-30 06:32:13,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-09-30 06:32:13,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-09-30 06:32:13,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-09-30 06:32:13,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-09-30 06:32:13,961 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-09-30 06:32:13,969 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-09-30 06:32:13,974 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-09-30 06:32:13,979 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-09-30 06:32:13,981 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-09-30 06:32:13,981 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-09-30 06:32:13,981 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-09-30 06:32:13,991 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 54459
2015-09-30 06:32:13,991 INFO org.mortbay.log: jetty-6.1.26
2015-09-30 06:32:14,144 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54459
2015-09-30 06:32:14,226 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-09-30 06:32:14,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-09-30 06:32:14,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-09-30 06:32:14,266 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-09-30 06:32:14,278 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-09-30 06:32:14,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-09-30 06:32:14,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-09-30 06:32:14,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-09-30 06:32:14,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-09-30 06:32:14,358 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-09-30 06:32:14,366 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-09-30 06:32:15,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:16,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:17,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:18,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:19,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:20,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:21,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:22,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:23,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:24,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:24,438 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:32:30,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:31,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:32,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:33,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:34,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:35,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:36,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:37,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:38,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:39,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:39,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:32:45,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:46,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:47,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:48,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:49,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:50,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:51,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:52,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:53,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:54,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:32:54,456 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:33:00,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:01,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:02,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:03,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:04,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:05,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:06,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:07,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:08,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:09,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:09,466 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:33:15,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:16,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:17,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:18,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:19,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:20,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:21,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:22,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:23,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:24,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:24,474 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:33:30,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:31,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:32,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:33,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:34,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:35,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:36,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:37,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:38,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:39,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:39,483 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:33:45,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:46,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:47,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:48,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:49,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:50,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:51,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:52,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:53,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:54,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:33:54,492 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:34:00,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:01,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:02,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:03,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:04,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:05,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:06,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:07,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:08,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:09,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:09,501 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:34:15,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:16,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:17,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:18,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:19,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:20,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:21,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:22,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:23,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:24,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:24,510 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:34:30,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:31,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:32,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:33,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:34,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:35,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:36,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:37,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:38,517 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:39,517 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:39,518 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:34:45,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:46,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:47,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:48,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:49,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:50,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:51,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:52,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:53,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:54,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:34:54,528 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:35:00,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:01,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:02,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:03,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:04,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:05,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:06,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:07,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:08,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:09,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:09,537 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:35:15,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:16,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:17,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:18,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:19,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:20,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:21,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:22,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:23,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:24,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:24,546 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:35:30,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:31,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:32,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:33,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:34,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:35,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:36,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:37,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:38,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:39,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:39,554 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:35:45,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:46,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:47,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:48,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:49,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:50,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:51,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:52,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:53,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:54,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:35:54,563 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:36:00,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:01,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:02,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:03,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:04,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:05,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:06,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:07,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:08,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:09,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:09,573 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:36:15,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:16,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:17,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:18,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:19,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:20,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:21,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:22,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:23,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:24,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:24,582 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:36:30,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:31,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:32,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:33,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:34,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:35,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:36,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:37,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:38,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:39,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:39,591 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:36:45,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:46,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:47,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:48,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:49,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:50,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:51,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:52,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:53,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:54,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:36:54,600 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:37:00,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:01,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:02,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:03,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:04,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:05,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:06,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:07,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:08,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:09,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:09,609 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:37:15,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:16,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:17,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:18,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:19,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:20,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:21,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:22,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:23,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:24,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:24,619 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:37:30,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:31,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:32,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:33,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:34,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:35,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:36,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:37,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:38,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:39,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:39,627 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:37:45,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:46,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:47,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:48,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:49,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:50,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:51,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:52,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:53,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:54,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:37:54,637 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:37:57,050 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-09-30 06:37:57,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-09-30 06:38:44,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-09-30 06:38:44,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-09-30 06:38:44,901 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-09-30 06:38:44,966 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-09-30 06:38:44,966 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-09-30 06:38:44,971 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-09-30 06:38:44,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-09-30 06:38:44,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-09-30 06:38:45,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-09-30 06:38:45,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-09-30 06:38:45,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-09-30 06:38:45,091 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-09-30 06:38:45,099 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-09-30 06:38:45,104 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-09-30 06:38:45,109 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-09-30 06:38:45,111 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-09-30 06:38:45,112 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-09-30 06:38:45,112 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-09-30 06:38:45,122 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 51026
2015-09-30 06:38:45,122 INFO org.mortbay.log: jetty-6.1.26
2015-09-30 06:38:45,309 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:51026
2015-09-30 06:38:45,392 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-09-30 06:38:45,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-09-30 06:38:45,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-09-30 06:38:45,433 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-09-30 06:38:45,444 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-09-30 06:38:45,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-09-30 06:38:45,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-09-30 06:38:45,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-09-30 06:38:45,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-09-30 06:38:45,524 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-09-30 06:38:45,525 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-09-30 06:38:46,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:38:47,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:38:48,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:38:49,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:38:50,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:38:51,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:38:52,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:38:53,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:38:54,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:38:55,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:38:55,606 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:39:01,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:02,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:03,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:04,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:05,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:06,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:07,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:08,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:09,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:10,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:10,616 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:39:16,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:17,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:18,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:19,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:20,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:21,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:22,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:23,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:24,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:25,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:25,624 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:39:31,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:32,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:33,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:34,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:35,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:36,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:37,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:38,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:39,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:40,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:40,633 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:39:46,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:47,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:48,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:49,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:50,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:51,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:52,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:53,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:54,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:55,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:39:55,642 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:40:01,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:02,644 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:03,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:04,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:05,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:06,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:07,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:08,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:09,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:10,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:10,651 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:40:16,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:17,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:18,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:19,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:20,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:21,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:22,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:23,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:24,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:25,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:25,660 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:40:31,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:32,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:33,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:34,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:35,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:36,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:37,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:38,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:39,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:40,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:40,669 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:40:46,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:47,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:48,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:49,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:50,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:51,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:52,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:53,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:54,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:55,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:40:55,678 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:41:01,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:02,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:03,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:04,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:05,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:06,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:07,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:08,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:09,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:10,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:10,686 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:41:16,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:17,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:18,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:19,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:20,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:21,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:22,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:23,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:24,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:25,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:25,695 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:41:31,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:32,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:33,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:34,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:35,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:36,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:37,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:38,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:39,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:40,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:40,704 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:41:46,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:47,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:48,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:49,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:50,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:51,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:52,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:53,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:54,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:55,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:41:55,713 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:42:01,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:02,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:03,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:04,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:05,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:06,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:07,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:08,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:09,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:10,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:10,722 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:42:16,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:17,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:18,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:19,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:20,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:21,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:22,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:23,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:24,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:25,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:25,731 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:42:31,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:32,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:33,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:34,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:35,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:36,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:37,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:38,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:39,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:40,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-09-30 06:42:40,740 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-09-30 06:42:41,573 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-09-30 06:42:41,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-04 23:49:36,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-04 23:49:36,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-04 23:49:38,171 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-04 23:49:38,333 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-04 23:49:38,333 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-04 23:49:38,340 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-04 23:49:38,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-04 23:49:38,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-04 23:49:38,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-04 23:49:38,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-04 23:49:38,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-04 23:49:38,699 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-04 23:49:38,709 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-04 23:49:38,717 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-04 23:49:38,723 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-04 23:49:38,726 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-04 23:49:38,726 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-04 23:49:38,726 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-04 23:49:38,740 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36945
2015-10-04 23:49:38,740 INFO org.mortbay.log: jetty-6.1.26
2015-10-04 23:49:39,744 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36945
2015-10-04 23:49:40,242 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-04 23:49:40,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-04 23:49:40,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-04 23:49:40,441 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-04 23:49:40,496 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-04 23:49:40,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-04 23:49:40,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-04 23:49:40,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-04 23:49:40,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-04 23:49:40,815 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-04 23:49:40,816 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-04 23:49:42,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:43,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:44,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:45,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:46,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:47,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:48,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:49,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:50,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:51,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:51,060 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:49:57,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:58,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:49:59,063 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:00,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:01,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:02,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:03,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:04,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:05,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:06,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:06,069 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:50:12,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:13,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:14,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:15,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:16,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:17,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:18,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:19,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:20,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:21,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:21,078 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:50:27,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:28,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:29,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:30,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:31,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:32,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:33,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:34,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:35,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:36,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:36,086 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:50:42,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:43,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:44,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:45,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:46,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:47,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:48,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:49,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:50,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:51,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:51,094 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:50:57,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:58,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:50:59,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:00,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:01,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:02,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:03,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:04,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:05,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:06,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:06,103 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:51:12,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:13,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:14,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:15,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:16,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:17,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:18,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:19,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:20,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:21,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:21,112 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:51:27,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:28,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:29,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:30,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:31,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:32,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:33,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:34,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:35,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:36,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:36,120 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:51:42,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:43,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:44,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:45,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:46,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:47,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:48,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:49,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:50,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:51,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:51,128 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:51:57,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:58,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:51:59,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:00,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:01,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:02,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:03,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:04,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:05,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:06,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:06,140 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:52:12,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:13,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:14,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:15,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:16,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:17,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:18,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:19,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:20,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:21,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:21,148 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:52:27,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:28,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:29,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:30,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:31,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:32,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:33,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:34,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:35,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:36,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:36,157 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:52:42,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:43,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:44,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:45,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:46,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:47,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:48,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:49,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:50,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:51,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:51,165 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:52:57,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:58,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:52:59,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:00,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:01,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:02,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:03,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:04,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:05,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:06,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:06,174 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:53:12,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:13,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:14,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:15,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:16,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:17,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:18,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:19,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:20,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:21,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:21,182 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:53:27,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:28,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:29,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:30,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:31,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:32,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:33,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:34,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:35,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:36,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:36,192 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:53:42,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:43,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:44,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:45,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:46,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:47,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:48,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:49,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:50,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:51,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:51,201 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:53:57,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:58,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:53:59,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:54:00,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:54:01,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:54:02,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:54:03,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:54:04,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:54:05,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:54:06,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-04 23:54:06,209 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-04 23:54:08,367 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-04 23:54:08,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 00:07:01,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 00:07:01,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 00:07:02,372 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 00:07:02,436 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 00:07:02,436 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 00:07:02,441 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 00:07:02,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 00:07:02,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 00:07:02,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 00:07:02,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 00:07:02,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 00:07:02,560 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 00:07:02,567 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 00:07:02,572 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 00:07:02,577 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 00:07:02,579 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 00:07:02,580 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 00:07:02,580 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 00:07:02,590 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 54832
2015-10-05 00:07:02,590 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 00:07:02,745 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54832
2015-10-05 00:07:02,827 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 00:07:02,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 00:07:02,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 00:07:02,866 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 00:07:02,877 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 00:07:02,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 00:07:02,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 00:07:02,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 00:07:02,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 00:07:02,956 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 00:07:02,956 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 00:07:04,041 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:05,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:06,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:07,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:08,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:09,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:10,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:11,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:12,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:13,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:13,049 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:07:19,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:20,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:21,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:22,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:23,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:24,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:25,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:26,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:27,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:28,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:28,057 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:07:34,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:35,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:36,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:37,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:38,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:39,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:40,063 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:41,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:42,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:43,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:43,066 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:07:49,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:50,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:51,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:52,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:53,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:54,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:55,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:56,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:57,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:58,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:07:58,075 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:08:04,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:05,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:06,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:07,078 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:08,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:09,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:10,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:11,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:12,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:13,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:13,083 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:08:19,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:20,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:21,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:22,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:23,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:24,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:25,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:26,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:27,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:28,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:28,091 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:08:34,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:35,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:36,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:37,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:38,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:39,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:40,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:41,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:42,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:43,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:43,100 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:08:49,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:50,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:51,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:52,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:53,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:54,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:55,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:56,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:57,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:58,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:08:58,108 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:09:04,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:05,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:06,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:07,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:08,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:09,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:10,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:11,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:12,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:13,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:13,117 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:09:19,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:20,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:21,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:22,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:23,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:24,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:25,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:26,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:27,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:28,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:28,125 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:09:34,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:35,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:36,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:37,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:38,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:39,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:40,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:41,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:42,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:43,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:43,134 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:09:49,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:50,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:51,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:52,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:53,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:54,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:55,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:56,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:57,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:58,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:09:58,143 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:10:04,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:05,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:06,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:07,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:08,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:09,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:10,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:11,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:12,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:13,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:13,151 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:10:19,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:20,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:21,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:22,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:23,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:24,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:25,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:26,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:27,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:28,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:28,159 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:10:34,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:35,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:36,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:37,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:38,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:39,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:40,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:41,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:42,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:43,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:43,168 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:10:49,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:50,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:51,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:52,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:53,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:54,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:55,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:56,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:57,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:58,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:10:58,177 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:11:04,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:05,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:06,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:07,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:08,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:09,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:10,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:11,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:12,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:13,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:13,185 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:11:19,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:20,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:21,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:22,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:23,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:24,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:25,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:26,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:27,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:28,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:28,193 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:11:34,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:35,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:36,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:37,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:38,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:39,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:40,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:41,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:42,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:43,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:43,202 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:11:49,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:50,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:51,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:52,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:53,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:54,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:55,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:56,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:57,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:58,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:11:58,210 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:12:04,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:05,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:06,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:07,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:08,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:09,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:10,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:11,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:12,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:13,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:13,218 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:12:19,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:20,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:21,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:22,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:23,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:24,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:25,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:26,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:27,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:28,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:28,226 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:12:34,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:35,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:36,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:37,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:38,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:39,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:40,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:41,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:42,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:43,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:43,234 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:12:49,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:50,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:51,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:52,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:53,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:54,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:55,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:56,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:57,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:58,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:12:58,243 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:13:04,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:05,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:06,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:07,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:08,247 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:09,247 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:10,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:11,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:12,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:13,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:13,251 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:13:19,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:20,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:21,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:22,254 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:23,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:24,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:25,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:26,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:27,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:28,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:28,259 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:13:34,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:35,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:36,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:37,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:38,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:39,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:40,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:41,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:42,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:43,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:43,268 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:13:49,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:50,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:51,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:52,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:53,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:54,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:55,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:56,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:57,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:58,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:13:58,277 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:14:04,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:05,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:06,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:07,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:08,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:09,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:10,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:11,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:12,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:13,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:13,286 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:14:19,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:20,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:21,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:22,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:23,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:24,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:25,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:26,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:27,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:28,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:28,294 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:14:34,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:35,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:36,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:37,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:38,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:39,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:40,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:41,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:42,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:43,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:43,301 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:14:49,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:50,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:51,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:52,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:53,305 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:54,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:55,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:56,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:57,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:58,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:14:58,309 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:15:04,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:05,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:06,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:07,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:08,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:09,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:10,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:11,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:12,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:13,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:13,318 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:15:19,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:20,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:21,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:22,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:23,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:24,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:25,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:26,324 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:27,324 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:28,325 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:28,326 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:15:34,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:35,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:36,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:37,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:38,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:39,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:40,331 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:41,332 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:42,332 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:43,333 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:43,334 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:15:49,335 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:50,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:51,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:52,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:53,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:54,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:55,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:56,340 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:57,340 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:58,341 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:15:58,342 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:16:04,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:05,344 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:06,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:07,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:08,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:09,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:10,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:11,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:12,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:13,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:13,350 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:16:19,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:20,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:21,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:22,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:23,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:24,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:25,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:26,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:27,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:28,358 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:28,359 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:16:34,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:35,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:36,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:37,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:38,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:39,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:40,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:41,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:42,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:43,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:43,367 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:16:49,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:50,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:51,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:52,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:53,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:54,371 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:55,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:56,373 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:57,373 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:58,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:16:58,375 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:17:04,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:05,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:06,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:07,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:08,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:09,379 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:10,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:11,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:12,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:13,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:13,383 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:17:19,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:20,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:21,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:22,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:23,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:24,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:25,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:26,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:27,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:28,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:28,392 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:17:34,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:35,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:36,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:37,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:38,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:39,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:40,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:41,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:42,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:43,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:43,400 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:17:49,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:50,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:51,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:52,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:53,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:54,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:55,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:56,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:57,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:58,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:17:58,408 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:18:04,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:05,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:06,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:07,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:08,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:09,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:10,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:11,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:12,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:13,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:13,416 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:18:19,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:20,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:21,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:22,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:23,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:24,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:25,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:26,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:27,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:28,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:28,424 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:18:34,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:35,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:36,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:37,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:38,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:39,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:40,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:41,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:42,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:43,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:43,433 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:18:49,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:50,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:51,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:52,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:53,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:54,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:55,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:56,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:57,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:58,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:18:58,441 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:19:04,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:05,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:06,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:07,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:08,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:09,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:10,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:11,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:12,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:13,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:13,449 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:19:19,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:20,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:21,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:22,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:23,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:24,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:25,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:26,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:27,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:28,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:28,457 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:19:34,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:35,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:36,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:37,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:38,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:39,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:40,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:41,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:42,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:43,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:43,465 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:19:49,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:50,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:51,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:52,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:53,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:54,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:55,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:56,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:57,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:58,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:19:58,473 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:20:04,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:05,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:06,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:07,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:08,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:09,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:10,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:11,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:12,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:13,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:13,481 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:20:19,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:20,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:21,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:22,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:23,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:24,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:25,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:26,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:27,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:28,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:28,489 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:20:34,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:35,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:36,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:37,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:38,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:39,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:40,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:41,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:42,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:43,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:43,497 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:20:49,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:50,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:51,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:52,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:53,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:54,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:55,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:56,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:57,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:58,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:20:58,506 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 00:21:04,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:21:05,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:21:06,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:21:07,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:21:08,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 00:21:09,408 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 00:21:09,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 00:22:18,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 00:22:18,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 00:22:19,289 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 00:22:19,351 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 00:22:19,351 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 00:22:19,356 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 00:22:19,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 00:22:19,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 00:22:19,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 00:22:19,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 00:22:19,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 00:22:19,475 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 00:22:19,482 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 00:22:19,488 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 00:22:19,492 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 00:22:19,495 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 00:22:19,495 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 00:22:19,495 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 00:22:19,505 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 52922
2015-10-05 00:22:19,505 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 00:22:19,663 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:52922
2015-10-05 00:22:19,745 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 00:22:19,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 00:22:19,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 00:22:19,784 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 00:22:19,795 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 00:22:19,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 00:22:19,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 00:22:19,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 00:22:19,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 00:22:19,875 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 00:22:19,875 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 00:22:20,166 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 7530@rushikesh2
2015-10-05 00:22:20,167 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /app/hadoop/tmp/dfs/data is not formatted for BP-1289193924-192.168.6.248-1444027603222
2015-10-05 00:22:20,167 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 00:22:20,311 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1289193924-192.168.6.248-1444027603222
2015-10-05 00:22:20,311 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1289193924-192.168.6.248-1444027603222
2015-10-05 00:22:20,312 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /app/hadoop/tmp/dfs/data/current/BP-1289193924-192.168.6.248-1444027603222 is not formatted for BP-1289193924-192.168.6.248-1444027603222
2015-10-05 00:22:20,312 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 00:22:20,312 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1289193924-192.168.6.248-1444027603222 directory /app/hadoop/tmp/dfs/data/current/BP-1289193924-192.168.6.248-1444027603222/current
2015-10-05 00:22:20,342 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-05 00:22:20,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1731338917;bpid=BP-1289193924-192.168.6.248-1444027603222;lv=-56;nsInfo=lv=-63;cid=CID-835494d7-181a-47ee-a6fd-c158f23855e2;nsid=1731338917;c=0;bpid=BP-1289193924-192.168.6.248-1444027603222;dnuuid=null
2015-10-05 00:22:20,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID aac2fe12-fb51-4a55-b833-01469e530aca
2015-10-05 00:22:20,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-45e44e1f-5356-4f65-b602-9b6cb8cc9ab8
2015-10-05 00:22:20,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-05 00:22:20,478 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-05 00:22:20,478 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1289193924-192.168.6.248-1444027603222
2015-10-05 00:22:20,479 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1289193924-192.168.6.248-1444027603222 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 00:22:20,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1289193924-192.168.6.248-1444027603222 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-10-05 00:22:20,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1289193924-192.168.6.248-1444027603222: 9ms
2015-10-05 00:22:20,489 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1289193924-192.168.6.248-1444027603222 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 00:22:20,489 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1289193924-192.168.6.248-1444027603222 on volume /app/hadoop/tmp/dfs/data/current: 0ms
2015-10-05 00:22:20,489 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-10-05 00:22:20,710 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1289193924-192.168.6.248-1444027603222 on volume /app/hadoop/tmp/dfs/data
2015-10-05 00:22:20,712 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1443986628712 with interval 21600000
2015-10-05 00:22:20,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1289193924-192.168.6.248-1444027603222 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-05 00:22:20,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1289193924-192.168.6.248-1444027603222 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-05 00:22:20,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-05 00:22:20,753 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-45e44e1f-5356-4f65-b602-9b6cb8cc9ab8): finished scanning block pool BP-1289193924-192.168.6.248-1444027603222
2015-10-05 00:22:20,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1289193924-192.168.6.248-1444027603222 (Datanode Uuid aac2fe12-fb51-4a55-b833-01469e530aca) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1
2015-10-05 00:22:20,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1289193924-192.168.6.248-1444027603222 (Datanode Uuid aac2fe12-fb51-4a55-b833-01469e530aca) service to rushikesh1/192.168.6.248:54310
2015-10-05 00:22:20,805 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-45e44e1f-5356-4f65-b602-9b6cb8cc9ab8): no suitable block pools found to scan.  Waiting 1814399905 ms.
2015-10-05 00:22:20,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x21a2735404c,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 22 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 00:22:20,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1289193924-192.168.6.248-1444027603222
2015-10-05 00:36:11,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1289193924-192.168.6.248-1444027603222:blk_1073741825_1001 src: /192.168.6.238:58298 dest: /192.168.6.249:50010
2015-10-05 00:36:17,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:58298, dest: /192.168.6.249:50010, bytes: 68143668, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-563106838_1, offset: 0, srvID: aac2fe12-fb51-4a55-b833-01469e530aca, blockid: BP-1289193924-192.168.6.248-1444027603222:blk_1073741825_1001, duration: 6092502417
2015-10-05 00:36:17,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1289193924-192.168.6.248-1444027603222:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-05 00:44:04,006 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-45e44e1f-5356-4f65-b602-9b6cb8cc9ab8): Scheduling suspect block BP-1289193924-192.168.6.248-1444027603222:blk_1073741825_1001 for rescanning.
2015-10-05 00:44:17,526 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-45e44e1f-5356-4f65-b602-9b6cb8cc9ab8): Not scheduling suspect block BP-1289193924-192.168.6.248-1444027603222:blk_1073741825_1001 for rescanning, because we rescanned it recently.
2015-10-05 00:44:17,527 WARN io.netty.handler.stream.ChunkedWriteHandler: ChunkedInput.isEndOfInput() failed
java.io.IOException: Stream closed
	at java.io.PushbackInputStream.ensureOpen(PushbackInputStream.java:74)
	at java.io.PushbackInputStream.read(PushbackInputStream.java:135)
	at io.netty.handler.stream.ChunkedStream.isEndOfInput(ChunkedStream.java:82)
	at io.netty.handler.stream.ChunkedWriteHandler.discard(ChunkedWriteHandler.java:177)
	at io.netty.handler.stream.ChunkedWriteHandler.doFlush(ChunkedWriteHandler.java:203)
	at io.netty.handler.stream.ChunkedWriteHandler.channelInactive(ChunkedWriteHandler.java:147)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
	at io.netty.handler.codec.ReplayingDecoder.channelInactive(ReplayingDecoder.java:352)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:769)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$5.run(AbstractChannel.java:567)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 00:45:09,509 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-45e44e1f-5356-4f65-b602-9b6cb8cc9ab8): no suitable block pools found to scan.  Waiting 1813031201 ms.
2015-10-05 00:53:48,717 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1289193924-192.168.6.248-1444027603222 Total blocks: 1, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-05 01:18:16,050 WARN io.netty.handler.stream.ChunkedWriteHandler: ChunkedInput.isEndOfInput() failed
java.io.IOException: Stream closed
	at java.io.PushbackInputStream.ensureOpen(PushbackInputStream.java:74)
	at java.io.PushbackInputStream.read(PushbackInputStream.java:135)
	at io.netty.handler.stream.ChunkedStream.isEndOfInput(ChunkedStream.java:82)
	at io.netty.handler.stream.ChunkedWriteHandler.discard(ChunkedWriteHandler.java:177)
	at io.netty.handler.stream.ChunkedWriteHandler.doFlush(ChunkedWriteHandler.java:203)
	at io.netty.handler.stream.ChunkedWriteHandler.channelInactive(ChunkedWriteHandler.java:147)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
	at io.netty.handler.codec.ReplayingDecoder.channelInactive(ReplayingDecoder.java:352)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:769)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$5.run(AbstractChannel.java:567)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 01:18:16,053 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-45e44e1f-5356-4f65-b602-9b6cb8cc9ab8): Scheduling suspect block BP-1289193924-192.168.6.248-1444027603222:blk_1073741825_1001 for rescanning.
2015-10-05 01:19:21,056 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-45e44e1f-5356-4f65-b602-9b6cb8cc9ab8): no suitable block pools found to scan.  Waiting 1810979654 ms.
2015-10-05 01:22:13,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x55eba9116ab,  containing 1 storage report(s), of which we sent 1. The reports had 1 total blocks and used 1 RPC(s). This took 1 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 01:22:13,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1289193924-192.168.6.248-1444027603222
2015-10-05 02:01:49,869 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-05 02:01:53,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:01:54,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:01:55,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:01:55,991 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 02:01:55,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:02:51,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:02:51,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:02:52,130 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:02:52,194 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:02:52,194 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:02:52,199 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:02:52,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:02:52,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:02:52,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:02:52,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:02:52,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:02:52,318 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:02:52,326 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:02:52,331 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:02:52,336 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:02:52,338 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:02:52,338 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:02:52,338 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:02:52,349 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43024
2015-10-05 02:02:52,349 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:02:52,506 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43024
2015-10-05 02:02:52,589 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:02:52,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:02:52,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:02:52,629 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:02:52,640 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:02:52,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:02:52,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:02:52,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:02:52,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:02:52,721 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:02:52,722 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:02:52,940 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 18121@rushikesh2
2015-10-05 02:02:52,942 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-0263b19d-6040-4c0e-a744-ce12d3c03f5c; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 02:02:52,943 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 02:02:52,944 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 02:02:53,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 02:02:55,045 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 02:02:55,047 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 02:02:55,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:06:11,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:06:11,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:06:12,523 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:06:12,586 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:06:12,586 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:06:12,591 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:06:12,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:06:12,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:06:12,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:06:12,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:06:12,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:06:12,708 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:06:12,716 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:06:12,721 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:06:12,726 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:06:12,728 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:06:12,728 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:06:12,728 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:06:12,738 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45793
2015-10-05 02:06:12,738 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:06:12,893 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45793
2015-10-05 02:06:12,974 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:06:12,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:06:12,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:06:13,013 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:06:13,024 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:06:13,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:06:13,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:06:13,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:06:13,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:06:13,102 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:06:13,103 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:06:13,311 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 18945@rushikesh2
2015-10-05 02:06:13,312 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-e69bfabe-c44c-4976-9ee1-582158f3e7c9; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 02:06:13,313 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 02:06:13,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 02:06:13,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 02:06:15,415 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 02:06:15,417 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 02:06:15,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:14:04,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:14:04,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:14:05,608 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:14:05,671 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:14:05,671 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:14:05,676 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:14:05,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:14:05,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:14:05,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:14:05,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:14:05,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:14:05,795 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:14:05,803 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:14:05,808 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:14:05,813 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:14:05,815 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:14:05,815 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:14:05,815 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:14:05,825 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 48998
2015-10-05 02:14:05,825 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:14:05,979 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:48998
2015-10-05 02:14:06,060 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:14:06,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:14:06,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:14:06,100 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:14:06,111 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:14:06,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:14:06,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:14:06,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:14:06,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:14:06,190 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:14:06,190 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:14:06,404 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 20001@rushikesh2
2015-10-05 02:14:06,406 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-e69bfabe-c44c-4976-9ee1-582158f3e7c9; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 02:14:06,407 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 02:14:06,408 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 02:14:06,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 02:14:08,509 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 02:14:08,511 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 02:14:08,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:15:07,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:15:07,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:15:07,882 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:15:07,945 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:15:07,945 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:15:07,950 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:15:07,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:15:07,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:15:07,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:15:07,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:15:07,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:15:08,069 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:15:08,077 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:15:08,082 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:15:08,087 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:15:08,089 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:15:08,089 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:15:08,089 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:15:08,099 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 57692
2015-10-05 02:15:08,099 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:15:08,253 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57692
2015-10-05 02:15:08,335 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:15:08,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:15:08,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:15:08,375 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:15:08,386 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:15:08,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:15:08,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:15:08,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:15:08,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:15:08,467 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:15:08,467 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:15:09,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:10,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:11,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:12,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:13,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:14,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:15,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:16,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:17,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:18,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:18,547 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:15:24,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:25,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:26,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:27,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:28,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:29,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:30,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:31,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:32,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:33,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:33,556 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:15:39,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:40,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:41,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:42,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:43,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:44,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:45,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:46,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:15:47,045 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 02:15:47,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:16:19,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:16:19,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:16:20,132 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:16:20,194 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:16:20,194 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:16:20,199 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:16:20,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:16:20,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:16:20,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:16:20,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:16:20,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:16:20,317 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:16:20,325 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:16:20,330 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:16:20,335 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:16:20,337 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:16:20,338 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:16:20,338 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:16:20,347 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40923
2015-10-05 02:16:20,347 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:16:20,501 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40923
2015-10-05 02:16:20,582 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:16:20,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:16:20,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:16:20,621 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:16:20,632 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:16:20,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:16:20,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:16:20,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:16:20,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:16:20,711 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:16:20,711 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:16:21,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:22,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:23,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:24,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:25,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:26,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:27,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:28,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:29,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:30,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:30,792 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:16:36,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:37,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:38,795 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:39,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:40,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:41,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:42,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:43,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:44,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:16:45,317 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 02:16:45,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:17:26,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:17:26,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:17:27,474 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:17:27,537 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:17:27,537 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:17:27,542 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:17:27,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:17:27,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:17:27,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:17:27,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:17:27,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:17:27,660 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:17:27,668 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:17:27,673 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:17:27,678 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:17:27,680 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:17:27,680 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:17:27,681 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:17:27,691 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 55251
2015-10-05 02:17:27,691 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:17:27,844 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:55251
2015-10-05 02:17:27,926 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:17:27,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:17:27,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:17:27,966 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:17:27,977 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:17:28,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:17:28,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:17:28,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:17:28,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:17:28,057 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:17:28,058 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:17:29,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:30,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:31,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:32,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:33,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:34,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:35,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:36,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:37,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:38,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:38,139 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:17:44,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:45,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:46,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:47,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:48,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:49,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:50,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:51,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:52,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:53,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:17:53,147 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:17:59,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:00,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:01,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:02,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:03,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:04,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:05,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:06,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:07,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:08,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:08,156 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:18:14,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:15,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:16,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:17,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:18,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:19,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:20,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:21,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:22,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:23,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:23,165 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:18:29,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:30,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:31,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:32,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:33,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:34,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:35,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:36,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:37,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:38,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:38,173 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:18:44,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:45,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:46,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:47,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:48,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:49,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:50,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:51,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:52,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:53,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:18:53,186 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:18:59,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:19:00,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:19:01,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:19:02,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:19:03,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:19:04,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:19:05,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:19:06,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:19:07,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:19:08,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:19:08,194 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:19:10,541 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 02:19:10,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:20:14,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:20:14,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:20:15,202 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:20:15,265 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:20:15,265 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:20:15,270 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:20:15,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:20:15,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:20:15,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:20:15,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:20:15,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:20:15,387 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:20:15,395 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:20:15,400 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:20:15,405 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:20:15,407 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:20:15,408 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:20:15,408 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:20:15,418 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38539
2015-10-05 02:20:15,418 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:20:15,575 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38539
2015-10-05 02:20:15,658 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:20:15,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:20:15,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:20:15,698 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:20:15,709 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:20:15,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:20:15,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:20:15,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:20:15,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:20:15,789 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:20:15,789 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:20:16,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:17,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:18,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:19,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:20,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:21,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:22,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:23,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:24,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:25,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:25,870 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:20:31,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:32,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:33,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:34,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:35,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:36,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:37,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:38,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:39,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:40,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:40,879 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:20:46,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:47,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:48,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:49,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:50,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:51,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:52,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:20:53,305 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 02:20:53,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:23:20,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:23:20,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:23:20,768 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:23:20,832 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:23:20,832 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:23:20,837 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:23:20,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:23:20,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:23:20,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:23:20,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:23:20,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:23:20,955 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:23:20,963 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:23:20,968 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:23:20,973 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:23:20,975 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:23:20,975 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:23:20,976 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:23:20,985 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45366
2015-10-05 02:23:20,985 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:23:21,138 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45366
2015-10-05 02:23:21,220 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:23:21,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:23:21,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:23:21,260 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:23:21,271 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:23:21,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:23:21,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:23:21,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:23:21,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:23:21,351 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:23:21,351 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:23:22,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:23,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:24,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:25,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:26,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:27,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:28,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:29,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:30,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:31,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:31,431 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:23:37,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:38,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:39,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:40,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:41,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:42,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:43,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:44,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:45,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:46,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:46,440 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:23:52,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:53,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:54,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:55,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:56,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:57,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:58,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:23:59,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:00,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:01,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:01,448 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:24:07,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:08,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:09,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:10,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:11,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:12,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:13,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:14,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:15,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:16,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:16,457 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:24:22,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:23,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:24,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:25,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:26,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:27,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:28,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:29,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:30,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:31,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:31,465 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:24:37,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:38,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:39,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:40,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:41,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:42,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:43,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:44,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:45,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:46,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:46,474 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:24:52,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:53,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:54,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:55,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:56,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:57,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:58,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:24:59,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:00,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:01,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:01,483 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:25:07,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:08,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:09,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:10,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:11,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:12,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:13,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:14,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:15,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:16,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:16,492 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:25:22,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:23,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:24,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:25,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:26,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:27,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:28,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:29,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:30,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:31,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:31,500 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:25:37,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:38,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:39,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:40,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:41,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:42,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:43,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:44,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:45,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:46,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:46,509 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:25:52,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:53,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:54,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:55,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:56,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:57,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:58,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:25:59,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:26:00,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:26:01,517 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:26:01,557 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:26:02,192 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 02:26:02,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:26:35,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:26:35,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:26:36,401 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:26:36,464 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:26:36,464 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:26:36,469 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:26:36,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:26:36,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:26:36,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:26:36,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:26:36,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:26:36,586 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:26:36,594 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:26:36,599 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:26:36,604 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:26:36,606 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:26:36,606 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:26:36,606 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:26:36,616 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37542
2015-10-05 02:26:36,616 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:26:36,769 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37542
2015-10-05 02:26:36,850 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:26:36,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:26:36,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:26:36,889 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:26:36,900 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:26:36,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:26:36,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:26:36,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:26:36,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:26:36,979 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:26:36,979 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:26:37,199 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 23754@rushikesh2
2015-10-05 02:26:37,201 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-a364f9a4-e79a-44a6-81f9-9bb294355762; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 02:26:37,202 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 02:26:37,203 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 02:26:37,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 02:26:39,304 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 02:26:39,306 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 02:26:39,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:28:07,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:28:07,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:28:08,004 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:28:08,068 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:28:08,068 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:28:08,073 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:28:08,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:28:08,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:28:08,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:28:08,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:28:08,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:28:08,192 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:28:08,200 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:28:08,205 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:28:08,210 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:28:08,212 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:28:08,212 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:28:08,212 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:28:08,222 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 60951
2015-10-05 02:28:08,222 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:28:08,379 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:60951
2015-10-05 02:28:08,462 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:28:08,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:28:08,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:28:08,502 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:28:08,513 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:28:08,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:28:08,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:28:08,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:28:08,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:28:08,593 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:28:08,593 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:28:08,805 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 24384@rushikesh2
2015-10-05 02:28:08,807 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-16e26b6a-c6d3-4305-9258-6ac2377f9361; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 02:28:08,808 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 02:28:08,809 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 02:28:08,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 02:28:10,910 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 02:28:10,912 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 02:28:10,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:29:53,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:29:53,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:29:54,034 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:29:54,097 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:29:54,097 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:29:54,102 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:29:54,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:29:54,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:29:54,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:29:54,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:29:54,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:29:54,220 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:29:54,228 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:29:54,233 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:29:54,238 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:29:54,240 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:29:54,241 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:29:54,241 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:29:54,251 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 47764
2015-10-05 02:29:54,251 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:29:54,404 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:47764
2015-10-05 02:29:54,486 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:29:54,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:29:54,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:29:54,526 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:29:54,537 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:29:54,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:29:54,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:29:54,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:29:54,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:29:54,617 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:29:54,617 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:29:54,846 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 25008@rushikesh2
2015-10-05 02:29:54,848 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-b7b4773c-aa10-4221-b19a-bc503e2eb21d; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 02:29:54,849 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 02:29:54,850 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 02:29:54,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 02:29:56,951 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 02:29:56,953 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 02:29:56,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:33:07,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:33:07,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:33:07,718 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:33:07,781 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:33:07,781 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:33:07,786 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:33:07,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:33:07,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:33:07,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:33:07,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:33:07,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:33:07,905 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:33:07,912 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:33:07,918 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:33:07,922 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:33:07,925 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:33:07,925 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:33:07,925 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:33:07,935 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45498
2015-10-05 02:33:07,935 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:33:08,088 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45498
2015-10-05 02:33:08,171 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:33:08,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:33:08,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:33:08,210 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:33:08,221 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:33:08,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:33:08,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:33:08,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:33:08,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:33:08,300 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:33:08,301 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:33:08,513 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 25808@rushikesh2
2015-10-05 02:33:08,514 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-3d97d519-575f-4203-ab3b-88fb90d7ad6e; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 02:33:08,515 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 02:33:08,516 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 02:33:08,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 02:33:10,617 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 02:33:10,619 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 02:33:10,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:35:07,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:35:07,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:35:07,936 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:35:07,999 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:35:07,999 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:35:08,004 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:35:08,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:35:08,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:35:08,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:35:08,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:35:08,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:35:08,122 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:35:08,130 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:35:08,135 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:35:08,140 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:35:08,142 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:35:08,142 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:35:08,142 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:35:08,152 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 33169
2015-10-05 02:35:08,152 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:35:08,304 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33169
2015-10-05 02:35:08,386 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:35:08,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:35:08,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:35:08,425 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:35:08,436 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:35:08,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:35:08,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:35:08,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:35:08,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:35:08,515 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:35:08,515 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:35:09,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:10,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:11,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:12,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:13,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:14,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:15,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:16,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:17,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:18,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:18,597 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:35:24,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:25,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:26,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:27,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:28,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:29,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:30,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:31,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:32,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:33,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:33,606 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:35:39,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:40,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:41,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:42,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:43,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:44,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:35:45,033 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 02:35:45,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:37:04,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:37:04,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:37:04,786 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:37:04,849 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:37:04,849 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:37:04,854 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:37:04,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:37:04,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:37:04,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:37:04,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:37:04,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:37:04,972 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:37:04,980 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:37:04,985 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:37:04,990 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:37:04,992 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:37:04,992 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:37:04,992 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:37:05,002 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34579
2015-10-05 02:37:05,002 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:37:05,155 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34579
2015-10-05 02:37:05,235 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:37:05,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:37:05,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:37:05,274 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:37:05,285 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:37:05,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:37:05,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:37:05,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:37:05,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:37:05,364 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:37:05,364 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:37:05,588 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 27103@rushikesh2
2015-10-05 02:37:05,590 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-0341059b-3255-4303-b873-ad811b4bbab8; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 02:37:05,590 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 02:37:05,592 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 02:37:05,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 02:37:07,693 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 02:37:07,694 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 02:37:07,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:39:08,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:39:08,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:39:08,784 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:39:08,847 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:39:08,847 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:39:08,852 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:39:08,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:39:08,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:39:08,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:39:08,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:39:08,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:39:08,972 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:39:08,979 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:39:08,984 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:39:08,989 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:39:08,992 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:39:08,992 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:39:08,992 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:39:09,002 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 56229
2015-10-05 02:39:09,002 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:39:09,155 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:56229
2015-10-05 02:39:09,238 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:39:09,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:39:09,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:39:09,278 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:39:09,289 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:39:09,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:39:09,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:39:09,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:39:09,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:39:09,368 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:39:09,368 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:39:09,587 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 27747@rushikesh2
2015-10-05 02:39:09,589 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-24649b29-13ea-4ce3-8cc6-d04d7aa1f2f6; datanode clusterID = CID-835494d7-181a-47ee-a6fd-c158f23855e2
2015-10-05 02:39:09,590 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 02:39:09,591 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 02:39:09,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 02:39:11,692 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 02:39:11,694 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 02:39:11,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:41:11,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:41:11,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:41:12,131 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:41:12,194 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:41:12,194 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:41:12,199 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:41:12,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:41:12,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:41:12,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:41:12,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:41:12,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:41:12,318 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:41:12,325 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:41:12,331 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:41:12,335 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:41:12,338 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:41:12,338 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:41:12,338 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:41:12,348 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 57039
2015-10-05 02:41:12,348 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:41:12,502 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57039
2015-10-05 02:41:12,583 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:41:12,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:41:12,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:41:12,623 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:41:12,634 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:41:12,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:41:12,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:41:12,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:41:12,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:41:12,714 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:41:12,715 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:41:13,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:14,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:15,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:16,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:17,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:18,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:19,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:20,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:21,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:22,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:22,796 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:41:28,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:29,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:30,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:31,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:32,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:33,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:34,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:35,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:36,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:37,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:37,805 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:41:43,806 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:44,806 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:45,807 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:46,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:47,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:48,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:49,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:50,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:51,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:52,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:52,813 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:41:58,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:41:59,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:42:00,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:42:01,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:42:02,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:42:03,186 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 02:42:03,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:47:16,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:47:16,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:47:16,717 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:47:16,782 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:47:16,782 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:47:16,787 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:47:16,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:47:16,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:47:16,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:47:16,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:47:16,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:47:16,900 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:47:16,907 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:47:16,912 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:47:16,917 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:47:16,919 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:47:16,919 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:47:16,919 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:47:16,928 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 54613
2015-10-05 02:47:16,929 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:47:17,082 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54613
2015-10-05 02:47:17,164 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:47:17,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:47:17,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:47:17,204 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:47:17,216 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:47:17,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:47:17,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:47:17,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:47:17,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:47:17,295 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:47:17,296 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:47:17,502 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 29465@rushikesh2
2015-10-05 02:47:17,503 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /app/hadoop/tmp/dfs/data is not formatted for BP-1395697138-192.168.6.248-1444036297760
2015-10-05 02:47:17,503 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 02:47:17,602 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1395697138-192.168.6.248-1444036297760
2015-10-05 02:47:17,602 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1395697138-192.168.6.248-1444036297760
2015-10-05 02:47:17,603 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /app/hadoop/tmp/dfs/data/current/BP-1395697138-192.168.6.248-1444036297760 is not formatted for BP-1395697138-192.168.6.248-1444036297760
2015-10-05 02:47:17,603 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 02:47:17,603 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1395697138-192.168.6.248-1444036297760 directory /app/hadoop/tmp/dfs/data/current/BP-1395697138-192.168.6.248-1444036297760/current
2015-10-05 02:47:17,636 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-05 02:47:17,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1415072375;bpid=BP-1395697138-192.168.6.248-1444036297760;lv=-56;nsInfo=lv=-63;cid=CID-45e22ac5-343f-48fe-b7d6-c5047ca13ee5;nsid=1415072375;c=0;bpid=BP-1395697138-192.168.6.248-1444036297760;dnuuid=null
2015-10-05 02:47:17,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 586ef8c8-9774-475e-b516-62db9d75b4f4
2015-10-05 02:47:17,766 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-43941787-de0e-46f9-a46b-c31a3f7d7c13
2015-10-05 02:47:17,766 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-05 02:47:17,771 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-05 02:47:17,772 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1395697138-192.168.6.248-1444036297760
2015-10-05 02:47:17,773 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1395697138-192.168.6.248-1444036297760 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 02:47:17,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1395697138-192.168.6.248-1444036297760 on /app/hadoop/tmp/dfs/data/current: 10ms
2015-10-05 02:47:17,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1395697138-192.168.6.248-1444036297760: 11ms
2015-10-05 02:47:17,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1395697138-192.168.6.248-1444036297760 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 02:47:17,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1395697138-192.168.6.248-1444036297760 on volume /app/hadoop/tmp/dfs/data/current: 1ms
2015-10-05 02:47:17,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-10-05 02:47:17,903 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1395697138-192.168.6.248-1444036297760 on volume /app/hadoop/tmp/dfs/data
2015-10-05 02:47:17,904 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444009318904 with interval 21600000
2015-10-05 02:47:17,905 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-43941787-de0e-46f9-a46b-c31a3f7d7c13): finished scanning block pool BP-1395697138-192.168.6.248-1444036297760
2015-10-05 02:47:17,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1395697138-192.168.6.248-1444036297760 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-05 02:47:17,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1395697138-192.168.6.248-1444036297760 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-05 02:47:17,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-05 02:47:17,944 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-43941787-de0e-46f9-a46b-c31a3f7d7c13): no suitable block pools found to scan.  Waiting 1814399959 ms.
2015-10-05 02:47:17,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1395697138-192.168.6.248-1444036297760 (Datanode Uuid 586ef8c8-9774-475e-b516-62db9d75b4f4) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1
2015-10-05 02:47:17,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1395697138-192.168.6.248-1444036297760 (Datanode Uuid 586ef8c8-9774-475e-b516-62db9d75b4f4) service to rushikesh1/192.168.6.248:54310
2015-10-05 02:47:17,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa031e370be0,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 28 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 02:47:17,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1395697138-192.168.6.248-1444036297760
2015-10-05 02:48:05,289 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-05 02:48:09,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:48:10,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:48:10,346 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 02:48:10,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:49:06,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:49:06,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:49:06,720 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:49:06,783 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:49:06,783 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:49:06,788 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:49:06,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:49:06,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:49:06,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:49:06,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:49:06,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:49:06,907 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:49:06,914 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:49:06,919 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:49:06,924 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:49:06,927 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:49:06,927 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:49:06,927 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:49:06,937 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 51443
2015-10-05 02:49:06,937 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:49:07,089 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:51443
2015-10-05 02:49:07,170 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:49:07,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:49:07,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:49:07,210 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:49:07,221 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:49:07,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:49:07,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:49:07,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:49:07,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:49:07,301 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:49:07,301 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:49:08,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:09,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:10,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:11,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:12,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:13,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:14,379 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:15,379 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:16,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:17,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:17,383 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:49:23,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:24,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:25,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:26,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:27,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:28,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:29,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:30,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:31,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:32,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:32,392 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:49:38,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:49:44,880 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 02:49:44,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:52:16,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:52:16,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:52:18,504 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:52:18,639 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:52:18,639 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:52:18,645 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:52:18,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:52:18,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:52:18,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:52:18,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:52:18,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:52:18,937 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:52:18,946 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:52:18,953 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:52:18,958 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:52:18,960 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:52:18,960 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:52:18,960 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:52:19,010 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 33659
2015-10-05 02:52:19,010 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:52:19,321 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33659
2015-10-05 02:52:19,555 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:52:19,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:52:19,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:52:19,819 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:52:19,890 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:52:20,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:52:20,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:52:20,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:52:20,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:52:20,867 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:52:20,868 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:52:22,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:23,078 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:24,078 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:25,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:26,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:27,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:28,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:29,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:30,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:31,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:31,084 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:52:37,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:38,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:39,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:40,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:41,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:42,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:43,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:44,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:45,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:46,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:46,093 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:52:52,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:53,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:54,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:55,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:56,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:57,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:58,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:52:59,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:00,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:01,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:01,102 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:53:07,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:08,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:09,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:10,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:11,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:12,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:13,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:14,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:15,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:16,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:16,110 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:53:22,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:23,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:24,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:25,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:26,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:27,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:28,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:29,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:30,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:31,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:31,118 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:53:37,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:38,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:39,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:40,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:41,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:42,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:43,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:44,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:45,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:46,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:46,127 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:53:52,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:53,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:54,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:55,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:56,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:57,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:58,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:53:59,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:00,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:01,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:01,135 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:54:07,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:08,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:09,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:10,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:11,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:12,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:13,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:14,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:15,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:16,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:16,143 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:54:22,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:23,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:24,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:25,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:26,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:27,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:28,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:29,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:30,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:31,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:31,152 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:54:37,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:38,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:39,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:40,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:41,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:42,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:43,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:44,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:45,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:46,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:46,160 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:54:52,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:53,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:54,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:55,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:56,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:57,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:58,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:54:59,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:00,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:01,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:01,169 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:55:07,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:08,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:09,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:10,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:11,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:12,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:13,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:14,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:15,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:16,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:16,177 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:55:22,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:23,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:24,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:25,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:26,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:27,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:28,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:29,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:30,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:31,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:31,185 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:55:37,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:38,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:39,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:40,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:41,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:42,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:43,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:44,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:45,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:46,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:46,194 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:55:52,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:53,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:54,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:55,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:56,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:57,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:58,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:55:59,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:00,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:01,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:01,202 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:56:07,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:08,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:09,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:10,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:11,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:12,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:13,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:14,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:15,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:16,210 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:16,212 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:56:22,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:23,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:24,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:25,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:26,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:27,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:28,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:29,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:30,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:31,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:31,220 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: rushikesh1/192.168.6.248:54310
2015-10-05 02:56:37,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:38,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 02:56:39,135 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 02:56:39,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 02:57:39,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 02:57:39,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 02:57:40,012 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 02:57:40,075 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 02:57:40,075 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 02:57:40,080 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 02:57:40,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 02:57:40,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 02:57:40,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 02:57:40,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 02:57:40,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 02:57:40,199 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 02:57:40,206 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 02:57:40,211 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 02:57:40,216 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 02:57:40,219 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 02:57:40,219 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 02:57:40,219 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 02:57:40,229 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 47873
2015-10-05 02:57:40,229 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 02:57:40,381 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:47873
2015-10-05 02:57:40,462 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 02:57:40,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 02:57:40,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 02:57:40,502 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 02:57:40,513 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 02:57:40,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 02:57:40,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 02:57:40,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 02:57:40,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 02:57:40,592 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 02:57:40,593 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 02:57:40,970 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3678@rushikesh2
2015-10-05 02:57:40,999 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-1c598070-6447-4935-b7b1-7797894dd1ab; datanode clusterID = CID-45e22ac5-343f-48fe-b7d6-c5047ca13ee5
2015-10-05 02:57:41,000 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 02:57:41,001 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 02:57:41,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 02:57:43,115 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 02:57:43,117 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 02:57:43,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 03:01:47,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 03:01:47,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 03:01:47,723 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 03:01:47,785 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 03:01:47,785 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 03:01:47,790 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 03:01:47,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 03:01:47,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 03:01:47,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 03:01:47,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 03:01:47,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 03:01:47,907 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 03:01:47,915 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 03:01:47,920 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 03:01:47,925 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 03:01:47,927 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 03:01:47,927 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 03:01:47,927 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 03:01:47,937 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 59412
2015-10-05 03:01:47,937 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 03:01:48,091 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:59412
2015-10-05 03:01:48,173 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 03:01:48,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 03:01:48,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 03:01:48,212 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 03:01:48,223 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 03:01:48,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 03:01:48,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 03:01:48,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 03:01:48,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 03:01:48,302 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 03:01:48,303 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 03:01:48,523 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 4521@rushikesh2
2015-10-05 03:01:48,524 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-1905fb61-f005-431a-8d60-f7510caef36b; datanode clusterID = CID-45e22ac5-343f-48fe-b7d6-c5047ca13ee5
2015-10-05 03:01:48,525 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 03:01:48,526 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 03:01:48,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 03:01:50,627 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 03:01:50,629 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 03:01:50,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 03:04:56,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 03:04:56,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 03:04:56,741 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 03:04:56,804 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 03:04:56,804 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 03:04:56,809 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 03:04:56,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 03:04:56,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 03:04:56,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 03:04:56,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 03:04:56,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 03:04:56,928 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 03:04:56,936 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 03:04:56,941 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 03:04:56,946 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 03:04:56,948 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 03:04:56,948 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 03:04:56,948 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 03:04:56,958 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 44850
2015-10-05 03:04:56,958 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 03:04:57,110 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44850
2015-10-05 03:04:57,191 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 03:04:57,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 03:04:57,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 03:04:57,230 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 03:04:57,241 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 03:04:57,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 03:04:57,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 03:04:57,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 03:04:57,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 03:04:57,320 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 03:04:57,320 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 03:04:57,543 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 5322@rushikesh2
2015-10-05 03:04:57,544 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-1905fb61-f005-431a-8d60-f7510caef36b; datanode clusterID = CID-45e22ac5-343f-48fe-b7d6-c5047ca13ee5
2015-10-05 03:04:57,545 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 03:04:57,546 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 03:04:57,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 03:04:59,647 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 03:04:59,649 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 03:04:59,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 03:06:27,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 03:06:27,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 03:06:28,575 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 03:06:28,638 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 03:06:28,638 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 03:06:28,643 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 03:06:28,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 03:06:28,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 03:06:28,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 03:06:28,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 03:06:28,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 03:06:28,762 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 03:06:28,770 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 03:06:28,775 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 03:06:28,780 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 03:06:28,782 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 03:06:28,782 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 03:06:28,782 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 03:06:28,792 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 54454
2015-10-05 03:06:28,792 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 03:06:28,949 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54454
2015-10-05 03:06:29,030 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 03:06:29,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 03:06:29,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 03:06:29,069 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 03:06:29,081 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 03:06:29,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 03:06:29,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 03:06:29,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 03:06:29,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 03:06:29,160 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 03:06:29,160 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 03:06:29,388 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 5968@rushikesh2
2015-10-05 03:06:29,390 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-d6becb97-7542-4f0c-8e82-431112946387; datanode clusterID = CID-45e22ac5-343f-48fe-b7d6-c5047ca13ee5
2015-10-05 03:06:29,391 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 03:06:29,392 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 03:06:29,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 03:06:31,493 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 03:06:31,495 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 03:06:31,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 03:08:16,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 03:08:16,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 03:08:16,649 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 03:08:16,711 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 03:08:16,711 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 03:08:16,717 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 03:08:16,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 03:08:16,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 03:08:16,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 03:08:16,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 03:08:16,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 03:08:16,835 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 03:08:16,843 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 03:08:16,848 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 03:08:16,853 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 03:08:16,855 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 03:08:16,855 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 03:08:16,855 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 03:08:16,865 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 57878
2015-10-05 03:08:16,865 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 03:08:17,019 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57878
2015-10-05 03:08:17,100 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 03:08:17,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 03:08:17,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 03:08:17,140 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 03:08:17,150 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 03:08:17,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 03:08:17,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 03:08:17,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 03:08:17,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 03:08:17,230 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 03:08:17,230 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 03:08:17,478 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 6603@rushikesh2
2015-10-05 03:08:17,479 WARN org.apache.hadoop.hdfs.server.common.Storage: java.io.IOException: Incompatible clusterIDs in /app/hadoop/tmp/dfs/data: namenode clusterID = CID-0bdb7046-0c42-4885-a155-0fa51af982fa; datanode clusterID = CID-45e22ac5-343f-48fe-b7d6-c5047ca13ee5
2015-10-05 03:08:17,480 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
	at java.lang.Thread.run(Thread.java:745)
2015-10-05 03:08:17,481 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310
2015-10-05 03:08:17,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-10-05 03:08:19,582 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-05 03:08:19,584 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-05 03:08:19,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 03:10:16,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 03:10:16,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 03:10:17,339 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /app/hadoop/tmp/dfs/data : 
java.io.FileNotFoundException: File file:/app/hadoop/tmp/dfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:606)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:819)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:596)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:139)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:156)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2344)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2386)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2368)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2260)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2307)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2484)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2508)
2015-10-05 03:10:17,340 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/app/hadoop/tmp/dfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2395)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2368)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2260)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2307)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2484)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2508)
2015-10-05 03:10:17,342 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-10-05 03:10:17,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 03:12:11,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 03:12:11,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 03:12:12,156 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /app/hadoop/tmp/dfs/data : 
java.io.FileNotFoundException: File file:/app/hadoop/tmp/dfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:606)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:819)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:596)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:139)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:156)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2344)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2386)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2368)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2260)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2307)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2484)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2508)
2015-10-05 03:12:12,158 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/app/hadoop/tmp/dfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2395)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2368)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2260)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2307)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2484)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2508)
2015-10-05 03:12:12,159 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-10-05 03:12:12,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-05 03:21:43,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-05 03:21:43,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-05 03:21:44,042 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-05 03:21:44,108 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-05 03:21:44,108 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-05 03:21:44,113 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-05 03:21:44,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-05 03:21:44,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-05 03:21:44,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-05 03:21:44,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-05 03:21:44,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-05 03:21:44,227 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-05 03:21:44,234 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-05 03:21:44,239 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-05 03:21:44,243 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-05 03:21:44,245 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-05 03:21:44,245 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-05 03:21:44,245 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-05 03:21:44,255 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 44455
2015-10-05 03:21:44,255 INFO org.mortbay.log: jetty-6.1.26
2015-10-05 03:21:44,408 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44455
2015-10-05 03:21:44,490 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-05 03:21:44,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-05 03:21:44,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-05 03:21:44,531 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-05 03:21:44,542 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-05 03:21:44,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-05 03:21:44,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-05 03:21:44,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-05 03:21:44,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-05 03:21:44,623 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-05 03:21:44,623 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-05 03:21:44,851 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 9224@rushikesh2
2015-10-05 03:21:44,852 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /app/hadoop/tmp/dfs/data is not formatted for BP-1750158012-192.168.6.248-1444037565733
2015-10-05 03:21:44,852 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 03:21:44,995 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-05 03:21:44,995 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-05 03:21:44,996 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733 is not formatted for BP-1750158012-192.168.6.248-1444037565733
2015-10-05 03:21:44,996 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-05 03:21:44,996 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1750158012-192.168.6.248-1444037565733 directory /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current
2015-10-05 03:21:45,018 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-05 03:21:45,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=null
2015-10-05 03:21:45,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-10-05 03:21:45,189 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-10-05 03:21:45,189 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-05 03:21:45,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-05 03:21:45,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 03:21:45,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 03:21:45,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 37ms
2015-10-05 03:21:45,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 38ms
2015-10-05 03:21:45,236 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-05 03:21:45,236 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 0ms
2015-10-05 03:21:45,236 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-10-05 03:21:45,460 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data
2015-10-05 03:21:45,462 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444009336462 with interval 21600000
2015-10-05 03:21:45,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-05 03:21:45,499 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): finished scanning block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 03:21:45,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-05 03:21:45,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-05 03:21:45,565 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1814399895 ms.
2015-10-05 03:21:45,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=4
2015-10-05 03:21:45,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-10-05 03:21:45,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1b6b2161b54,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-05 03:21:45,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-05 03:27:41,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741825_1001 src: /192.168.6.248:51140 dest: /192.168.6.249:50010
2015-10-05 03:27:47,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:51140, dest: /192.168.6.249:50010, bytes: 68143668, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1631392185_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741825_1001, duration: 6127539057
2015-10-05 03:27:47,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-05 03:49:29,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2015-10-05 03:49:29,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741825_1001 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741825
2015-10-05 03:53:25,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004 src: /192.168.6.237:54397 dest: /192.168.6.249:50010
2015-10-05 03:53:25,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54397, dest: /192.168.6.249:50010, bytes: 4045946, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1734310134_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004, duration: 366272054
2015-10-05 03:53:25,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-10-05 03:54:14,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741830_1006 src: /192.168.6.248:51562 dest: /192.168.6.249:50010
2015-10-05 03:54:14,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:51562, dest: /192.168.6.249:50010, bytes: 4045946, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-633820094_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741830_1006, duration: 366864624
2015-10-05 03:54:14,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-10-05 04:25:08,616 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-05 04:25:12,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 04:25:13,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 04:25:14,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-05 04:25:14,775 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-05 04:25:14,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-06 00:32:28,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-06 00:32:28,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-06 00:32:30,302 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-06 00:32:30,493 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-06 00:32:30,493 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-06 00:32:30,500 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-06 00:32:30,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-06 00:32:30,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-06 00:32:30,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-06 00:32:30,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-06 00:32:30,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-06 00:32:30,773 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-06 00:32:30,806 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-06 00:32:30,830 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-06 00:32:30,837 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-06 00:32:30,841 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-06 00:32:30,841 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-06 00:32:30,841 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-06 00:32:30,864 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 33313
2015-10-06 00:32:30,864 INFO org.mortbay.log: jetty-6.1.26
2015-10-06 00:32:31,170 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33313
2015-10-06 00:32:31,327 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-06 00:32:31,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-06 00:32:31,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-06 00:32:31,551 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-06 00:32:31,569 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-06 00:32:31,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-06 00:32:31,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-06 00:32:31,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-06 00:32:31,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-06 00:32:31,875 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-06 00:32:31,875 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-06 00:32:32,338 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 2977@rushikesh2
2015-10-06 00:32:32,481 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-06 00:32:32,481 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-06 00:32:32,482 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-06 00:32:32,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-10-06 00:32:32,613 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-10-06 00:32:32,613 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-06 00:32:32,664 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-06 00:32:32,664 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 00:32:32,665 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 00:32:32,770 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 104ms
2015-10-06 00:32:32,770 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 105ms
2015-10-06 00:32:32,771 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 00:32:32,775 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-10-06 00:32:32,775 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 6ms
2015-10-06 00:32:33,470 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1738151990 ms.
2015-10-06 00:32:33,473 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444088845473 with interval 21600000
2015-10-06 00:32:33,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-06 00:32:33,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-06 00:32:33,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-06 00:32:33,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=72
2015-10-06 00:32:33,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-10-06 00:32:33,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4835d3bc74,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 3 msec to generate and 48 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 00:32:33,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 03:01:38,077 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 03:01:41,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 03:01:42,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 03:01:43,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 03:01:44,385 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-06 03:01:44,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-06 03:03:09,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-06 03:03:09,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-06 03:03:09,964 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-06 03:03:10,027 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-06 03:03:10,027 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-06 03:03:10,031 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-06 03:03:10,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-06 03:03:10,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-06 03:03:10,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-06 03:03:10,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-06 03:03:10,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-06 03:03:10,150 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-06 03:03:10,158 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-06 03:03:10,163 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-06 03:03:10,168 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-06 03:03:10,170 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-06 03:03:10,170 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-06 03:03:10,170 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-06 03:03:10,180 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 56289
2015-10-06 03:03:10,180 INFO org.mortbay.log: jetty-6.1.26
2015-10-06 03:03:10,332 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:56289
2015-10-06 03:03:10,413 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-06 03:03:10,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-06 03:03:10,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-06 03:03:10,453 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-06 03:03:10,464 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-06 03:03:10,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-06 03:03:10,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-06 03:03:10,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-06 03:03:10,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-06 03:03:10,542 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-06 03:03:10,543 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-06 03:03:10,784 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 18864@rushikesh2
2015-10-06 03:03:10,858 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-06 03:03:10,858 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-06 03:03:10,859 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-06 03:03:10,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-10-06 03:03:10,956 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-10-06 03:03:10,956 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-06 03:03:10,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-06 03:03:10,987 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 03:03:10,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 03:03:10,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 8200192
2015-10-06 03:03:10,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-10-06 03:03:10,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-10-06 03:03:10,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 03:03:10,998 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-10-06 03:03:10,999 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-06 03:03:11,167 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1729114294 ms.
2015-10-06 03:03:11,169 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444089454169 with interval 21600000
2015-10-06 03:03:11,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-06 03:03:11,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-06 03:03:11,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-06 03:03:11,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=89
2015-10-06 03:03:11,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-10-06 03:03:11,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8807230d98b,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 3 msec to generate and 22 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 03:03:11,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 03:58:49,537 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 03:59:12,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); maxRetries=45
2015-10-06 03:59:32,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); maxRetries=45
2015-10-06 03:59:52,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); maxRetries=45
2015-10-06 04:00:12,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); maxRetries=45
2015-10-06 04:00:32,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); maxRetries=45
2015-10-06 04:00:52,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); maxRetries=45
2015-10-06 04:01:12,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); maxRetries=45
2015-10-06 04:01:32,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); maxRetries=45
2015-10-06 04:01:52,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); maxRetries=45
2015-10-06 04:11:31,536 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 04:11:54,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); maxRetries=45
2015-10-06 04:12:14,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); maxRetries=45
2015-10-06 04:12:34,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); maxRetries=45
2015-10-06 04:20:58,595 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 04:21:02,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 04:21:03,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 04:21:04,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 04:21:04,833 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-06 04:21:04,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-06 04:21:44,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-06 04:21:44,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-06 04:21:44,859 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-06 04:21:44,922 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-06 04:21:44,922 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-06 04:21:44,927 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-06 04:21:44,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-06 04:21:44,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-06 04:21:44,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-06 04:21:44,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-06 04:21:44,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-06 04:21:45,050 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-06 04:21:45,058 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-06 04:21:45,067 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-06 04:21:45,072 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-06 04:21:45,075 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-06 04:21:45,075 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-06 04:21:45,075 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-06 04:21:45,085 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 54365
2015-10-06 04:21:45,085 INFO org.mortbay.log: jetty-6.1.26
2015-10-06 04:21:45,240 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54365
2015-10-06 04:21:45,325 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-06 04:21:45,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-06 04:21:45,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-06 04:21:45,366 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-06 04:21:45,377 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-06 04:21:45,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-06 04:21:45,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-06 04:21:45,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-06 04:21:45,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-06 04:21:45,458 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-06 04:21:45,459 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-06 04:21:45,682 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 26738@rushikesh2
2015-10-06 04:21:45,756 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-06 04:21:45,756 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-06 04:21:45,757 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-06 04:21:45,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-10-06 04:21:45,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-10-06 04:21:45,822 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-06 04:21:45,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-06 04:21:45,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 04:21:45,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 04:21:45,863 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 8200192
2015-10-06 04:21:45,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-10-06 04:21:45,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-10-06 04:21:45,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 04:21:45,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-10-06 04:21:45,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-06 04:21:46,030 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1724399430 ms.
2015-10-06 04:21:46,032 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444090096031 with interval 21600000
2015-10-06 04:21:46,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-06 04:21:46,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-06 04:21:46,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-06 04:21:46,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=94
2015-10-06 04:21:46,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-10-06 04:21:46,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xcca35d0f628,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 3 msec to generate and 21 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 04:21:46,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 05:38:16,038 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-06 06:05:48,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1277a0089610,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 0 msec to generate and 1 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 06:05:48,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 06:08:18,451 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 06:08:22,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 06:08:23,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 06:08:23,666 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-06 06:08:23,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-06 06:11:38,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-06 06:11:38,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-06 06:11:39,581 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-06 06:11:39,647 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-06 06:11:39,647 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-06 06:11:39,652 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-06 06:11:39,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-06 06:11:39,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-06 06:11:39,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-06 06:11:39,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-06 06:11:39,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-06 06:11:39,772 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-06 06:11:39,779 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-06 06:11:39,785 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-06 06:11:39,789 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-06 06:11:39,792 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-06 06:11:39,792 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-06 06:11:39,792 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-06 06:11:39,803 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40034
2015-10-06 06:11:39,803 INFO org.mortbay.log: jetty-6.1.26
2015-10-06 06:11:39,961 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40034
2015-10-06 06:11:40,044 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-06 06:11:40,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-06 06:11:40,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-06 06:11:40,085 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-06 06:11:40,097 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-06 06:11:40,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-06 06:11:40,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-06 06:11:40,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-06 06:11:40,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-06 06:11:40,180 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-06 06:11:40,180 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-06 06:11:40,393 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 5442@rushikesh2
2015-10-06 06:11:40,475 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-06 06:11:40,475 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-06 06:11:40,476 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-06 06:11:40,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-10-06 06:11:40,543 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-10-06 06:11:40,543 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-06 06:11:40,578 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-06 06:11:40,579 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 06:11:40,579 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 06:11:40,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 8200192
2015-10-06 06:11:40,588 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-10-06 06:11:40,588 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-10-06 06:11:40,588 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 06:11:40,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-10-06 06:11:40,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-06 06:11:40,751 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1717804709 ms.
2015-10-06 06:11:40,752 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444099271752 with interval 21600000
2015-10-06 06:11:40,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-06 06:11:40,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-06 06:11:40,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-06 06:11:40,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=99
2015-10-06 06:11:40,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-10-06 06:11:40,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x12c9a9ccc950,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 3 msec to generate and 21 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 06:11:40,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 06:17:40,173 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 06:17:44,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 06:17:44,888 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-06 06:17:44,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-06 06:19:23,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-06 06:19:23,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-06 06:19:24,305 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-06 06:19:24,372 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-06 06:19:24,372 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-06 06:19:24,378 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-06 06:19:24,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-06 06:19:24,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-06 06:19:24,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-06 06:19:24,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-06 06:19:24,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-06 06:19:24,501 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-06 06:19:24,509 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-06 06:19:24,516 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-06 06:19:24,521 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-06 06:19:24,523 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-06 06:19:24,524 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-06 06:19:24,524 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-06 06:19:24,534 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 33770
2015-10-06 06:19:24,534 INFO org.mortbay.log: jetty-6.1.26
2015-10-06 06:19:24,699 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33770
2015-10-06 06:19:24,783 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-06 06:19:24,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-06 06:19:24,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-06 06:19:24,822 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-06 06:19:24,834 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-06 06:19:24,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-06 06:19:24,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-06 06:19:24,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-06 06:19:24,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-06 06:19:24,914 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-06 06:19:24,914 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-06 06:19:25,161 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 6681@rushikesh2
2015-10-06 06:19:25,239 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-06 06:19:25,240 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-06 06:19:25,240 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-06 06:19:25,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-10-06 06:19:25,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-10-06 06:19:25,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-06 06:19:25,335 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-06 06:19:25,335 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 06:19:25,336 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 06:19:25,345 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 8200192
2015-10-06 06:19:25,347 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 11ms
2015-10-06 06:19:25,347 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 12ms
2015-10-06 06:19:25,348 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-06 06:19:25,352 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-10-06 06:19:25,352 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-10-06 06:19:25,509 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1717339951 ms.
2015-10-06 06:19:25,511 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444104351511 with interval 21600000
2015-10-06 06:19:25,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-06 06:19:25,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-06 06:19:25,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-06 06:19:25,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=102
2015-10-06 06:19:25,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-10-06 06:19:25,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1335df8ec9ff,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 2 msec to generate and 21 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-06 06:19:25,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-06 06:20:45,907 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-06 06:20:49,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-06 06:20:50,636 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-06 06:20:50,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-07 01:34:27,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-07 01:34:27,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-07 01:34:29,191 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-07 01:34:29,413 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-07 01:34:29,413 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-07 01:34:29,422 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-07 01:34:29,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-07 01:34:29,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-07 01:34:29,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-07 01:34:29,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-07 01:34:29,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-07 01:34:29,766 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-07 01:34:29,778 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-07 01:34:29,786 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-07 01:34:29,793 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-07 01:34:29,796 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-07 01:34:29,796 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-07 01:34:29,796 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-07 01:34:29,829 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43636
2015-10-07 01:34:29,830 INFO org.mortbay.log: jetty-6.1.26
2015-10-07 01:34:30,223 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43636
2015-10-07 01:34:30,431 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-07 01:34:30,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-07 01:34:30,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-07 01:34:30,805 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-07 01:34:30,837 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-07 01:34:31,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-07 01:34:31,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-07 01:34:31,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-07 01:34:31,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-07 01:34:31,496 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-07 01:34:31,496 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-07 01:34:32,001 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 8433@rushikesh2
2015-10-07 01:34:32,088 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-07 01:34:32,088 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-07 01:34:32,088 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-07 01:34:32,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-10-07 01:34:32,193 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-10-07 01:34:32,193 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-07 01:34:32,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-07 01:34:32,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 01:34:32,243 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 01:34:32,278 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 35ms
2015-10-07 01:34:32,278 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 36ms
2015-10-07 01:34:32,279 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 01:34:32,283 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-10-07 01:34:32,283 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-10-07 01:34:32,650 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1648032810 ms.
2015-10-07 01:34:32,653 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444168111652 with interval 21600000
2015-10-07 01:34:32,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-07 01:34:32,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-07 01:34:32,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-07 01:34:32,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=105
2015-10-07 01:34:32,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-10-07 01:34:32,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2f9a8df18a8,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 4 msec to generate and 33 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-07 01:34:32,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 01:35:02,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 src: /192.168.6.248:36828 dest: /192.168.6.249:50010
2015-10-07 01:35:02,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 src: /192.168.6.248:36827 dest: /192.168.6.249:50010
2015-10-07 01:35:25,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 src: /192.168.6.248:36827 dest: /192.168.6.249:50010 of size 134217728
2015-10-07 01:35:25,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 src: /192.168.6.248:36828 dest: /192.168.6.249:50010 of size 134217728
2015-10-07 01:35:25,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 src: /192.168.6.248:36834 dest: /192.168.6.249:50010
2015-10-07 01:35:25,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 src: /192.168.6.248:36835 dest: /192.168.6.249:50010
2015-10-07 01:35:25,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 received exception org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException: Block BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 already exists in state FINALIZED and thus cannot be created.
2015-10-07 01:35:25,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 received exception org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException: Block BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 already exists in state FINALIZED and thus cannot be created.
2015-10-07 01:35:25,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh2:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.6.248:36834 dst: /192.168.6.249:50010; org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException: Block BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 already exists in state FINALIZED and thus cannot be created.
2015-10-07 01:35:25,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh2:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.6.248:36835 dst: /192.168.6.249:50010; org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException: Block BP-1750158012-192.168.6.248-1444037565733:blk_1073741829_1005 already exists in state FINALIZED and thus cannot be created.
2015-10-07 01:36:37,189 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-07 01:36:41,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-07 01:36:42,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-07 01:36:42,330 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-07 01:36:42,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-07 01:39:32,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-07 01:39:32,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-07 01:39:33,352 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-07 01:39:33,415 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-07 01:39:33,415 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-07 01:39:33,420 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-07 01:39:33,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-07 01:39:33,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-07 01:39:33,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-07 01:39:33,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-07 01:39:33,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-07 01:39:33,539 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-07 01:39:33,546 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-07 01:39:33,552 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-07 01:39:33,556 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-07 01:39:33,559 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-07 01:39:33,559 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-07 01:39:33,559 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-07 01:39:33,569 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41006
2015-10-07 01:39:33,569 INFO org.mortbay.log: jetty-6.1.26
2015-10-07 01:39:33,722 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41006
2015-10-07 01:39:33,804 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-07 01:39:33,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-07 01:39:33,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-07 01:39:33,843 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-07 01:39:33,854 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-07 01:39:33,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-07 01:39:33,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-07 01:39:33,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-07 01:39:33,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-07 01:39:33,934 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-07 01:39:33,934 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-07 01:39:34,158 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 9404@rushikesh2
2015-10-07 01:39:34,233 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-07 01:39:34,233 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-07 01:39:34,234 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-07 01:39:34,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-10-07 01:39:34,298 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-10-07 01:39:34,298 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-07 01:39:34,332 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-07 01:39:34,332 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 01:39:34,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 01:39:34,340 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278732814
2015-10-07 01:39:34,341 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-10-07 01:39:34,341 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-10-07 01:39:34,341 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 01:39:34,344 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-10-07 01:39:34,344 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-07 01:39:34,503 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1647730957 ms.
2015-10-07 01:39:34,504 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444179311504 with interval 21600000
2015-10-07 01:39:34,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-07 01:39:34,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-07 01:39:34,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-07 01:39:34,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=106
2015-10-07 01:39:34,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-10-07 01:39:34,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x33fed39c8e3,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 20 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-07 01:39:34,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 02:13:48,928 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-07 02:13:52,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-07 02:13:53,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-07 02:13:54,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-07 02:13:55,275 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-07 02:13:55,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-07 02:14:38,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-07 02:14:38,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-07 02:14:38,786 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-07 02:14:38,849 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-07 02:14:38,849 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-07 02:14:38,854 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-07 02:14:38,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-07 02:14:38,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-07 02:14:38,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-07 02:14:38,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-07 02:14:38,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-07 02:14:38,972 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-07 02:14:38,980 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-07 02:14:38,985 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-07 02:14:38,990 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-07 02:14:38,992 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-07 02:14:38,992 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-07 02:14:38,992 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-07 02:14:39,002 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 47176
2015-10-07 02:14:39,002 INFO org.mortbay.log: jetty-6.1.26
2015-10-07 02:14:39,154 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:47176
2015-10-07 02:14:39,237 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-07 02:14:39,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-07 02:14:39,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-07 02:14:39,276 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-07 02:14:39,287 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-07 02:14:39,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-07 02:14:39,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-07 02:14:39,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-07 02:14:39,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-07 02:14:39,367 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-07 02:14:39,367 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-07 02:14:39,578 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 13186@rushikesh2
2015-10-07 02:14:39,652 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-07 02:14:39,653 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-07 02:14:39,653 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-07 02:14:39,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-10-07 02:14:39,718 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-10-07 02:14:39,718 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-07 02:14:39,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-07 02:14:39,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 02:14:39,753 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 02:14:39,760 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278749184
2015-10-07 02:14:39,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 7ms
2015-10-07 02:14:39,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-10-07 02:14:39,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 02:14:39,764 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-10-07 02:14:39,764 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-10-07 02:14:39,923 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1645625537 ms.
2015-10-07 02:14:39,924 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444174253924 with interval 21600000
2015-10-07 02:14:39,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-07 02:14:39,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-07 02:14:39,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-07 02:14:39,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=107
2015-10-07 02:14:39,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-10-07 02:14:39,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x52a220406f3,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 19 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-07 02:14:39,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 02:40:33,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x693cf191eec,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-07 02:40:33,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 03:48:24,361 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-07 03:48:28,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-07 03:48:28,795 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-07 03:48:28,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-07 03:50:28,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-07 03:50:28,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-07 03:50:29,405 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-07 03:50:29,468 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-07 03:50:29,468 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-07 03:50:29,473 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-07 03:50:29,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-07 03:50:29,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-07 03:50:29,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-07 03:50:29,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-07 03:50:29,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-07 03:50:29,592 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-07 03:50:29,600 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-07 03:50:29,605 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-07 03:50:29,610 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-07 03:50:29,612 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-07 03:50:29,612 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-07 03:50:29,612 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-07 03:50:29,622 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41959
2015-10-07 03:50:29,622 INFO org.mortbay.log: jetty-6.1.26
2015-10-07 03:50:29,778 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41959
2015-10-07 03:50:29,859 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-07 03:50:29,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-07 03:50:29,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-07 03:50:29,898 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-07 03:50:29,909 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-07 03:50:29,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-07 03:50:29,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-07 03:50:29,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-07 03:50:29,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-07 03:50:29,989 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-07 03:50:29,989 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-07 03:50:30,220 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 24099@rushikesh2
2015-10-07 03:50:30,326 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-07 03:50:30,327 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-07 03:50:30,327 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-07 03:50:30,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-10-07 03:50:30,394 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-10-07 03:50:30,394 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-07 03:50:30,423 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-07 03:50:30,423 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 03:50:30,424 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 03:50:30,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278749184
2015-10-07 03:50:30,432 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-10-07 03:50:30,432 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-10-07 03:50:30,432 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-07 03:50:30,435 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-10-07 03:50:30,435 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-10-07 03:50:30,601 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1639874859 ms.
2015-10-07 03:50:30,603 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444171493603 with interval 21600000
2015-10-07 03:50:30,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-07 03:50:30,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-07 03:50:30,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-07 03:50:30,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=113
2015-10-07 03:50:30,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-10-07 03:50:30,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa651159e89e,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 21 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-07 03:50:30,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-07 04:14:53,613 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-07 05:12:11,984 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-07 05:12:15,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-07 05:12:16,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-07 05:12:17,730 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-07 05:12:17,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-10-14 23:04:17,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-10-14 23:04:17,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-14 23:04:19,389 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-14 23:04:19,596 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-14 23:04:19,596 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-14 23:04:19,603 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-10-14 23:04:19,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-10-14 23:04:19,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-14 23:04:19,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-14 23:04:19,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-14 23:04:19,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-14 23:04:19,931 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-14 23:04:19,941 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-10-14 23:04:19,966 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-14 23:04:19,971 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-14 23:04:19,974 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-14 23:04:19,974 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-14 23:04:19,974 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-14 23:04:19,987 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 55838
2015-10-14 23:04:19,987 INFO org.mortbay.log: jetty-6.1.26
2015-10-14 23:04:20,274 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:55838
2015-10-14 23:04:20,465 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-10-14 23:04:20,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-10-14 23:04:20,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-14 23:04:20,671 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-14 23:04:20,719 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-14 23:04:20,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-14 23:04:20,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-14 23:04:20,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-14 23:04:21,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-10-14 23:04:21,071 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-14 23:04:21,071 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-14 23:04:21,504 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 2703@rushikesh2
2015-10-14 23:04:21,647 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-10-14 23:04:21,647 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-10-14 23:04:21,648 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-14 23:04:21,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-10-14 23:04:21,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-10-14 23:04:21,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-10-14 23:04:21,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-14 23:04:21,806 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-14 23:04:21,806 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-14 23:04:21,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 66ms
2015-10-14 23:04:21,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 68ms
2015-10-14 23:04:21,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-10-14 23:04:21,878 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-10-14 23:04:21,878 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-10-14 23:04:22,174 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 965843286 ms.
2015-10-14 23:04:22,176 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1444859103176 with interval 21600000
2015-10-14 23:04:22,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-10-14 23:04:22,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-10-14 23:04:22,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-14 23:04:22,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=118
2015-10-14 23:04:22,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-10-14 23:04:22,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x28b5d51f14,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 39 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-14 23:04:22,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-15 01:21:54,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x7a9f5a61bc9,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-10-15 01:21:54,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-10-15 03:15:03,185 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-15 04:08:33,006 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-10-15 04:08:36,765 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-15 04:08:36,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-16 22:33:01,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-16 22:33:01,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-16 22:33:03,151 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-16 22:33:03,312 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-16 22:33:03,312 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-16 22:33:03,318 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-16 22:33:03,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-16 22:33:03,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-16 22:33:03,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-16 22:33:03,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-16 22:33:03,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-16 22:33:03,631 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-16 22:33:03,644 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-16 22:33:03,653 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-16 22:33:03,660 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-16 22:33:03,664 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-16 22:33:03,664 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-16 22:33:03,664 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-16 22:33:03,679 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 49651
2015-11-16 22:33:03,679 INFO org.mortbay.log: jetty-6.1.26
2015-11-16 22:33:04,010 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:49651
2015-11-16 22:33:04,372 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-16 22:33:04,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-16 22:33:04,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-16 22:33:04,730 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-16 22:33:04,748 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-16 22:33:04,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-16 22:33:04,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-16 22:33:05,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-16 22:33:05,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-16 22:33:05,137 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-16 22:33:05,137 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-16 22:33:05,586 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 2687@rushikesh2
2015-11-16 22:33:05,688 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-16 22:33:05,688 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-16 22:33:05,688 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-16 22:33:05,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-16 22:33:05,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-16 22:33:05,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-16 22:33:05,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-16 22:33:05,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-16 22:33:05,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-16 22:33:05,926 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 78ms
2015-11-16 22:33:05,926 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 78ms
2015-11-16 22:33:05,927 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-16 22:33:05,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-11-16 22:33:05,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-11-16 22:33:06,293 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data, after more than 504 hour(s)
2015-11-16 22:33:06,295 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447714313294 with interval 21600000
2015-11-16 22:33:06,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-16 22:33:06,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-16 22:33:06,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-16 22:33:06,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=129
2015-11-16 22:33:06,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-16 22:33:06,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x24cd262546,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 35 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-16 22:33:06,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-16 22:37:44,050 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-16 22:37:46,771 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-16 22:37:46,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-16 22:38:24,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-16 22:38:24,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-16 22:38:24,849 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-16 22:38:24,912 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-16 22:38:24,912 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-16 22:38:24,917 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-16 22:38:24,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-16 22:38:24,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-16 22:38:24,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-16 22:38:24,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-16 22:38:24,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-16 22:38:25,035 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-16 22:38:25,043 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-16 22:38:25,048 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-16 22:38:25,053 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-16 22:38:25,055 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-16 22:38:25,055 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-16 22:38:25,055 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-16 22:38:25,065 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 55452
2015-11-16 22:38:25,065 INFO org.mortbay.log: jetty-6.1.26
2015-11-16 22:38:25,218 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:55452
2015-11-16 22:38:25,300 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-16 22:38:25,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-16 22:38:25,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-16 22:38:25,339 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-16 22:38:25,350 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-16 22:38:25,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-16 22:38:25,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-16 22:38:25,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-16 22:38:25,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-16 22:38:25,429 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-16 22:38:25,429 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-16 22:38:25,703 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3852@rushikesh2
2015-11-16 22:38:25,785 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-16 22:38:25,785 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-16 22:38:25,785 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-16 22:38:25,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-16 22:38:25,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-16 22:38:25,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-16 22:38:25,884 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-16 22:38:25,884 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-16 22:38:25,885 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-16 22:38:25,898 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278749184
2015-11-16 22:38:25,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 14ms
2015-11-16 22:38:25,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 15ms
2015-11-16 22:38:25,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-16 22:38:25,902 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-11-16 22:38:25,902 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2015-11-16 22:38:26,059 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data, after more than 504 hour(s)
2015-11-16 22:38:26,061 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447702337061 with interval 21600000
2015-11-16 22:38:26,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-16 22:38:26,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-16 22:38:26,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-16 22:38:26,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=132
2015-11-16 22:38:26,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-16 22:38:26,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6f3a0e0053,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 5 msec to generate and 57 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-16 22:38:26,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-16 22:38:58,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741830_1006 to 192.168.6.237:50010 
2015-11-16 22:38:59,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741830_1006 (numBytes=4045946) to /192.168.6.237:50010
2015-11-16 22:42:52,351 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): finished scanning block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-16 22:42:52,364 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1814133695 ms.
2015-11-17 01:02:17,069 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 4, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-17 02:55:40,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe78cb3d6377,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-17 02:55:40,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-17 03:02:10,423 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-17 03:02:33,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); maxRetries=45
2015-11-17 03:02:53,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); maxRetries=45
2015-11-17 03:02:57,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:00,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:03,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:06,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:09,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:12,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:15,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:18,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:21,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:24,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:26,443 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:03:30,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:33,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:36,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:39,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:42,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:45,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:48,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:51,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:54,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:57,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:03:59,443 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:04:03,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:06,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:09,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:12,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:15,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:18,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:21,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:24,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:27,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:30,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:32,443 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:04:36,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:39,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:42,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:45,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:48,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:51,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:54,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:04:57,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:00,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:03,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:05,443 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:05:09,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:12,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:15,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:18,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:21,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:24,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:27,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:30,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:33,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:36,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:38,443 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:05:42,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:45,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:48,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:51,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:54,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:05:57,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:00,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:03,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:06,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:09,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:11,443 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:06:15,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:18,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:21,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:24,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:27,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:30,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:33,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:36,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:39,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:42,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:44,443 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:06:48,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:51,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:54,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:06:57,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:00,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:03,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:06,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:09,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:12,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:15,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:17,443 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:07:21,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:24,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:27,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:30,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:33,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:36,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:39,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:42,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:45,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:48,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:50,443 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:07:54,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:07:57,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:00,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:03,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:06,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:09,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:12,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:15,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:18,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:21,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:23,443 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:08:27,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:30,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:33,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:36,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:39,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:42,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:45,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:48,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:51,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:54,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:08:56,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:09:00,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:03,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:06,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:09,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:12,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:15,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:18,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:21,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:24,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:27,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:29,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:09:33,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:36,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:39,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:42,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:45,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:48,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:51,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:54,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:09:57,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:00,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:02,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:10:06,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:09,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:12,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:15,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:18,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:21,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:24,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:27,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:30,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:33,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:35,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:10:39,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:42,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:45,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:48,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:51,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:54,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:10:57,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:00,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:03,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:06,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:08,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:11:12,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:15,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:18,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:21,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:24,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:27,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:30,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:33,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:36,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:39,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:41,448 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:11:45,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:48,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:51,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:54,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:11:57,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:00,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:03,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:06,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:09,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:12,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:14,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:12:18,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:21,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:24,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:27,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:30,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:33,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:36,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:39,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:42,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:45,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:47,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:12:51,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:54,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:12:57,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:00,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:03,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:06,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:09,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:12,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:15,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:18,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:20,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:13:24,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:27,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:30,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:33,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:36,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:39,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:42,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:45,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:48,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:51,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:13:53,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:13:57,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:00,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:03,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:06,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:09,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:12,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:15,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:18,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:21,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:24,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:26,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:14:30,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:33,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:36,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:39,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:42,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:45,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:48,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:51,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:54,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:57,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:14:59,447 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:15:03,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:06,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:09,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:12,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:15,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:18,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:21,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:24,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:27,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:30,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:32,451 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:15:36,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:39,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:42,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:45,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:48,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:51,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:54,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:15:57,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:00,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:03,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:05,451 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:16:09,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:12,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:15,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:18,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:21,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:24,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:27,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:30,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:33,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:36,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:38,451 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:16:42,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:45,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:48,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:51,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:54,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:16:57,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:00,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:03,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:06,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:09,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:11,451 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:17:15,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:18,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:21,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:24,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:27,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:30,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:33,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:36,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:39,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:42,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:44,451 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:17:48,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:51,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:54,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:17:57,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:00,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:03,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:06,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:09,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:12,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:15,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:17,451 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:18:21,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:24,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:27,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:30,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:33,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:36,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:39,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:42,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:45,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:48,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:50,451 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:18:54,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:18:57,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:00,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:03,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:06,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:09,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:12,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:15,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:18,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:21,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:23,455 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:19:27,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:30,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:33,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:36,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:39,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:42,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:45,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:48,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:51,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:54,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:19:56,455 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:20:00,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:03,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:06,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:09,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:12,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:15,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:18,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:21,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:24,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:27,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:29,455 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:20:33,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:36,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:39,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:42,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:45,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:48,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:51,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:54,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:20:57,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:00,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:02,455 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:21:06,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:09,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:12,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:15,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:18,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:21,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:24,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:27,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:30,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:33,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:35,455 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:21:39,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:42,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:45,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:48,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:51,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:54,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:21:57,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:00,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:03,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:06,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:08,455 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:22:12,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:15,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:18,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:21,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:24,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:27,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:30,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:33,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:36,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:39,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:41,455 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:22:45,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:48,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:51,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:54,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:22:57,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:00,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:03,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:06,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:09,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:12,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:14,455 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:23:18,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:21,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:24,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:27,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:30,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:33,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:36,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:39,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:42,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:45,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:47,455 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:23:51,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:54,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:23:57,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:00,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:03,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:06,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:09,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:12,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:15,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:18,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:20,455 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:24:24,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:27,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:30,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:33,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:36,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:39,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:42,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:45,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:48,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:51,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:24:53,459 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:24:57,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:00,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:03,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:06,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:09,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:12,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:15,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:18,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:21,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:24,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:26,459 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:25:30,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:33,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:36,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:39,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:42,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:45,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:48,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:51,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:54,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:57,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:25:59,459 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:26:03,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:06,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:09,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:12,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:15,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:18,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:21,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:24,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:27,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:30,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:32,459 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:26:36,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:39,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:42,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:45,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:48,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:51,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:54,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:26:57,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:00,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:03,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:05,463 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:27:09,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:12,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:15,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:18,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:21,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:24,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:27,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:30,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:33,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:36,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:38,463 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:27:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:45,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:48,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:51,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:54,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:27:57,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:00,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:03,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:06,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:09,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:11,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:28:15,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:18,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:21,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:24,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:27,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:30,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:33,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:36,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:39,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:44,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:28:48,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:51,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:54,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:28:57,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:00,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:03,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:06,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:09,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:12,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:15,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:17,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:29:21,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:24,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:27,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:30,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:33,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:36,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:39,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:45,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:48,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:50,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:29:54,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:29:57,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:00,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:03,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:06,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:09,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:12,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:15,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:18,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:21,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:23,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:30:27,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:30,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:33,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:36,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:39,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:45,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:48,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:51,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:54,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:30:56,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:31:00,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:03,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:06,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:09,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:12,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:15,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:18,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:21,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:24,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:27,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:29,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:31:33,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:36,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:39,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:45,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:48,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:51,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:54,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:31:57,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:00,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:02,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:32:06,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:09,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:12,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:15,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:18,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:21,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:24,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:27,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:30,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:33,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:35,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:32:39,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:45,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:48,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:51,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:54,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:32:57,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:00,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:03,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:06,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:08,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:33:12,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:15,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:18,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:21,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:24,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:27,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:30,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:33,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:36,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:39,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:41,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:33:45,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:48,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:51,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:54,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:33:57,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:00,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:03,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:06,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:09,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:12,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:14,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:34:18,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:21,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:24,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:27,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:30,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:33,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:36,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:39,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:45,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:47,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:34:51,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:54,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:34:57,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:00,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:03,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:06,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:09,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:12,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:15,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:18,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:20,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:35:24,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:27,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:30,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:33,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:36,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:39,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:45,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:48,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:51,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:35:53,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:35:57,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:00,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:03,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:06,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:09,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:12,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:15,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:18,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:21,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:24,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:26,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:36:30,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:33,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:36,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:39,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:45,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:48,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:51,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:54,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:57,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:36:59,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:37:03,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:06,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:09,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:12,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:15,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:18,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:21,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:24,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:27,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:30,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:32,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:37:36,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:39,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:42,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:45,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:48,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:51,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:54,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:37:57,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:00,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:03,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:05,475 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:38:09,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:12,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:15,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:18,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:21,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:24,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:27,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:30,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:33,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:36,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:38,475 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:38:42,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:45,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:48,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:51,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:54,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:38:57,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:00,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:03,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:06,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:09,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:11,475 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:39:15,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:18,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:21,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:24,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:27,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:30,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:33,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:36,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:39,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:42,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:44,475 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:39:48,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:51,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:54,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:39:57,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:00,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:03,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:06,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:09,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:12,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:15,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:17,479 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:40:21,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:24,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:27,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:30,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:33,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:36,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:39,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:42,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:45,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:48,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:50,480 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:40:54,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:40:57,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:00,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:03,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:06,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:09,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:12,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:15,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:18,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:21,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:23,479 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:41:27,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:30,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:33,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:36,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:39,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:42,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:45,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:48,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:51,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:54,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:41:56,479 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:42:00,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:03,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:06,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:09,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:12,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:15,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:18,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:21,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:24,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:27,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:29,483 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:42:33,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:36,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:39,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:42,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:45,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:48,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:51,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:54,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:42:57,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:00,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:02,487 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:43:06,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:09,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:12,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:15,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:18,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:21,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:24,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:27,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:29,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:30,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:30,489 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:43:31,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:32,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:33,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:34,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:35,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:36,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:37,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:38,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:39,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:40,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:40,498 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:43:41,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:42,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:43,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:44,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:45,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:46,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:47,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:48,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:49,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:50,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:50,507 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:43:51,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:52,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:53,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:54,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:55,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:56,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:57,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:58,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:43:59,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:00,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:00,516 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:44:01,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:02,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:03,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:04,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:05,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:06,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:07,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:08,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:09,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:10,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:10,525 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:44:11,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:12,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:13,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:14,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:15,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:16,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:17,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:18,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:19,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:20,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:20,534 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:44:21,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:22,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:23,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:24,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:25,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:26,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:27,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:28,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:29,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:30,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:30,543 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:44:31,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:32,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:33,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:34,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:35,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:36,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:37,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:38,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:39,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:40,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:40,552 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:44:41,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:42,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:43,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:44,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:45,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:46,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:47,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:48,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:49,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:50,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:50,561 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:44:51,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:52,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:53,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:54,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:55,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:56,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:57,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:58,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:44:59,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:00,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:00,570 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:45:01,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:02,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:03,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:04,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:05,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:06,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:07,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:08,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:09,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:10,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:10,580 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:45:11,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:12,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:13,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:14,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:15,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:16,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:17,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:18,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:19,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:20,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:20,589 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:45:21,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:22,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:23,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:24,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:25,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:26,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:27,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:28,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:29,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:30,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:30,598 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:45:31,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:32,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:33,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:34,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:35,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:36,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:37,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:38,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:39,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:40,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:40,606 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:45:41,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:42,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:43,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:44,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:45,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:46,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:47,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:48,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:49,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:50,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:50,615 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:45:51,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:52,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:53,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:54,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:55,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:56,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:57,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:58,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:45:59,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:00,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:00,625 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:46:01,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:02,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:03,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:04,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:05,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:06,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:07,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:08,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:09,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:10,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:10,635 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:46:11,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:12,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:13,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:14,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:15,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:16,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:17,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:18,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:19,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:20,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:20,644 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:46:21,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:22,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:23,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:24,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:25,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:26,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:27,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:28,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:29,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:30,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:30,653 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:46:31,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:32,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:33,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:34,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:35,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:36,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:37,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:38,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:39,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:40,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:40,662 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:46:41,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:42,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:43,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:44,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:45,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:46,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:47,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:48,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:49,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:50,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:50,671 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:46:51,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:52,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:53,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:54,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:55,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:56,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:57,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:58,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:46:59,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:00,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:00,680 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:47:01,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:02,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:03,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:04,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:05,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:06,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:07,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:08,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:09,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:10,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:10,690 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:47:11,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:12,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:13,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:14,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:15,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:16,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:17,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:18,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:19,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:20,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:20,699 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:47:21,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:22,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:23,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:24,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:25,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:26,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:27,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:28,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:29,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:30,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:30,708 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:47:31,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:32,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:33,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:34,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:35,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:36,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:37,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:38,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:39,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:40,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:40,720 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:47:41,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:42,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:43,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:44,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:45,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:46,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:47,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:48,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:49,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:50,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:50,729 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:47:51,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:52,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:53,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:54,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:55,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:56,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:57,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:58,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:47:59,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:00,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:00,737 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:48:01,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:02,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:03,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:04,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:05,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:06,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:07,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:08,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:09,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:10,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:10,747 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:48:11,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:12,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:13,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:14,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:15,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:16,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:17,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:18,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:19,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:20,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:20,756 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:48:21,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:22,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:23,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:24,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:25,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:26,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:27,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:28,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:29,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:30,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:30,768 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:48:31,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:32,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:33,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:34,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:35,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:36,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:37,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:38,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:39,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:40,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:40,777 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:48:41,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:42,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:43,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:44,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:45,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:46,782 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:47,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:48,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:49,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:50,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:50,786 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:48:51,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:52,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:53,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:54,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:55,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:56,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:57,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:58,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:48:59,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:00,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:00,795 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:49:01,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:02,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:03,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:04,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:05,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:06,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:07,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:08,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:09,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:10,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:10,805 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:49:11,806 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:12,807 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:13,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:14,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:15,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:16,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:17,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:18,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:19,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:20,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:20,814 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:49:21,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:22,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:23,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:24,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:25,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:26,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:27,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:28,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:29,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:30,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:30,823 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:49:31,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:32,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:33,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:34,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:35,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:36,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:37,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:38,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:39,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:40,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:40,833 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:49:41,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:42,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:43,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:44,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:45,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:46,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:47,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:48,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:49,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:50,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:50,842 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:49:51,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:52,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:53,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:54,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:55,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:56,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:57,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:58,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:49:59,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:00,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:00,851 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:50:01,853 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:02,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:03,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:04,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:05,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:06,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:07,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:08,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:09,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:10,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:10,860 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:50:11,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:12,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:13,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:14,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:15,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:16,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:17,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:18,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:19,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:20,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:20,870 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:50:21,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:22,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:23,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:24,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:25,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:26,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:27,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:28,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:29,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:30,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:30,879 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:50:31,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:32,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:33,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:34,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:35,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:36,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:37,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:38,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:39,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:40,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:40,888 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:50:41,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:42,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:43,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:44,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:45,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:46,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:47,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:48,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:49,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:50,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:50,897 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:50:51,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:52,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:53,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:54,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:55,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:56,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:57,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:58,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:50:59,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:00,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:00,906 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:51:01,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:02,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:03,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:04,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:05,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:06,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:07,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:08,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:09,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:10,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:10,915 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:51:11,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:12,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:13,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:14,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:15,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:16,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:17,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:18,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:19,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:20,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:20,924 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:51:21,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:22,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:23,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:24,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:25,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:26,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:27,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:28,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:29,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:30,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:30,933 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:51:31,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:32,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:33,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:34,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:35,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:36,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:37,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:38,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:39,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:40,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:40,943 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:51:41,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:42,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:43,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:44,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:45,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:46,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:47,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:48,950 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:49,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:50,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:50,952 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:51:51,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:52,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:53,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:54,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:55,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:56,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:57,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:58,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:51:59,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:00,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:00,961 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:52:01,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:02,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:03,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:04,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:05,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:06,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:07,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:08,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:09,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:10,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:10,970 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:52:11,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:12,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:13,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:14,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:15,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:16,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:17,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:18,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:19,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:20,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:20,979 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:52:21,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:22,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:23,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:24,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:25,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:26,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:27,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:28,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:29,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:30,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:30,988 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:52:31,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:32,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:33,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:34,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:35,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:36,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:37,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:38,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:39,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:40,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:40,998 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:52:41,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:43,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:44,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:45,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:46,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:47,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:48,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:49,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:50,005 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:51,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:51,007 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:52:52,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:53,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:54,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:55,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:56,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:57,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:58,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:52:59,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:00,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:01,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:01,016 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:53:02,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:03,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:04,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:05,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:06,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:07,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:08,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:09,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:10,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:11,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:11,025 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:53:12,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:13,027 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:14,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:15,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:16,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:17,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:18,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:19,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:20,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:21,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:21,034 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:53:22,036 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:23,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:24,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:25,038 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:26,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:27,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:28,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:29,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:30,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:31,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:31,045 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:53:32,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:33,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:34,048 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:35,049 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:36,049 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:37,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:38,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:39,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:40,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:41,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:41,054 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:53:42,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:43,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:44,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:45,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:46,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:47,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:48,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:49,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:50,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:51,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:51,063 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:53:52,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:53,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:54,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:55,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:56,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:57,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:58,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:53:59,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:00,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:01,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:01,075 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:54:02,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:03,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:04,078 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:05,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:06,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:07,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:08,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:09,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:10,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:11,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:11,084 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:54:12,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:13,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:14,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:15,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:16,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:17,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:18,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:19,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:20,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:21,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:21,093 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:54:22,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:23,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:24,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:25,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:26,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:27,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:28,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:29,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:30,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:31,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:31,102 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:54:32,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:33,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:34,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:35,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:36,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:37,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:38,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:39,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:40,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:41,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:41,111 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:54:42,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:43,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:44,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:45,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:46,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:47,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:48,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:49,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:50,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:51,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:51,121 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:54:52,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:53,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:54,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:55,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:56,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:57,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:58,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:54:59,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:00,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:01,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:01,130 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:55:02,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:03,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:04,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:05,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:06,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:07,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:08,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:09,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:10,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:11,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:11,139 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:55:12,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:13,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:14,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:15,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:16,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:17,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:18,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:19,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:20,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:21,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:21,214 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:55:22,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:23,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:24,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:25,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:26,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:27,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:28,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:29,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:30,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:31,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:31,224 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:55:32,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:33,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:34,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:35,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:36,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:37,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:38,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:39,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:40,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:41,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:41,232 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:55:42,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:43,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:44,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:45,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:46,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:47,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:48,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:49,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:50,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:51,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:51,241 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:55:52,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:53,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:54,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:55,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:56,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:57,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:58,247 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:55:59,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:00,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:01,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:01,250 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:56:02,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:03,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:04,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:05,254 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:06,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:07,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:08,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:09,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:10,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:11,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:11,259 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:56:12,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:13,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:14,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:15,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:16,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:17,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:18,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:19,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:20,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:21,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:21,268 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:56:22,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:23,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:24,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:25,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:26,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:27,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:28,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:29,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:30,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:31,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:31,277 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:56:32,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:33,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:34,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:35,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:36,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:37,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:38,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:39,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:40,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:41,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:41,286 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:56:42,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:43,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:44,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:45,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:46,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:47,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:48,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:49,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:50,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:51,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:51,295 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:56:52,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:53,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:54,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:55,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:56,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:57,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:58,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:56:59,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:00,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:01,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:01,305 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:57:02,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:03,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:04,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:05,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:06,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:07,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:08,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:09,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:10,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:11,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:11,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:57:12,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:13,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:14,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:15,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:16,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:17,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:18,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:19,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:20,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:21,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:21,323 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor10.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-17 03:57:22,324 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:23,325 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:24,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:25,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:26,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:27,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-17 03:57:47,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); maxRetries=45
2015-11-17 03:57:49,822 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-17 03:57:49,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-18 01:20:27,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-18 01:20:27,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-18 01:20:29,130 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-18 01:20:29,288 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-18 01:20:29,289 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-18 01:20:29,296 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-18 01:20:29,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-18 01:20:29,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-18 01:20:29,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-18 01:20:29,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-18 01:20:29,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-18 01:20:29,602 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-18 01:20:29,631 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-18 01:20:29,656 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-18 01:20:29,663 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-18 01:20:29,666 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-18 01:20:29,667 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-18 01:20:29,667 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-18 01:20:29,702 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 56118
2015-11-18 01:20:29,702 INFO org.mortbay.log: jetty-6.1.26
2015-11-18 01:20:30,016 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:56118
2015-11-18 01:20:30,172 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-18 01:20:30,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-18 01:20:30,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-18 01:20:30,398 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-18 01:20:30,444 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-18 01:20:30,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-18 01:20:30,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-18 01:20:30,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-18 01:20:30,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-18 01:20:30,802 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-18 01:20:30,803 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-18 01:20:31,204 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 13177@rushikesh2
2015-11-18 01:20:31,330 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-18 01:20:31,330 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-18 01:20:31,331 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-18 01:20:31,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-18 01:20:31,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-18 01:20:31,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-18 01:20:31,497 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-18 01:20:31,497 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 01:20:31,498 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 01:20:31,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 58ms
2015-11-18 01:20:31,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 59ms
2015-11-18 01:20:31,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 01:20:31,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-11-18 01:20:31,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-11-18 01:20:31,923 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1718274136 ms.
2015-11-18 01:20:31,925 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447810727925 with interval 21600000
2015-11-18 01:20:31,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-18 01:20:31,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-18 01:20:31,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-18 01:20:32,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=143
2015-11-18 01:20:32,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-18 01:20:32,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x63dd1b27899,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 298 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-18 01:20:32,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 01:21:57,706 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-18 01:22:01,155 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-18 01:22:01,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-18 01:59:28,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-18 01:59:28,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-18 01:59:29,573 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-18 01:59:29,636 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-18 01:59:29,636 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-18 01:59:29,641 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-18 01:59:29,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-18 01:59:29,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-18 01:59:29,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-18 01:59:29,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-18 01:59:29,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-18 01:59:29,761 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-18 01:59:29,768 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-18 01:59:29,773 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-18 01:59:29,778 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-18 01:59:29,781 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-18 01:59:29,781 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-18 01:59:29,781 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-18 01:59:29,791 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 57752
2015-11-18 01:59:29,791 INFO org.mortbay.log: jetty-6.1.26
2015-11-18 01:59:29,944 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:57752
2015-11-18 01:59:30,025 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-18 01:59:30,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-18 01:59:30,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-18 01:59:30,064 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-18 01:59:30,075 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-18 01:59:30,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-18 01:59:30,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-18 01:59:30,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-18 01:59:30,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-18 01:59:30,154 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-18 01:59:30,154 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-18 01:59:30,451 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 17300@rushikesh2
2015-11-18 01:59:30,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-18 01:59:30,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-18 01:59:30,526 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-18 01:59:30,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-18 01:59:30,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-18 01:59:30,591 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-18 01:59:30,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-18 01:59:30,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 01:59:30,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 01:59:30,638 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 12ms
2015-11-18 01:59:30,638 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 13ms
2015-11-18 01:59:30,638 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 01:59:30,640 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-11-18 01:59:30,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-18 01:59:30,798 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1715935261 ms.
2015-11-18 01:59:30,799 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447803933799 with interval 21600000
2015-11-18 01:59:30,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-18 01:59:30,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-18 01:59:30,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-18 01:59:30,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=146
2015-11-18 01:59:30,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-18 01:59:30,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x85e632c93c1,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 5 msec to generate and 57 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-18 01:59:30,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 02:36:06,149 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-18 02:36:08,574 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-18 02:36:08,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-18 02:37:04,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-18 02:37:04,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-18 02:37:04,831 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-18 02:37:04,894 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-18 02:37:04,894 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-18 02:37:04,899 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-18 02:37:04,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-18 02:37:04,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-18 02:37:04,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-18 02:37:04,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-18 02:37:04,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-18 02:37:05,017 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-18 02:37:05,025 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-18 02:37:05,030 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-18 02:37:05,035 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-18 02:37:05,038 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-18 02:37:05,038 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-18 02:37:05,038 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-18 02:37:05,048 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 54321
2015-11-18 02:37:05,048 INFO org.mortbay.log: jetty-6.1.26
2015-11-18 02:37:05,203 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:54321
2015-11-18 02:37:05,284 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-18 02:37:05,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-18 02:37:05,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-18 02:37:05,324 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-18 02:37:05,335 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-18 02:37:05,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-18 02:37:05,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-18 02:37:05,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-18 02:37:05,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-18 02:37:05,414 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-18 02:37:05,414 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-18 02:37:05,752 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 21303@rushikesh2
2015-11-18 02:37:05,827 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-18 02:37:05,827 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-18 02:37:05,828 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-18 02:37:05,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-18 02:37:05,892 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-18 02:37:05,892 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-18 02:37:05,926 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-18 02:37:05,926 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 02:37:05,927 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 02:37:05,934 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278749184
2015-11-18 02:37:05,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-18 02:37:05,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-18 02:37:05,936 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-18 02:37:05,938 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-18 02:37:05,938 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-18 02:37:06,096 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1713679963 ms.
2015-11-18 02:37:06,098 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447811245098 with interval 21600000
2015-11-18 02:37:06,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-18 02:37:06,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-18 02:37:06,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-18 02:37:06,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=149
2015-11-18 02:37:06,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-18 02:37:06,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa6b7c753b87,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 5 msec to generate and 61 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-18 02:37:06,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-18 02:37:38,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 to 192.168.6.238:50010 
2015-11-18 02:38:02,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741827_1003 (numBytes=134217728) to /192.168.6.238:50010
2015-11-18 02:38:29,408 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-18 02:38:32,779 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-18 02:38:32,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-19 00:21:43,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-19 00:21:43,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-19 00:21:45,053 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-19 00:21:45,165 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-19 00:21:45,165 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-19 00:21:45,171 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-19 00:21:45,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-19 00:21:45,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-19 00:21:45,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-19 00:21:45,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-19 00:21:45,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-19 00:21:45,563 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-19 00:21:45,577 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-19 00:21:45,586 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-19 00:21:45,594 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-19 00:21:45,597 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-19 00:21:45,597 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-19 00:21:45,598 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-19 00:21:45,640 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34513
2015-11-19 00:21:45,640 INFO org.mortbay.log: jetty-6.1.26
2015-11-19 00:21:45,984 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34513
2015-11-19 00:21:46,167 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-19 00:21:46,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-19 00:21:46,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-19 00:21:46,398 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-19 00:21:46,435 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-19 00:21:46,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-19 00:21:46,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-19 00:21:46,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-19 00:21:46,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-19 00:21:46,672 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-19 00:21:46,672 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-19 00:21:47,101 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3761@rushikesh2
2015-11-19 00:21:47,188 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-19 00:21:47,188 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-19 00:21:47,188 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-19 00:21:47,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-19 00:21:47,296 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-19 00:21:47,296 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-19 00:21:47,347 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-19 00:21:47,347 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 00:21:47,348 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 00:21:47,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 60ms
2015-11-19 00:21:47,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 61ms
2015-11-19 00:21:47,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 00:21:47,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-19 00:21:47,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-11-19 00:21:47,729 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1635398330 ms.
2015-11-19 00:21:47,731 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447884761731 with interval 21600000
2015-11-19 00:21:47,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 00:21:47,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 00:21:47,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-19 00:21:47,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=152
2015-11-19 00:21:47,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-19 00:21:47,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x93ef82039e,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 32 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 00:21:47,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 00:27:40,633 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-19 00:27:44,457 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-19 00:27:44,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-19 00:32:53,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-19 00:32:53,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-19 00:32:54,024 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-19 00:32:54,086 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-19 00:32:54,086 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-19 00:32:54,092 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-19 00:32:54,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-19 00:32:54,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-19 00:32:54,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-19 00:32:54,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-19 00:32:54,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-19 00:32:54,211 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-19 00:32:54,218 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-19 00:32:54,224 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-19 00:32:54,228 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-19 00:32:54,231 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-19 00:32:54,231 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-19 00:32:54,231 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-19 00:32:54,241 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34740
2015-11-19 00:32:54,241 INFO org.mortbay.log: jetty-6.1.26
2015-11-19 00:32:54,393 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34740
2015-11-19 00:32:54,475 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-19 00:32:54,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-19 00:32:54,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-19 00:32:54,514 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-19 00:32:54,525 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-19 00:32:54,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-19 00:32:54,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-19 00:32:54,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-19 00:32:54,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-19 00:32:54,604 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-19 00:32:54,605 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-19 00:32:54,923 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 5453@rushikesh2
2015-11-19 00:32:54,997 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-19 00:32:54,998 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-19 00:32:54,998 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-19 00:32:55,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-19 00:32:55,063 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-19 00:32:55,063 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-19 00:32:55,097 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-19 00:32:55,097 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 00:32:55,098 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 00:32:55,105 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278749184
2015-11-19 00:32:55,106 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-19 00:32:55,106 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-11-19 00:32:55,106 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 00:32:55,109 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-19 00:32:55,109 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-19 00:32:55,266 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1634730793 ms.
2015-11-19 00:32:55,268 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447887964268 with interval 21600000
2015-11-19 00:32:55,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 00:32:55,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 00:32:55,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-19 00:32:55,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=153
2015-11-19 00:32:55,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-19 00:32:55,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x12f5c046706,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 4 msec to generate and 47 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 00:32:55,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 00:54:42,652 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.SocketTimeoutException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.6.249:39202 remote=rushikesh1/192.168.6.248:54310]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.6.249:39202 remote=rushikesh1/192.168.6.248:54310]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-19 00:54:46,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:54:49,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:54:52,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:54:55,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:54:58,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:01,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:04,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:07,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:10,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:13,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:15,076 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 00:55:19,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:22,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:25,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:28,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:31,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:34,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:40,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:43,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:46,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:49,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:51,348 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 00:55:55,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:55:58,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:01,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:04,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:07,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:10,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:13,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:16,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:19,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:22,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:24,348 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 00:56:28,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:31,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:34,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:37,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:40,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:43,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:46,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:49,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:52,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:55,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:56:57,352 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 00:57:01,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:04,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:07,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:10,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:13,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:16,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:19,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:22,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:25,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:28,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:30,352 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 00:57:34,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:37,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:40,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:45,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:48,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:51,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:54,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:57:57,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:00,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:03,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:05,352 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 00:58:09,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:12,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:15,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:18,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:21,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:24,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:27,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:30,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:33,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:36,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:38,352 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 00:58:42,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:45,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:48,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:51,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:54,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:58:57,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:00,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:03,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:06,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:09,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:11,352 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 00:59:15,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:18,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:21,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:24,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:27,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:30,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:33,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:36,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:39,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:42,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:44,352 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 00:59:48,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:51,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:54,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 00:59:57,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:00,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:03,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:06,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:09,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:12,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:15,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:17,352 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:00:21,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:24,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:27,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:30,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:33,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:36,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:39,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:42,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:45,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:48,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:50,352 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:00:54,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:00:57,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:00,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:03,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:06,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:09,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:12,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:15,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:18,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:21,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:41,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); maxRetries=45
2015-11-19 01:01:41,373 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:01:42,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:43,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:44,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:45,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:46,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:47,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:48,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:49,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:50,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:51,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:51,378 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:01:52,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:53,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:54,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:55,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:56,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:57,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:58,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:01:59,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:00,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:01,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:01,384 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:02:02,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:03,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:04,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:05,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:06,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:07,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:08,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:09,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:10,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:11,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:11,390 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:02:12,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:13,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:14,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:15,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:16,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:17,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:18,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:19,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:20,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:21,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:21,395 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:02:22,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:23,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:24,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:25,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:26,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:27,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:28,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:29,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:30,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:31,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:31,401 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:02:32,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:33,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:34,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:35,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:36,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:37,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:38,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:39,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:40,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:41,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:41,406 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:02:42,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:43,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:44,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:45,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:46,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:47,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:48,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:49,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:50,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:51,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:51,412 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:02:52,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:53,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:54,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:55,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:56,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:57,416 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:58,416 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:02:59,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:00,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:01,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:01,418 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:03:02,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:03,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:04,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:05,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:06,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:07,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:08,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:09,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:10,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:11,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:11,424 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:03:12,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:13,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:14,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:15,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:16,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:17,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:18,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:19,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:20,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:21,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:21,430 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:03:22,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:23,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:24,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:25,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:26,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:27,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:28,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:29,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:30,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:31,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:31,435 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:03:32,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:33,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:34,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:35,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:36,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:37,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:38,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:39,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:40,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:41,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:41,441 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:03:42,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:43,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:44,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:45,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:46,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:47,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:48,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:49,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:50,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:51,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:51,446 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:03:52,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:53,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:54,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:55,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:56,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:57,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:58,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:03:59,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:00,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:01,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:01,451 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:04:02,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:03,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:04,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:05,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:06,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:07,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:08,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:09,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:10,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:11,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:11,457 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:04:12,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:13,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:14,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:15,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:16,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:17,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:18,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:19,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:20,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:21,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:21,462 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:04:22,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:23,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:24,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:25,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:26,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:27,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:28,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:29,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:30,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:31,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:31,467 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:04:32,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:33,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:34,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:35,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:36,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:37,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:38,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:39,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:40,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:41,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:41,472 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:04:42,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:43,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:44,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:45,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:46,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:47,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:48,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:49,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:50,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:51,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:51,478 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:04:52,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:53,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:54,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:55,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:56,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:57,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:58,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:04:59,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:00,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:01,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:01,484 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:05:02,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:03,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:04,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:05,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:06,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:07,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:08,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:09,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:10,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:11,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:11,489 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:05:12,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:13,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:14,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:15,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:16,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:17,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:18,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:19,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:20,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:21,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:21,495 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:05:22,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:23,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:24,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:25,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:26,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:27,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:28,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:29,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:30,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:31,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:31,500 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:05:32,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:33,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:34,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:35,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:36,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:37,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:38,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:39,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:40,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:41,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:41,505 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:05:42,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:43,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:44,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:45,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:46,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:47,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:48,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:49,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:50,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:51,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:51,511 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:05:52,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:53,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:54,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:55,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:56,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:57,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:58,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:05:59,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:00,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:01,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:01,516 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:06:02,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:03,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:04,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:05,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:06,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:07,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:08,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:09,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:10,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:11,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:11,522 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:06:12,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:13,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:14,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:15,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:16,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:17,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:18,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:19,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:20,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:21,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:21,527 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:06:22,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:23,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:24,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:25,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:26,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:27,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:28,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:29,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:30,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:31,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:31,533 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:06:32,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:33,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:34,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:35,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:36,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:37,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:38,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:39,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:40,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:41,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:41,539 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:06:42,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:43,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:44,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:45,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:46,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:47,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:48,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:49,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:50,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:51,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:51,544 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:06:52,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:53,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:54,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:55,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:56,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:57,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:58,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:06:59,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:00,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:01,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:01,550 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:07:02,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:03,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:04,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:05,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:06,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:07,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:08,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:09,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:10,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:11,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:11,555 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:07:12,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:13,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:14,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:15,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:16,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:17,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:18,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:19,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:20,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:21,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:21,561 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:07:22,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:23,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:24,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:25,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:26,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:27,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:28,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:29,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:30,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:31,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:31,566 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:07:32,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:33,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:34,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:35,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:36,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:37,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:38,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:39,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:40,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:41,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:41,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:07:42,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:43,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:44,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:45,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:46,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:47,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:48,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:49,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:50,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:51,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:51,577 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:07:52,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:53,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:54,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:55,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:56,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:57,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:58,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:07:59,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:00,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:01,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:01,582 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:08:02,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:03,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:04,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:05,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:06,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:07,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:08,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:09,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:10,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:11,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:11,588 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:08:12,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:13,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:14,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:15,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:16,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:17,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:18,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:19,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:20,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:21,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:21,593 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:08:22,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:23,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:24,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:25,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:26,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:27,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:28,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:29,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:30,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:31,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:31,599 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.net.SocketException: Network is unreachable; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketException: Network is unreachable
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:466)
	at sun.nio.ch.Net.connect(Net.java:458)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:671)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:08:32,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:36,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:39,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:42,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:45,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:48,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:51,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:54,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:08:57,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:00,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:02,600 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.NoRouteToHostException: No Route to Host from  rushikesh2/192.168.6.249 to rushikesh1:54310 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:09:03,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:04,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:05,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:06,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:07,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:08,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:09,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:10,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:11,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:12,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:12,609 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:09:13,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:14,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:15,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:16,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:17,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:18,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:19,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:20,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:21,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:22,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:22,617 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:09:23,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:24,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:25,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:26,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:27,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:28,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:29,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:30,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:31,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:32,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:32,626 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:09:33,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:34,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:35,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:36,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:37,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:38,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:39,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:40,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:41,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:42,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:42,635 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:09:43,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:44,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:45,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:46,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:47,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:48,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:49,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:50,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:51,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:52,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:52,644 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:09:53,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:54,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:55,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:56,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:57,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:58,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:09:59,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:00,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:01,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:02,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:02,653 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:10:03,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:04,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:05,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:06,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:07,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:08,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:09,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:10,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:11,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:12,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:12,662 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:10:13,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:14,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:15,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:16,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:17,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:18,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:19,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:20,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:21,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:22,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:22,671 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:10:23,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:24,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:25,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:26,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:27,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:28,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:29,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:30,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:31,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:32,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:32,680 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:10:33,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:34,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:35,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:36,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:37,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:38,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:39,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:40,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:41,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:42,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:42,689 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:10:43,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:44,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:45,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:46,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:47,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:48,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:49,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:50,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:51,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:52,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:52,698 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:10:53,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:54,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:55,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:56,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:57,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:58,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:10:59,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:00,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:01,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:02,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:02,710 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:11:03,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:04,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:05,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:06,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:07,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:08,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:09,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:10,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:11,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:12,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:12,719 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:11:13,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:14,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:15,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:16,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:17,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:18,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:19,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:20,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:21,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:22,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:22,728 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:11:23,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:24,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:25,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:26,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:27,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:28,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:29,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:30,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:31,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:32,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:32,736 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:11:33,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:34,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:35,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:36,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:37,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:38,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:39,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:40,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:41,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:42,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:42,746 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:11:43,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:44,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:45,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:46,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:47,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:48,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:49,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:50,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:51,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:52,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:52,754 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:11:53,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:54,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:55,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:56,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:57,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:58,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:11:59,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:00,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:01,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:02,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:02,769 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:12:03,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:04,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:05,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:06,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:07,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:08,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:09,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:10,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:11,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:12,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:12,778 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:12:13,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:14,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:15,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:16,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:17,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:18,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:19,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:20,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:21,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:22,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:22,790 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:12:23,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:24,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:25,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:26,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:27,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:28,795 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:29,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:30,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:31,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:32,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:32,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:12:33,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:34,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:35,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:36,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:37,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:38,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:39,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:40,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:41,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:42,806 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:42,807 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:12:43,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:44,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:45,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:46,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:47,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:48,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:49,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:50,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:51,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:52,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:52,816 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:12:53,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:54,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:55,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:56,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:57,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:58,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:12:59,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:13:00,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:13:01,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:13:02,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:13:02,824 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From rushikesh2/192.168.6.249 to rushikesh1:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 8 more
2015-11-19 01:13:03,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:13:04,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:13:05,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from rushikesh1/192.168.6.248:54310 with active state
2015-11-19 01:13:05,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 01:13:05,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 01:13:05,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x36067e5037d,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 27 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 01:13:05,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 01:15:56,049 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-19 01:16:00,049 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 01:16:00,776 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-19 01:16:00,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-19 02:14:17,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-19 02:14:17,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-19 02:14:18,226 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-19 02:14:18,289 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-19 02:14:18,289 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-19 02:14:18,294 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-19 02:14:18,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-19 02:14:18,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-19 02:14:18,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-19 02:14:18,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-19 02:14:18,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-19 02:14:18,412 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-19 02:14:18,420 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-19 02:14:18,425 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-19 02:14:18,430 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-19 02:14:18,432 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-19 02:14:18,432 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-19 02:14:18,432 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-19 02:14:18,442 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 56458
2015-11-19 02:14:18,442 INFO org.mortbay.log: jetty-6.1.26
2015-11-19 02:14:18,595 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:56458
2015-11-19 02:14:18,676 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-19 02:14:18,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-19 02:14:18,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-19 02:14:18,716 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-19 02:14:18,727 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-19 02:14:18,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-19 02:14:18,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-19 02:14:18,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-19 02:14:18,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-19 02:14:18,806 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-19 02:14:18,806 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-19 02:14:19,032 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 23984@rushikesh2
2015-11-19 02:14:19,105 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-19 02:14:19,105 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-19 02:14:19,106 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-19 02:14:19,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-19 02:14:19,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-19 02:14:19,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-19 02:14:19,213 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-19 02:14:19,213 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 02:14:19,214 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 02:14:19,227 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 13ms
2015-11-19 02:14:19,227 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 14ms
2015-11-19 02:14:19,228 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 02:14:19,231 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-19 02:14:19,231 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-19 02:14:19,387 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1628646672 ms.
2015-11-19 02:14:19,389 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447892481389 with interval 21600000
2015-11-19 02:14:19,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 02:14:19,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 02:14:19,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-19 02:14:19,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=157
2015-11-19 02:14:19,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-19 02:14:19,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6b7ea7cb687,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 2 msec to generate and 19 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 02:14:19,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 04:25:45,800 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-19 04:25:49,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 04:25:50,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 04:25:51,562 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-19 04:25:51,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-19 04:26:36,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-19 04:26:36,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-19 04:26:37,539 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-19 04:26:37,603 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-19 04:26:37,603 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-19 04:26:37,608 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-19 04:26:37,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-19 04:26:37,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-19 04:26:37,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-19 04:26:37,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-19 04:26:37,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-19 04:26:37,727 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-19 04:26:37,735 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-19 04:26:37,740 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-19 04:26:37,744 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-19 04:26:37,747 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-19 04:26:37,747 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-19 04:26:37,747 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-19 04:26:37,757 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 51983
2015-11-19 04:26:37,757 INFO org.mortbay.log: jetty-6.1.26
2015-11-19 04:26:37,910 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:51983
2015-11-19 04:26:37,995 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-19 04:26:38,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-19 04:26:38,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-19 04:26:38,035 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-19 04:26:38,046 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-19 04:26:38,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-19 04:26:38,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-19 04:26:38,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-19 04:26:38,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-19 04:26:38,126 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-19 04:26:38,127 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-19 04:26:38,355 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 24031@rushikesh2
2015-11-19 04:26:38,429 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-19 04:26:38,429 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-19 04:26:38,430 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-19 04:26:38,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-19 04:26:38,503 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-19 04:26:38,504 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-19 04:26:38,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-19 04:26:38,537 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 04:26:38,538 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 04:26:38,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278749184
2015-11-19 04:26:38,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 7ms
2015-11-19 04:26:38,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 8ms
2015-11-19 04:26:38,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 04:26:38,549 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-11-19 04:26:38,549 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2015-11-19 04:26:38,715 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1620707344 ms.
2015-11-19 04:26:38,717 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447894892717 with interval 21600000
2015-11-19 04:26:38,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 04:26:38,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 04:26:38,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-19 04:26:38,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=164
2015-11-19 04:26:38,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-19 04:26:38,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xdf06f781f2c,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 21 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 04:26:38,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 05:58:05,319 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-19 05:58:09,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 05:58:10,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 05:58:11,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-19 05:58:11,898 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-19 05:58:11,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-19 06:21:30,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-19 06:21:30,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-19 06:21:30,912 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-19 06:21:30,975 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-19 06:21:30,975 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-19 06:21:30,980 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-19 06:21:30,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-19 06:21:30,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-19 06:21:31,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-19 06:21:31,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-19 06:21:31,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-19 06:21:31,099 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-19 06:21:31,107 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-19 06:21:31,112 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-19 06:21:31,117 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-19 06:21:31,119 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-19 06:21:31,119 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-19 06:21:31,119 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-19 06:21:31,129 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 44104
2015-11-19 06:21:31,129 INFO org.mortbay.log: jetty-6.1.26
2015-11-19 06:21:31,288 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44104
2015-11-19 06:21:31,370 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-19 06:21:31,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-19 06:21:31,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-19 06:21:31,410 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-19 06:21:31,421 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-19 06:21:31,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-19 06:21:31,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-19 06:21:31,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-19 06:21:31,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-19 06:21:31,502 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-19 06:21:31,502 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-19 06:21:31,783 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3289@rushikesh2
2015-11-19 06:21:31,860 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-19 06:21:31,860 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-19 06:21:31,861 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-19 06:21:31,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-19 06:21:31,976 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-19 06:21:31,976 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-19 06:21:32,009 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-19 06:21:32,010 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 06:21:32,011 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 06:21:32,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 11ms
2015-11-19 06:21:32,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 12ms
2015-11-19 06:21:32,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-19 06:21:32,025 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-11-19 06:21:32,025 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-11-19 06:21:32,188 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1613813871 ms.
2015-11-19 06:21:32,190 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447905187190 with interval 21600000
2015-11-19 06:21:32,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-19 06:21:32,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-19 06:21:32,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-19 06:21:32,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=169
2015-11-19 06:21:32,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-19 06:21:32,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x143572520efc,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 2 msec to generate and 19 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 06:21:32,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 06:33:22,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x14dad0f7924b,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-19 06:33:22,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-19 06:34:52,616 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-19 06:35:06,830 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-19 06:35:06,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-20 00:19:18,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-20 00:19:19,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-20 00:19:20,622 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-20 00:19:20,766 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-20 00:19:20,767 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-20 00:19:20,772 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-20 00:19:20,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-20 00:19:20,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-20 00:19:20,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-20 00:19:20,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-20 00:19:20,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-20 00:19:21,060 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-20 00:19:21,073 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-20 00:19:21,094 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-20 00:19:21,101 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-20 00:19:21,104 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-20 00:19:21,104 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-20 00:19:21,104 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-20 00:19:21,143 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 44861
2015-11-20 00:19:21,143 INFO org.mortbay.log: jetty-6.1.26
2015-11-20 00:19:21,557 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44861
2015-11-20 00:19:21,810 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-20 00:19:21,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-20 00:19:21,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-20 00:19:22,305 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-20 00:19:22,361 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-20 00:19:22,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-20 00:19:22,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-20 00:19:22,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-20 00:19:22,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-20 00:19:23,025 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-20 00:19:23,026 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-20 00:19:23,579 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3559@rushikesh2
2015-11-20 00:19:23,681 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-20 00:19:23,681 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-20 00:19:23,682 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-20 00:19:23,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-20 00:19:23,778 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-20 00:19:23,778 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-20 00:19:23,828 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-20 00:19:23,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 00:19:23,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 00:19:23,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 51ms
2015-11-20 00:19:23,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 52ms
2015-11-20 00:19:23,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 00:19:23,886 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 4ms
2015-11-20 00:19:23,886 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-11-20 00:19:24,220 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1549141839 ms.
2015-11-20 00:19:24,223 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447965568223 with interval 21600000
2015-11-20 00:19:24,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-20 00:19:24,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-20 00:19:24,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-20 00:19:24,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=173
2015-11-20 00:19:24,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-20 00:19:24,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x36733dab1c,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 35 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-20 00:19:24,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 00:40:37,946 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-20 00:40:40,948 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-20 00:40:40,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-20 01:01:48,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-20 01:01:48,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-20 01:01:49,281 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-20 01:01:49,344 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-20 01:01:49,344 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-20 01:01:49,349 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-20 01:01:49,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-20 01:01:49,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-20 01:01:49,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-20 01:01:49,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-20 01:01:49,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-20 01:01:49,467 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-20 01:01:49,474 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-20 01:01:49,479 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-20 01:01:49,484 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-20 01:01:49,486 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-20 01:01:49,487 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-20 01:01:49,487 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-20 01:01:49,496 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 60950
2015-11-20 01:01:49,496 INFO org.mortbay.log: jetty-6.1.26
2015-11-20 01:01:49,648 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:60950
2015-11-20 01:01:49,729 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-20 01:01:49,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-20 01:01:49,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-20 01:01:49,768 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-20 01:01:49,779 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-20 01:01:49,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-20 01:01:49,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-20 01:01:49,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-20 01:01:49,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-20 01:01:49,858 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-20 01:01:49,858 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-20 01:01:50,189 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 8749@rushikesh2
2015-11-20 01:01:50,270 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-20 01:01:50,270 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-20 01:01:50,271 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-20 01:01:50,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-20 01:01:50,336 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-20 01:01:50,336 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-20 01:01:50,370 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-20 01:01:50,370 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 01:01:50,371 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 01:01:50,385 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 14ms
2015-11-20 01:01:50,385 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 15ms
2015-11-20 01:01:50,386 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 01:01:50,389 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-20 01:01:50,389 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-11-20 01:01:50,546 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1546595513 ms.
2015-11-20 01:01:50,548 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447976992548 with interval 21600000
2015-11-20 01:01:50,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-20 01:01:50,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-20 01:01:50,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-20 01:01:50,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=184
2015-11-20 01:01:50,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-20 01:01:50,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x28751d63f68,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 5 msec to generate and 52 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-20 01:01:50,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 01:35:13,853 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-20 01:35:17,853 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-20 01:35:18,466 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-20 01:35:18,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-20 01:36:01,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-20 01:36:01,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-20 01:36:02,159 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-20 01:36:02,222 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-20 01:36:02,222 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-20 01:36:02,227 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-20 01:36:02,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-20 01:36:02,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-20 01:36:02,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-20 01:36:02,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-20 01:36:02,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-20 01:36:02,344 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-20 01:36:02,352 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-20 01:36:02,357 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-20 01:36:02,362 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-20 01:36:02,364 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-20 01:36:02,365 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-20 01:36:02,365 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-20 01:36:02,374 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34667
2015-11-20 01:36:02,375 INFO org.mortbay.log: jetty-6.1.26
2015-11-20 01:36:02,528 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34667
2015-11-20 01:36:02,611 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-20 01:36:02,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-20 01:36:02,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-20 01:36:02,651 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-20 01:36:02,662 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-20 01:36:02,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-20 01:36:02,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-20 01:36:02,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-20 01:36:02,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-20 01:36:02,742 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-20 01:36:02,742 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-20 01:36:03,056 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 16379@rushikesh2
2015-11-20 01:36:03,145 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-20 01:36:03,145 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-20 01:36:03,146 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-20 01:36:03,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-20 01:36:03,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-20 01:36:03,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-20 01:36:03,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-20 01:36:03,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 01:36:03,253 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 01:36:03,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 278749184
2015-11-20 01:36:03,261 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-20 01:36:03,261 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-20 01:36:03,261 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 01:36:03,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 2ms
2015-11-20 01:36:03,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-20 01:36:03,429 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1544542630 ms.
2015-11-20 01:36:03,431 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447974573431 with interval 21600000
2015-11-20 01:36:03,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-20 01:36:03,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-20 01:36:03,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-20 01:36:03,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=187
2015-11-20 01:36:03,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-20 01:36:03,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4654a17fdf4,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 51 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-20 01:36:03,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 02:10:14,737 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-20 02:10:18,629 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-20 02:10:18,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-20 04:34:48,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-20 04:34:48,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-20 04:34:48,657 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-20 04:34:48,722 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-20 04:34:48,722 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-20 04:34:48,727 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-20 04:34:48,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-20 04:34:48,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-20 04:34:48,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-20 04:34:48,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-20 04:34:48,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-20 04:34:48,847 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-20 04:34:48,854 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-20 04:34:48,859 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-20 04:34:48,864 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-20 04:34:48,867 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-20 04:34:48,867 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-20 04:34:48,867 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-20 04:34:48,877 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42783
2015-11-20 04:34:48,877 INFO org.mortbay.log: jetty-6.1.26
2015-11-20 04:34:49,030 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42783
2015-11-20 04:34:49,112 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-20 04:34:49,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-20 04:34:49,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-20 04:34:49,151 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-20 04:34:49,162 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-20 04:34:49,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-20 04:34:49,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-20 04:34:49,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-20 04:34:49,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-20 04:34:49,242 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-20 04:34:49,243 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-20 04:34:49,645 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 9306@rushikesh2
2015-11-20 04:34:49,719 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-20 04:34:49,719 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-20 04:34:49,720 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-20 04:34:49,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-20 04:34:49,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-20 04:34:49,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-20 04:34:49,827 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-20 04:34:49,827 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-20 04:34:49,828 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 04:34:49,838 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 10ms
2015-11-20 04:34:49,838 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 11ms
2015-11-20 04:34:49,838 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-20 04:34:49,840 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-20 04:34:49,841 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-20 04:34:49,998 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1533816061 ms.
2015-11-20 04:34:49,999 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1447994497999 with interval 21600000
2015-11-20 04:34:50,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-20 04:34:50,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-20 04:34:50,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-20 04:34:50,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=190
2015-11-20 04:34:50,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-20 04:34:50,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe26c6417fd2,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 5 msec to generate and 68 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-20 04:34:50,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-23 03:29:16,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-23 03:29:16,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-23 03:29:17,446 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-23 03:29:17,556 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-23 03:29:17,556 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-23 03:29:17,561 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-23 03:29:17,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-23 03:29:17,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-23 03:29:17,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-23 03:29:17,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-23 03:29:17,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-23 03:29:17,887 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-23 03:29:17,897 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-23 03:29:17,923 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-23 03:29:17,928 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-23 03:29:17,931 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-23 03:29:17,931 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-23 03:29:17,931 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-23 03:29:17,970 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40249
2015-11-23 03:29:17,971 INFO org.mortbay.log: jetty-6.1.26
2015-11-23 03:29:18,188 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40249
2015-11-23 03:29:18,331 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-23 03:29:18,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-23 03:29:18,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-23 03:29:18,585 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-23 03:29:18,636 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-23 03:29:18,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-23 03:29:18,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-23 03:29:18,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-23 03:29:19,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-23 03:29:19,054 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-23 03:29:19,054 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-23 03:29:19,487 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 21318@rushikesh2
2015-11-23 03:29:19,579 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-23 03:29:19,579 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-23 03:29:19,579 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-23 03:29:19,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-23 03:29:19,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-23 03:29:19,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-23 03:29:19,719 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-23 03:29:19,719 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-23 03:29:19,720 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-23 03:29:19,820 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 100ms
2015-11-23 03:29:19,820 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 102ms
2015-11-23 03:29:19,821 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-23 03:29:19,824 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 3ms
2015-11-23 03:29:19,824 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-11-23 03:29:20,077 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1278545982 ms.
2015-11-23 03:29:20,079 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448249817079 with interval 21600000
2015-11-23 03:29:20,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-23 03:29:20,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-23 03:29:20,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-23 03:29:20,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=196
2015-11-23 03:29:20,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-23 03:29:20,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x71d5c6fdb22,  containing 1 storage report(s), of which we sent 1. The reports had 4 total blocks and used 1 RPC(s). This took 3 msec to generate and 28 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-23 03:29:20,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-23 03:32:44,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741831_1007 src: /192.168.6.248:60054 dest: /192.168.6.249:50010
2015-11-23 03:32:55,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60054, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741831_1007, duration: 11571581843
2015-11-23 03:32:55,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:32:56,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741832_1008 src: /192.168.6.248:60058 dest: /192.168.6.249:50010
2015-11-23 03:33:07,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60058, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741832_1008, duration: 11738646681
2015-11-23 03:33:07,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:33:07,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741833_1009 src: /192.168.6.248:60070 dest: /192.168.6.249:50010
2015-11-23 03:33:15,951 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:629ms (threshold=300ms)
2015-11-23 03:33:20,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60070, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741833_1009, duration: 12247491687
2015-11-23 03:33:20,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:33:20,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741834_1010 src: /192.168.6.248:60074 dest: /192.168.6.249:50010
2015-11-23 03:33:34,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60074, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741834_1010, duration: 13997623921
2015-11-23 03:33:34,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:33:34,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741835_1011 src: /192.168.6.248:60081 dest: /192.168.6.249:50010
2015-11-23 03:33:45,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60081, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741835_1011, duration: 11517260641
2015-11-23 03:33:45,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:33:45,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741836_1012 src: /192.168.6.248:60086 dest: /192.168.6.249:50010
2015-11-23 03:33:57,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60086, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741836_1012, duration: 12264092070
2015-11-23 03:33:57,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:33:57,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741837_1013 src: /192.168.6.248:60092 dest: /192.168.6.249:50010
2015-11-23 03:34:10,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60092, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741837_1013, duration: 12756886328
2015-11-23 03:34:10,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:34:10,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741838_1014 src: /192.168.6.248:60096 dest: /192.168.6.249:50010
2015-11-23 03:34:22,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60096, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741838_1014, duration: 11513670177
2015-11-23 03:34:22,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:34:22,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741839_1015 src: /192.168.6.248:60101 dest: /192.168.6.249:50010
2015-11-23 03:34:36,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60101, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741839_1015, duration: 14232745574
2015-11-23 03:34:36,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:34:36,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741840_1016 src: /192.168.6.248:60108 dest: /192.168.6.249:50010
2015-11-23 03:34:48,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60108, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741840_1016, duration: 11504894223
2015-11-23 03:34:48,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:34:48,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741841_1017 src: /192.168.6.248:60112 dest: /192.168.6.249:50010
2015-11-23 03:34:50,895 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:485ms (threshold=300ms)
2015-11-23 03:35:00,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60112, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741841_1017, duration: 12031291167
2015-11-23 03:35:00,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:35:00,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741842_1018 src: /192.168.6.248:60118 dest: /192.168.6.249:50010
2015-11-23 03:35:13,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60118, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741842_1018, duration: 13080143138
2015-11-23 03:35:13,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:35:13,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741843_1019 src: /192.168.6.248:60124 dest: /192.168.6.249:50010
2015-11-23 03:35:25,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60124, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741843_1019, duration: 12347305151
2015-11-23 03:35:25,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:35:25,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741844_1020 src: /192.168.6.248:60129 dest: /192.168.6.249:50010
2015-11-23 03:35:26,611 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:693ms (threshold=300ms)
2015-11-23 03:35:39,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60129, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741844_1020, duration: 14092791001
2015-11-23 03:35:39,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:35:40,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741845_1021 src: /192.168.6.248:60135 dest: /192.168.6.249:50010
2015-11-23 03:35:51,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60135, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741845_1021, duration: 11505192350
2015-11-23 03:35:51,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:35:51,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741846_1022 src: /192.168.6.248:60140 dest: /192.168.6.249:50010
2015-11-23 03:36:03,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60140, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741846_1022, duration: 11583644496
2015-11-23 03:36:03,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:36:03,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741847_1023 src: /192.168.6.248:60145 dest: /192.168.6.249:50010
2015-11-23 03:36:16,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60145, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741847_1023, duration: 12475005005
2015-11-23 03:36:16,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:36:16,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741848_1024 src: /192.168.6.248:60151 dest: /192.168.6.249:50010
2015-11-23 03:36:27,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60151, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741848_1024, duration: 11521327059
2015-11-23 03:36:27,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:36:27,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741849_1025 src: /192.168.6.248:60158 dest: /192.168.6.249:50010
2015-11-23 03:36:41,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60158, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741849_1025, duration: 13803531015
2015-11-23 03:36:41,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:36:41,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741850_1026 src: /192.168.6.248:60163 dest: /192.168.6.249:50010
2015-11-23 03:36:53,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60163, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741850_1026, duration: 12416329757
2015-11-23 03:36:53,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:36:53,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741851_1027 src: /192.168.6.248:60168 dest: /192.168.6.249:50010
2015-11-23 03:37:05,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60168, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741851_1027, duration: 11780879738
2015-11-23 03:37:05,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:37:05,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741852_1028 src: /192.168.6.248:60174 dest: /192.168.6.249:50010
2015-11-23 03:37:18,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60174, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741852_1028, duration: 12741504456
2015-11-23 03:37:18,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:37:18,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741853_1029 src: /192.168.6.248:60178 dest: /192.168.6.249:50010
2015-11-23 03:37:30,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60178, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741853_1029, duration: 11504825033
2015-11-23 03:37:30,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:37:30,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741854_1030 src: /192.168.6.248:60185 dest: /192.168.6.249:50010
2015-11-23 03:37:32,260 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:579ms (threshold=300ms)
2015-11-23 03:37:43,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60185, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741854_1030, duration: 13786140036
2015-11-23 03:37:43,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:37:43,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741855_1031 src: /192.168.6.248:60191 dest: /192.168.6.249:50010
2015-11-23 03:37:55,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60191, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741855_1031, duration: 11504712102
2015-11-23 03:37:55,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:37:55,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741856_1032 src: /192.168.6.248:60195 dest: /192.168.6.249:50010
2015-11-23 03:38:07,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60195, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741856_1032, duration: 11643607309
2015-11-23 03:38:07,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:38:07,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741857_1033 src: /192.168.6.248:60207 dest: /192.168.6.249:50010
2015-11-23 03:38:20,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60207, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741857_1033, duration: 12866744431
2015-11-23 03:38:20,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741857_1033, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:38:20,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741858_1034 src: /192.168.6.248:60217 dest: /192.168.6.249:50010
2015-11-23 03:38:32,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60217, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741858_1034, duration: 11508119053
2015-11-23 03:38:32,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741858_1034, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:38:32,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741859_1035 src: /192.168.6.248:60230 dest: /192.168.6.249:50010
2015-11-23 03:38:45,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60230, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741859_1035, duration: 13507186575
2015-11-23 03:38:45,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741859_1035, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:38:45,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741860_1036 src: /192.168.6.248:60236 dest: /192.168.6.249:50010
2015-11-23 03:38:47,278 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:531ms (threshold=300ms)
2015-11-23 03:38:57,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60236, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741860_1036, duration: 12025626001
2015-11-23 03:38:57,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:38:57,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741861_1037 src: /192.168.6.248:60242 dest: /192.168.6.249:50010
2015-11-23 03:39:09,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60242, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741861_1037, duration: 12178531509
2015-11-23 03:39:09,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:39:09,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741862_1038 src: /192.168.6.248:60246 dest: /192.168.6.249:50010
2015-11-23 03:39:22,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60246, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741862_1038, duration: 12758357939
2015-11-23 03:39:22,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741862_1038, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:39:22,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741863_1039 src: /192.168.6.248:60252 dest: /192.168.6.249:50010
2015-11-23 03:39:34,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60252, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741863_1039, duration: 11504465730
2015-11-23 03:39:34,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741863_1039, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:39:34,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741864_1040 src: /192.168.6.248:60257 dest: /192.168.6.249:50010
2015-11-23 03:39:47,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60257, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741864_1040, duration: 12996741873
2015-11-23 03:39:47,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741864_1040, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:39:47,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741865_1041 src: /192.168.6.248:60262 dest: /192.168.6.249:50010
2015-11-23 03:39:58,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60262, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741865_1041, duration: 11504984152
2015-11-23 03:39:58,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741865_1041, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:39:58,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741866_1042 src: /192.168.6.248:60268 dest: /192.168.6.249:50010
2015-11-23 03:40:10,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60268, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741866_1042, duration: 11539495640
2015-11-23 03:40:10,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741866_1042, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:40:10,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741867_1043 src: /192.168.6.248:60278 dest: /192.168.6.249:50010
2015-11-23 03:40:23,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60278, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741867_1043, duration: 13016819287
2015-11-23 03:40:23,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741867_1043, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:40:23,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741868_1044 src: /192.168.6.248:60284 dest: /192.168.6.249:50010
2015-11-23 03:40:35,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60284, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741868_1044, duration: 11505538943
2015-11-23 03:40:35,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741868_1044, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:40:35,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741869_1045 src: /192.168.6.248:60290 dest: /192.168.6.249:50010
2015-11-23 03:40:37,382 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:430ms (threshold=300ms)
2015-11-23 03:40:49,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60290, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741869_1045, duration: 13898580652
2015-11-23 03:40:49,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741869_1045, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:40:49,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741870_1046 src: /192.168.6.248:60294 dest: /192.168.6.249:50010
2015-11-23 03:41:00,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60294, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741870_1046, duration: 11505951824
2015-11-23 03:41:00,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741870_1046, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:41:00,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741871_1047 src: /192.168.6.248:60300 dest: /192.168.6.249:50010
2015-11-23 03:41:12,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60300, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741871_1047, duration: 11787826542
2015-11-23 03:41:12,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741871_1047, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:41:12,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741872_1048 src: /192.168.6.248:60311 dest: /192.168.6.249:50010
2015-11-23 03:41:27,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60311, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741872_1048, duration: 14950830410
2015-11-23 03:41:27,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:41:27,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741873_1049 src: /192.168.6.248:60319 dest: /192.168.6.249:50010
2015-11-23 03:41:39,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60319, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741873_1049, duration: 11506272726
2015-11-23 03:41:39,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:41:39,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741874_1050 src: /192.168.6.248:60323 dest: /192.168.6.249:50010
2015-11-23 03:41:53,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60323, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741874_1050, duration: 13972740172
2015-11-23 03:41:53,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741874_1050, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:41:53,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741875_1051 src: /192.168.6.248:60328 dest: /192.168.6.249:50010
2015-11-23 03:42:04,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60328, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741875_1051, duration: 11506091795
2015-11-23 03:42:04,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741875_1051, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:42:04,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741876_1052 src: /192.168.6.248:60333 dest: /192.168.6.249:50010
2015-11-23 03:42:16,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60333, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741876_1052, duration: 12252425413
2015-11-23 03:42:16,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741876_1052, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:42:17,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741877_1053 src: /192.168.6.248:60338 dest: /192.168.6.249:50010
2015-11-23 03:42:29,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60338, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741877_1053, duration: 12412479706
2015-11-23 03:42:29,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741877_1053, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:42:29,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741878_1054 src: /192.168.6.248:60345 dest: /192.168.6.249:50010
2015-11-23 03:42:41,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60345, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741878_1054, duration: 11997683342
2015-11-23 03:42:41,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741878_1054, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:42:41,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741879_1055 src: /192.168.6.248:60350 dest: /192.168.6.249:50010
2015-11-23 03:42:42,785 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:640ms (threshold=300ms)
2015-11-23 03:42:55,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60350, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741879_1055, duration: 14251114412
2015-11-23 03:42:55,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741879_1055, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:42:55,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741880_1056 src: /192.168.6.248:60355 dest: /192.168.6.249:50010
2015-11-23 03:43:07,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60355, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741880_1056, duration: 11504874066
2015-11-23 03:43:07,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741880_1056, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:43:07,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741881_1057 src: /192.168.6.248:60361 dest: /192.168.6.249:50010
2015-11-23 03:43:18,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60361, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741881_1057, duration: 11664295399
2015-11-23 03:43:18,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741881_1057, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:43:19,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741882_1058 src: /192.168.6.248:60365 dest: /192.168.6.249:50010
2015-11-23 03:43:32,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60365, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741882_1058, duration: 12937147336
2015-11-23 03:43:32,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741882_1058, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:43:32,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741883_1059 src: /192.168.6.248:60372 dest: /192.168.6.249:50010
2015-11-23 03:43:43,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60372, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741883_1059, duration: 11525930652
2015-11-23 03:43:43,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741883_1059, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:43:43,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741884_1060 src: /192.168.6.248:60378 dest: /192.168.6.249:50010
2015-11-23 03:43:57,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60378, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741884_1060, duration: 13306692910
2015-11-23 03:43:57,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741884_1060, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:43:57,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741885_1061 src: /192.168.6.248:60384 dest: /192.168.6.249:50010
2015-11-23 03:44:09,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60384, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741885_1061, duration: 12032733791
2015-11-23 03:44:09,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741885_1061, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:44:09,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741886_1062 src: /192.168.6.248:60388 dest: /192.168.6.249:50010
2015-11-23 03:44:21,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60388, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741886_1062, duration: 11990669163
2015-11-23 03:44:21,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741886_1062, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:44:21,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741887_1063 src: /192.168.6.248:60393 dest: /192.168.6.249:50010
2015-11-23 03:44:34,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60393, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741887_1063, duration: 12784503108
2015-11-23 03:44:34,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741887_1063, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:44:34,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741888_1064 src: /192.168.6.248:60399 dest: /192.168.6.249:50010
2015-11-23 03:44:45,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60399, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741888_1064, duration: 11505704227
2015-11-23 03:44:45,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741888_1064, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:44:45,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741889_1065 src: /192.168.6.248:60404 dest: /192.168.6.249:50010
2015-11-23 03:44:47,941 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:568ms (threshold=300ms)
2015-11-23 03:44:59,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60404, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741889_1065, duration: 13921552066
2015-11-23 03:44:59,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741889_1065, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:44:59,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741890_1066 src: /192.168.6.248:60410 dest: /192.168.6.249:50010
2015-11-23 03:45:11,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60410, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741890_1066, duration: 11504613906
2015-11-23 03:45:11,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741890_1066, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:45:11,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741891_1067 src: /192.168.6.248:60415 dest: /192.168.6.249:50010
2015-11-23 03:45:22,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60415, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741891_1067, duration: 11676011739
2015-11-23 03:45:22,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741891_1067, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:45:23,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741892_1068 src: /192.168.6.248:60420 dest: /192.168.6.249:50010
2015-11-23 03:45:35,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60420, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741892_1068, duration: 12433962253
2015-11-23 03:45:35,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741892_1068, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:45:35,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741893_1069 src: /192.168.6.248:60426 dest: /192.168.6.249:50010
2015-11-23 03:45:47,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60426, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741893_1069, duration: 11505270577
2015-11-23 03:45:47,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741893_1069, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:45:47,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741894_1070 src: /192.168.6.248:60430 dest: /192.168.6.249:50010
2015-11-23 03:46:01,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60430, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741894_1070, duration: 14146147782
2015-11-23 03:46:01,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741894_1070, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:46:01,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741895_1071 src: /192.168.6.248:60436 dest: /192.168.6.249:50010
2015-11-23 03:46:12,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60436, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741895_1071, duration: 11505048728
2015-11-23 03:46:12,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741895_1071, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:46:12,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741896_1072 src: /192.168.6.248:60447 dest: /192.168.6.249:50010
2015-11-23 03:46:24,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60447, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741896_1072, duration: 11574148084
2015-11-23 03:46:24,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741896_1072, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:46:24,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741897_1073 src: /192.168.6.248:60453 dest: /192.168.6.249:50010
2015-11-23 03:46:37,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60453, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741897_1073, duration: 12747349761
2015-11-23 03:46:37,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741897_1073, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:46:37,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741898_1074 src: /192.168.6.248:60459 dest: /192.168.6.249:50010
2015-11-23 03:46:51,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60459, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741898_1074, duration: 14028176203
2015-11-23 03:46:51,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741898_1074, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:46:51,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741899_1075 src: /192.168.6.248:60464 dest: /192.168.6.249:50010
2015-11-23 03:47:05,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60464, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741899_1075, duration: 13790826022
2015-11-23 03:47:05,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741899_1075, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:47:05,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741900_1076 src: /192.168.6.248:60470 dest: /192.168.6.249:50010
2015-11-23 03:47:16,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60470, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741900_1076, duration: 11505438542
2015-11-23 03:47:16,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741900_1076, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:47:16,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741901_1077 src: /192.168.6.248:60474 dest: /192.168.6.249:50010
2015-11-23 03:47:28,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60474, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741901_1077, duration: 11505127623
2015-11-23 03:47:28,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741901_1077, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:47:28,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741902_1078 src: /192.168.6.248:60487 dest: /192.168.6.249:50010
2015-11-23 03:47:41,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60487, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741902_1078, duration: 13349435673
2015-11-23 03:47:41,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741902_1078, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:47:41,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741903_1079 src: /192.168.6.248:60498 dest: /192.168.6.249:50010
2015-11-23 03:47:53,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60498, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741903_1079, duration: 11518171531
2015-11-23 03:47:53,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741903_1079, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:47:53,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741904_1080 src: /192.168.6.248:60502 dest: /192.168.6.249:50010
2015-11-23 03:48:07,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60502, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741904_1080, duration: 13990354321
2015-11-23 03:48:07,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741904_1080, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:48:07,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741905_1081 src: /192.168.6.248:60508 dest: /192.168.6.249:50010
2015-11-23 03:48:08,128 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:680ms (threshold=300ms)
2015-11-23 03:48:19,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60508, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741905_1081, duration: 12180912338
2015-11-23 03:48:19,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741905_1081, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:48:19,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741906_1082 src: /192.168.6.248:60512 dest: /192.168.6.249:50010
2015-11-23 03:48:31,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60512, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741906_1082, duration: 11672141450
2015-11-23 03:48:31,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741906_1082, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:48:31,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741907_1083 src: /192.168.6.248:60519 dest: /192.168.6.249:50010
2015-11-23 03:48:44,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60519, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741907_1083, duration: 12544424508
2015-11-23 03:48:44,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741907_1083, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:48:44,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741908_1084 src: /192.168.6.248:60525 dest: /192.168.6.249:50010
2015-11-23 03:48:55,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60525, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741908_1084, duration: 11506919765
2015-11-23 03:48:55,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741908_1084, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:48:55,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741909_1085 src: /192.168.6.248:60529 dest: /192.168.6.249:50010
2015-11-23 03:48:58,297 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:759ms (threshold=300ms)
2015-11-23 03:49:10,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60529, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741909_1085, duration: 14732990799
2015-11-23 03:49:10,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741909_1085, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:49:10,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741910_1086 src: /192.168.6.248:60539 dest: /192.168.6.249:50010
2015-11-23 03:49:21,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60539, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741910_1086, duration: 11505577794
2015-11-23 03:49:21,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741910_1086, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:49:21,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741911_1087 src: /192.168.6.248:60544 dest: /192.168.6.249:50010
2015-11-23 03:49:33,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60544, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741911_1087, duration: 11678002618
2015-11-23 03:49:33,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741911_1087, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:49:33,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741912_1088 src: /192.168.6.248:60550 dest: /192.168.6.249:50010
2015-11-23 03:49:46,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60550, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741912_1088, duration: 12812672475
2015-11-23 03:49:46,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741912_1088, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:49:46,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741913_1089 src: /192.168.6.248:60556 dest: /192.168.6.249:50010
2015-11-23 03:49:58,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60556, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741913_1089, duration: 11516036964
2015-11-23 03:49:58,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741913_1089, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:49:58,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741914_1090 src: /192.168.6.248:60562 dest: /192.168.6.249:50010
2015-11-23 03:50:11,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60562, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741914_1090, duration: 13639306115
2015-11-23 03:50:11,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741914_1090, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:50:11,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741915_1091 src: /192.168.6.248:60567 dest: /192.168.6.249:50010
2015-11-23 03:50:23,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60567, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741915_1091, duration: 12009762178
2015-11-23 03:50:23,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741915_1091, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:50:23,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741916_1092 src: /192.168.6.248:60572 dest: /192.168.6.249:50010
2015-11-23 03:50:26,179 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:572ms (threshold=300ms)
2015-11-23 03:50:35,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60572, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741916_1092, duration: 12062512099
2015-11-23 03:50:35,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741916_1092, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:50:36,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741917_1093 src: /192.168.6.248:60578 dest: /192.168.6.249:50010
2015-11-23 03:50:48,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60578, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741917_1093, duration: 12855158591
2015-11-23 03:50:48,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741917_1093, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:50:49,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741918_1094 src: /192.168.6.248:60582 dest: /192.168.6.249:50010
2015-11-23 03:51:00,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60582, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741918_1094, duration: 11506185235
2015-11-23 03:51:00,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741918_1094, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:51:00,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741919_1095 src: /192.168.6.248:60588 dest: /192.168.6.249:50010
2015-11-23 03:51:17,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60588, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741919_1095, duration: 16705072335
2015-11-23 03:51:17,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741919_1095, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:51:17,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741920_1096 src: /192.168.6.248:60594 dest: /192.168.6.249:50010
2015-11-23 03:51:28,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60594, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741920_1096, duration: 11505985080
2015-11-23 03:51:28,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741920_1096, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:51:28,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741921_1097 src: /192.168.6.248:60607 dest: /192.168.6.249:50010
2015-11-23 03:51:40,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60607, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741921_1097, duration: 11710738208
2015-11-23 03:51:40,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741921_1097, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:51:40,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741922_1098 src: /192.168.6.248:60611 dest: /192.168.6.249:50010
2015-11-23 03:51:53,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60611, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741922_1098, duration: 12508123458
2015-11-23 03:51:53,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741922_1098, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:51:53,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741923_1099 src: /192.168.6.248:60616 dest: /192.168.6.249:50010
2015-11-23 03:51:58,371 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1155ms (threshold=300ms)
2015-11-23 03:52:06,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60616, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741923_1099, duration: 12646572605
2015-11-23 03:52:06,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741923_1099, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:52:06,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741924_1100 src: /192.168.6.248:60622 dest: /192.168.6.249:50010
2015-11-23 03:52:19,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60622, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741924_1100, duration: 13503397550
2015-11-23 03:52:19,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741924_1100, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:52:19,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741925_1101 src: /192.168.6.248:60626 dest: /192.168.6.249:50010
2015-11-23 03:52:31,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60626, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741925_1101, duration: 11528324549
2015-11-23 03:52:31,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741925_1101, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:52:31,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741926_1102 src: /192.168.6.248:60633 dest: /192.168.6.249:50010
2015-11-23 03:52:43,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60633, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741926_1102, duration: 12030978042
2015-11-23 03:52:43,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741926_1102, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:52:43,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741927_1103 src: /192.168.6.248:60638 dest: /192.168.6.249:50010
2015-11-23 03:52:55,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60638, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741927_1103, duration: 12752025369
2015-11-23 03:52:55,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741927_1103, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:52:55,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741928_1104 src: /192.168.6.248:60643 dest: /192.168.6.249:50010
2015-11-23 03:53:07,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60643, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741928_1104, duration: 11505604732
2015-11-23 03:53:07,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741928_1104, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:53:07,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741929_1105 src: /192.168.6.248:60649 dest: /192.168.6.249:50010
2015-11-23 03:53:21,454 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:609ms (threshold=300ms)
2015-11-23 03:53:22,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60649, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741929_1105, duration: 14619978739
2015-11-23 03:53:22,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741929_1105, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:53:22,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741930_1106 src: /192.168.6.248:60654 dest: /192.168.6.249:50010
2015-11-23 03:53:33,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60654, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741930_1106, duration: 11504644847
2015-11-23 03:53:33,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741930_1106, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:53:33,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741931_1107 src: /192.168.6.248:60660 dest: /192.168.6.249:50010
2015-11-23 03:53:46,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60660, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741931_1107, duration: 12466372483
2015-11-23 03:53:46,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741931_1107, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:53:46,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741932_1108 src: /192.168.6.248:60666 dest: /192.168.6.249:50010
2015-11-23 03:53:59,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60666, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741932_1108, duration: 12683633565
2015-11-23 03:53:59,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741932_1108, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:53:59,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741933_1109 src: /192.168.6.248:60672 dest: /192.168.6.249:50010
2015-11-23 03:54:10,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60672, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741933_1109, duration: 11507259710
2015-11-23 03:54:10,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741933_1109, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:54:10,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741934_1110 src: /192.168.6.248:60676 dest: /192.168.6.249:50010
2015-11-23 03:54:24,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60676, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741934_1110, duration: 14279481690
2015-11-23 03:54:24,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741934_1110, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:54:25,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741935_1111 src: /192.168.6.248:60683 dest: /192.168.6.249:50010
2015-11-23 03:54:36,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60683, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741935_1111, duration: 11579851696
2015-11-23 03:54:36,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741935_1111, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:54:36,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741936_1112 src: /192.168.6.248:60689 dest: /192.168.6.249:50010
2015-11-23 03:54:48,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60689, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741936_1112, duration: 11689368903
2015-11-23 03:54:48,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741936_1112, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:54:48,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741937_1113 src: /192.168.6.248:60693 dest: /192.168.6.249:50010
2015-11-23 03:55:01,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60693, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741937_1113, duration: 12519619145
2015-11-23 03:55:01,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741937_1113, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:55:01,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741938_1114 src: /192.168.6.248:60699 dest: /192.168.6.249:50010
2015-11-23 03:55:12,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60699, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741938_1114, duration: 11507147745
2015-11-23 03:55:12,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741938_1114, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:55:12,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741939_1115 src: /192.168.6.248:60704 dest: /192.168.6.249:50010
2015-11-23 03:55:26,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60704, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741939_1115, duration: 13750057067
2015-11-23 03:55:26,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741939_1115, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:55:26,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741940_1116 src: /192.168.6.248:60710 dest: /192.168.6.249:50010
2015-11-23 03:55:38,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60710, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741940_1116, duration: 11673338356
2015-11-23 03:55:38,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741940_1116, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:55:38,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741941_1117 src: /192.168.6.248:60716 dest: /192.168.6.249:50010
2015-11-23 03:55:39,704 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:580ms (threshold=300ms)
2015-11-23 03:55:50,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60716, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741941_1117, duration: 12316417257
2015-11-23 03:55:50,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741941_1117, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:55:50,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741942_1118 src: /192.168.6.248:60720 dest: /192.168.6.249:50010
2015-11-23 03:56:03,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60720, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741942_1118, duration: 12726815173
2015-11-23 03:56:03,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741942_1118, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:56:03,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741943_1119 src: /192.168.6.248:60726 dest: /192.168.6.249:50010
2015-11-23 03:56:14,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60726, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741943_1119, duration: 11513659882
2015-11-23 03:56:14,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741943_1119, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:56:15,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741944_1120 src: /192.168.6.248:60732 dest: /192.168.6.249:50010
2015-11-23 03:56:28,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60732, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741944_1120, duration: 13643398190
2015-11-23 03:56:28,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741944_1120, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:56:28,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741945_1121 src: /192.168.6.248:60739 dest: /192.168.6.249:50010
2015-11-23 03:56:40,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60739, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741945_1121, duration: 11507668656
2015-11-23 03:56:40,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741945_1121, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:56:40,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741946_1122 src: /192.168.6.248:60743 dest: /192.168.6.249:50010
2015-11-23 03:56:51,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60743, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741946_1122, duration: 11614356269
2015-11-23 03:56:51,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741946_1122, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:56:52,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741947_1123 src: /192.168.6.248:60748 dest: /192.168.6.249:50010
2015-11-23 03:56:54,805 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:604ms (threshold=300ms)
2015-11-23 03:57:05,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60748, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741947_1123, duration: 13388221879
2015-11-23 03:57:05,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741947_1123, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:57:05,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741948_1124 src: /192.168.6.248:60754 dest: /192.168.6.249:50010
2015-11-23 03:57:17,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60754, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741948_1124, duration: 11508291646
2015-11-23 03:57:17,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741948_1124, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:57:17,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741949_1125 src: /192.168.6.248:60764 dest: /192.168.6.249:50010
2015-11-23 03:57:25,220 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:591ms (threshold=300ms)
2015-11-23 03:57:30,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60764, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741949_1125, duration: 13567213030
2015-11-23 03:57:30,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741949_1125, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:57:30,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741950_1126 src: /192.168.6.248:60771 dest: /192.168.6.249:50010
2015-11-23 03:57:42,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60771, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741950_1126, duration: 11506894437
2015-11-23 03:57:42,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741950_1126, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:57:42,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741951_1127 src: /192.168.6.248:60776 dest: /192.168.6.249:50010
2015-11-23 03:57:54,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60776, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741951_1127, duration: 12175229436
2015-11-23 03:57:54,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741951_1127, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:57:54,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741952_1128 src: /192.168.6.248:60781 dest: /192.168.6.249:50010
2015-11-23 03:58:07,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60781, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741952_1128, duration: 12699118205
2015-11-23 03:58:07,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741952_1128, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:58:07,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741953_1129 src: /192.168.6.248:60787 dest: /192.168.6.249:50010
2015-11-23 03:58:18,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60787, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741953_1129, duration: 11507502108
2015-11-23 03:58:18,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741953_1129, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:58:18,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741954_1130 src: /192.168.6.248:60791 dest: /192.168.6.249:50010
2015-11-23 03:58:32,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60791, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741954_1130, duration: 13593002653
2015-11-23 03:58:32,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741954_1130, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:58:32,323 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741955_1131 src: /192.168.6.248:60798 dest: /192.168.6.249:50010
2015-11-23 03:58:43,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60798, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741955_1131, duration: 11509974718
2015-11-23 03:58:43,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741955_1131, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:58:43,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741956_1132 src: /192.168.6.248:60803 dest: /192.168.6.249:50010
2015-11-23 03:58:55,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60803, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741956_1132, duration: 11649482278
2015-11-23 03:58:55,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741956_1132, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:58:56,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741957_1133 src: /192.168.6.248:60808 dest: /192.168.6.249:50010
2015-11-23 03:59:08,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60808, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741957_1133, duration: 12619847468
2015-11-23 03:59:08,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741957_1133, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:59:08,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741958_1134 src: /192.168.6.248:60814 dest: /192.168.6.249:50010
2015-11-23 03:59:20,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60814, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741958_1134, duration: 11508784744
2015-11-23 03:59:20,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741958_1134, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:59:20,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741959_1135 src: /192.168.6.248:60818 dest: /192.168.6.249:50010
2015-11-23 03:59:33,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60818, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741959_1135, duration: 13498488434
2015-11-23 03:59:33,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741959_1135, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 03:59:33,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741960_1136 src: /192.168.6.248:60825 dest: /192.168.6.249:50010
2015-11-23 03:59:38,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60825, dest: /192.168.6.249:50010, bytes: 51875046, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1677636661_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741960_1136, duration: 4448136357
2015-11-23 03:59:38,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741960_1136, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:06:46,019 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2015-11-23 04:06:46,054 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2015-11-23 04:06:46,087 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741829_1005 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741829
2015-11-23 04:06:46,087 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741830_1006 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741830
2015-11-23 04:10:29,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741961_1137 src: /192.168.6.248:60996 dest: /192.168.6.249:50010
2015-11-23 04:10:40,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:60996, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741961_1137, duration: 11540509252
2015-11-23 04:10:40,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741961_1137, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:10:40,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741962_1138 src: /192.168.6.248:61000 dest: /192.168.6.249:50010
2015-11-23 04:10:52,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:61000, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741962_1138, duration: 11639887688
2015-11-23 04:10:52,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741962_1138, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:10:52,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741963_1139 src: /192.168.6.248:32773 dest: /192.168.6.249:50010
2015-11-23 04:11:04,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32773, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741963_1139, duration: 12405995314
2015-11-23 04:11:04,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741963_1139, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:11:04,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741964_1140 src: /192.168.6.248:32777 dest: /192.168.6.249:50010
2015-11-23 04:11:16,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32777, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741964_1140, duration: 11507330029
2015-11-23 04:11:16,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741964_1140, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:11:16,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741965_1141 src: /192.168.6.248:32783 dest: /192.168.6.249:50010
2015-11-23 04:11:28,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32783, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741965_1141, duration: 11506511930
2015-11-23 04:11:28,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741965_1141, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:11:28,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741966_1142 src: /192.168.6.248:32790 dest: /192.168.6.249:50010
2015-11-23 04:11:39,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32790, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741966_1142, duration: 11515012823
2015-11-23 04:11:39,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741966_1142, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:11:39,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741967_1143 src: /192.168.6.248:32794 dest: /192.168.6.249:50010
2015-11-23 04:11:41,294 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:593ms (threshold=300ms)
2015-11-23 04:11:51,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32794, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741967_1143, duration: 12083672358
2015-11-23 04:11:51,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741967_1143, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:11:51,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741968_1144 src: /192.168.6.248:32801 dest: /192.168.6.249:50010
2015-11-23 04:12:04,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32801, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741968_1144, duration: 12315765781
2015-11-23 04:12:04,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741968_1144, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:12:04,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741969_1145 src: /192.168.6.248:32805 dest: /192.168.6.249:50010
2015-11-23 04:12:11,546 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:502ms (threshold=300ms)
2015-11-23 04:12:16,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32805, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741969_1145, duration: 11995078067
2015-11-23 04:12:16,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741969_1145, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:12:16,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741970_1146 src: /192.168.6.248:32811 dest: /192.168.6.249:50010
2015-11-23 04:12:17,267 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:723ms (threshold=300ms)
2015-11-23 04:12:28,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32811, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741970_1146, duration: 12215586031
2015-11-23 04:12:28,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741970_1146, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:12:28,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741971_1147 src: /192.168.6.248:32818 dest: /192.168.6.249:50010
2015-11-23 04:12:40,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32818, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741971_1147, duration: 12137541099
2015-11-23 04:12:40,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741971_1147, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:12:40,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741972_1148 src: /192.168.6.248:32822 dest: /192.168.6.249:50010
2015-11-23 04:12:52,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32822, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741972_1148, duration: 11520581435
2015-11-23 04:12:52,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741972_1148, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:12:52,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741973_1149 src: /192.168.6.248:32828 dest: /192.168.6.249:50010
2015-11-23 04:13:04,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32828, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741973_1149, duration: 12325568155
2015-11-23 04:13:04,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741973_1149, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:13:04,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741974_1150 src: /192.168.6.248:32832 dest: /192.168.6.249:50010
2015-11-23 04:13:15,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32832, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741974_1150, duration: 11505461067
2015-11-23 04:13:15,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741974_1150, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:13:15,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741975_1151 src: /192.168.6.248:32838 dest: /192.168.6.249:50010
2015-11-23 04:13:27,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32838, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741975_1151, duration: 11772395903
2015-11-23 04:13:27,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741975_1151, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:13:27,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741976_1152 src: /192.168.6.248:32845 dest: /192.168.6.249:50010
2015-11-23 04:13:40,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32845, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741976_1152, duration: 12236158290
2015-11-23 04:13:40,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741976_1152, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:13:40,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741977_1153 src: /192.168.6.248:32849 dest: /192.168.6.249:50010
2015-11-23 04:13:51,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32849, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741977_1153, duration: 11505382931
2015-11-23 04:13:51,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741977_1153, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:13:51,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741978_1154 src: /192.168.6.248:32855 dest: /192.168.6.249:50010
2015-11-23 04:14:03,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32855, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741978_1154, duration: 11507972173
2015-11-23 04:14:03,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741978_1154, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:14:03,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741979_1155 src: /192.168.6.248:32859 dest: /192.168.6.249:50010
2015-11-23 04:14:14,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32859, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741979_1155, duration: 11509132583
2015-11-23 04:14:14,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741979_1155, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:14:14,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741980_1156 src: /192.168.6.248:32865 dest: /192.168.6.249:50010
2015-11-23 04:14:26,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32865, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741980_1156, duration: 11675442514
2015-11-23 04:14:26,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741980_1156, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:14:27,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741981_1157 src: /192.168.6.248:32870 dest: /192.168.6.249:50010
2015-11-23 04:14:39,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32870, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741981_1157, duration: 12216458398
2015-11-23 04:14:39,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741981_1157, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:14:39,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741982_1158 src: /192.168.6.248:32876 dest: /192.168.6.249:50010
2015-11-23 04:14:42,436 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:590ms (threshold=300ms)
2015-11-23 04:14:51,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32876, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741982_1158, duration: 12259573610
2015-11-23 04:14:51,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741982_1158, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:14:51,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741983_1159 src: /192.168.6.248:32882 dest: /192.168.6.249:50010
2015-11-23 04:15:03,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32882, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741983_1159, duration: 12075488614
2015-11-23 04:15:03,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741983_1159, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:15:04,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741984_1160 src: /192.168.6.248:32886 dest: /192.168.6.249:50010
2015-11-23 04:15:12,456 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:556ms (threshold=300ms)
2015-11-23 04:15:16,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32886, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741984_1160, duration: 12553221483
2015-11-23 04:15:16,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741984_1160, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:15:16,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741985_1161 src: /192.168.6.248:32892 dest: /192.168.6.249:50010
2015-11-23 04:15:29,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32892, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741985_1161, duration: 12832238413
2015-11-23 04:15:29,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741985_1161, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:15:29,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741986_1162 src: /192.168.6.248:32899 dest: /192.168.6.249:50010
2015-11-23 04:15:41,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32899, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741986_1162, duration: 11872896903
2015-11-23 04:15:41,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741986_1162, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:15:41,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741987_1163 src: /192.168.6.248:32903 dest: /192.168.6.249:50010
2015-11-23 04:15:53,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32903, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741987_1163, duration: 11947243296
2015-11-23 04:15:53,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741987_1163, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:15:53,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741988_1164 src: /192.168.6.248:32909 dest: /192.168.6.249:50010
2015-11-23 04:15:57,899 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:946ms (threshold=300ms)
2015-11-23 04:16:06,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32909, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741988_1164, duration: 13592886904
2015-11-23 04:16:06,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741988_1164, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:16:07,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741989_1165 src: /192.168.6.248:32914 dest: /192.168.6.249:50010
2015-11-23 04:16:18,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32914, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741989_1165, duration: 11897091258
2015-11-23 04:16:18,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741989_1165, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:16:18,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741990_1166 src: /192.168.6.248:32925 dest: /192.168.6.249:50010
2015-11-23 04:16:30,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32925, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741990_1166, duration: 11926629184
2015-11-23 04:16:30,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741990_1166, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:16:30,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741991_1167 src: /192.168.6.248:32932 dest: /192.168.6.249:50010
2015-11-23 04:16:43,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32932, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741991_1167, duration: 12858526690
2015-11-23 04:16:43,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741991_1167, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:16:43,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741992_1168 src: /192.168.6.248:32937 dest: /192.168.6.249:50010
2015-11-23 04:16:55,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32937, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741992_1168, duration: 11922624735
2015-11-23 04:16:55,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741992_1168, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:16:55,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741993_1169 src: /192.168.6.248:32943 dest: /192.168.6.249:50010
2015-11-23 04:17:08,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32943, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741993_1169, duration: 12623134453
2015-11-23 04:17:08,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741993_1169, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:17:08,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741994_1170 src: /192.168.6.248:32948 dest: /192.168.6.249:50010
2015-11-23 04:17:13,056 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1035ms (threshold=300ms)
2015-11-23 04:17:21,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32948, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741994_1170, duration: 13218570000
2015-11-23 04:17:21,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741994_1170, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:17:21,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741995_1171 src: /192.168.6.248:32954 dest: /192.168.6.249:50010
2015-11-23 04:17:33,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32954, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741995_1171, duration: 11963284973
2015-11-23 04:17:33,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741995_1171, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:17:35,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741996_1172 src: /192.168.6.248:32959 dest: /192.168.6.249:50010
2015-11-23 04:17:47,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32959, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741996_1172, duration: 11826639462
2015-11-23 04:17:47,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741996_1172, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:17:47,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741997_1173 src: /192.168.6.248:32966 dest: /192.168.6.249:50010
2015-11-23 04:17:59,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32966, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741997_1173, duration: 12418823565
2015-11-23 04:17:59,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741997_1173, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:17:59,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741998_1174 src: /192.168.6.248:32972 dest: /192.168.6.249:50010
2015-11-23 04:18:11,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32972, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741998_1174, duration: 12063206387
2015-11-23 04:18:11,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741998_1174, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:18:11,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073741999_1175 src: /192.168.6.248:32980 dest: /192.168.6.249:50010
2015-11-23 04:18:23,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32980, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073741999_1175, duration: 12036879243
2015-11-23 04:18:23,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073741999_1175, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:18:23,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742000_1176 src: /192.168.6.248:32986 dest: /192.168.6.249:50010
2015-11-23 04:18:28,013 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:884ms (threshold=300ms)
2015-11-23 04:18:36,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32986, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742000_1176, duration: 12818435838
2015-11-23 04:18:36,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742000_1176, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:18:37,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742001_1177 src: /192.168.6.248:32991 dest: /192.168.6.249:50010
2015-11-23 04:18:49,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32991, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742001_1177, duration: 11833085728
2015-11-23 04:18:49,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742001_1177, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:18:49,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742002_1178 src: /192.168.6.248:32996 dest: /192.168.6.249:50010
2015-11-23 04:19:01,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:32996, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742002_1178, duration: 11909286701
2015-11-23 04:19:01,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742002_1178, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:19:01,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742003_1179 src: /192.168.6.248:33001 dest: /192.168.6.249:50010
2015-11-23 04:19:04,166 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1178ms (threshold=300ms)
2015-11-23 04:19:15,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33001, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742003_1179, duration: 13384451968
2015-11-23 04:19:15,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742003_1179, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:19:15,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742004_1180 src: /192.168.6.248:33007 dest: /192.168.6.249:50010
2015-11-23 04:19:27,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33007, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742004_1180, duration: 11924970977
2015-11-23 04:19:27,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742004_1180, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:19:27,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742005_1181 src: /192.168.6.248:33012 dest: /192.168.6.249:50010
2015-11-23 04:19:39,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33012, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742005_1181, duration: 12039295714
2015-11-23 04:19:39,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742005_1181, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:19:39,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742006_1182 src: /192.168.6.248:33018 dest: /192.168.6.249:50010
2015-11-23 04:19:44,055 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:919ms (threshold=300ms)
2015-11-23 04:19:52,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33018, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742006_1182, duration: 12988684420
2015-11-23 04:19:52,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742006_1182, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:19:52,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742007_1183 src: /192.168.6.248:33030 dest: /192.168.6.249:50010
2015-11-23 04:20:04,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33030, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742007_1183, duration: 11972744522
2015-11-23 04:20:04,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742007_1183, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:20:04,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742008_1184 src: /192.168.6.248:33034 dest: /192.168.6.249:50010
2015-11-23 04:20:16,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33034, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742008_1184, duration: 11846264415
2015-11-23 04:20:16,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742008_1184, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:20:16,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742009_1185 src: /192.168.6.248:33040 dest: /192.168.6.249:50010
2015-11-23 04:20:19,237 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1033ms (threshold=300ms)
2015-11-23 04:20:29,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33040, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742009_1185, duration: 13221288404
2015-11-23 04:20:29,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742009_1185, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:20:29,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742010_1186 src: /192.168.6.248:33047 dest: /192.168.6.249:50010
2015-11-23 04:20:41,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33047, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742010_1186, duration: 11919694859
2015-11-23 04:20:41,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742010_1186, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:20:41,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742011_1187 src: /192.168.6.248:33052 dest: /192.168.6.249:50010
2015-11-23 04:20:53,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33052, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742011_1187, duration: 12045614715
2015-11-23 04:20:53,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742011_1187, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:20:53,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742012_1188 src: /192.168.6.248:33057 dest: /192.168.6.249:50010
2015-11-23 04:21:06,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33057, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742012_1188, duration: 13148410911
2015-11-23 04:21:06,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742012_1188, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:21:08,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742013_1189 src: /192.168.6.248:33062 dest: /192.168.6.249:50010
2015-11-23 04:21:20,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33062, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742013_1189, duration: 11876087705
2015-11-23 04:21:20,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742013_1189, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:21:20,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742014_1190 src: /192.168.6.248:33067 dest: /192.168.6.249:50010
2015-11-23 04:21:32,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33067, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742014_1190, duration: 11882043889
2015-11-23 04:21:32,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742014_1190, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:21:32,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742015_1191 src: /192.168.6.248:33073 dest: /192.168.6.249:50010
2015-11-23 04:21:34,821 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1289ms (threshold=300ms)
2015-11-23 04:21:45,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33073, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742015_1191, duration: 13419044112
2015-11-23 04:21:45,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742015_1191, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:21:45,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742016_1192 src: /192.168.6.248:33079 dest: /192.168.6.249:50010
2015-11-23 04:21:57,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33079, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742016_1192, duration: 12006269265
2015-11-23 04:21:57,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742016_1192, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:21:58,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742017_1193 src: /192.168.6.248:33085 dest: /192.168.6.249:50010
2015-11-23 04:22:10,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33085, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742017_1193, duration: 12182422801
2015-11-23 04:22:10,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742017_1193, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:22:10,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742018_1194 src: /192.168.6.248:33089 dest: /192.168.6.249:50010
2015-11-23 04:22:22,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33089, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742018_1194, duration: 12165303989
2015-11-23 04:22:22,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742018_1194, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:22:23,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742019_1195 src: /192.168.6.248:33096 dest: /192.168.6.249:50010
2015-11-23 04:22:36,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33096, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742019_1195, duration: 12290197261
2015-11-23 04:22:36,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742019_1195, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:22:36,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742020_1196 src: /192.168.6.248:33101 dest: /192.168.6.249:50010
2015-11-23 04:22:48,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33101, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742020_1196, duration: 12709557122
2015-11-23 04:22:48,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742020_1196, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:22:48,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742021_1197 src: /192.168.6.248:33106 dest: /192.168.6.249:50010
2015-11-23 04:23:01,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33106, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742021_1197, duration: 12574233024
2015-11-23 04:23:01,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742021_1197, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:23:01,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742022_1198 src: /192.168.6.248:33119 dest: /192.168.6.249:50010
2015-11-23 04:23:09,479 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:741ms (threshold=300ms)
2015-11-23 04:23:14,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33119, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742022_1198, duration: 12841099115
2015-11-23 04:23:14,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742022_1198, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:23:14,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742023_1199 src: /192.168.6.248:33125 dest: /192.168.6.249:50010
2015-11-23 04:23:26,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33125, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742023_1199, duration: 12099884471
2015-11-23 04:23:26,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742023_1199, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:23:27,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742024_1200 src: /192.168.6.248:33130 dest: /192.168.6.249:50010
2015-11-23 04:23:40,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33130, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742024_1200, duration: 12938965922
2015-11-23 04:23:40,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742024_1200, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:23:40,323 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742025_1201 src: /192.168.6.248:33136 dest: /192.168.6.249:50010
2015-11-23 04:23:52,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33136, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742025_1201, duration: 12540380038
2015-11-23 04:23:52,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742025_1201, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:23:53,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742026_1202 src: /192.168.6.248:33142 dest: /192.168.6.249:50010
2015-11-23 04:24:05,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33142, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742026_1202, duration: 11877046227
2015-11-23 04:24:05,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742026_1202, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:24:05,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742027_1203 src: /192.168.6.248:33146 dest: /192.168.6.249:50010
2015-11-23 04:24:17,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33146, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742027_1203, duration: 12364432067
2015-11-23 04:24:17,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742027_1203, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:24:17,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742028_1204 src: /192.168.6.248:33152 dest: /192.168.6.249:50010
2015-11-23 04:24:20,123 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1227ms (threshold=300ms)
2015-11-23 04:24:31,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33152, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742028_1204, duration: 13727711233
2015-11-23 04:24:31,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742028_1204, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:24:31,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742029_1205 src: /192.168.6.248:33159 dest: /192.168.6.249:50010
2015-11-23 04:24:43,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33159, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742029_1205, duration: 12229623859
2015-11-23 04:24:43,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742029_1205, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:24:44,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742030_1206 src: /192.168.6.248:33165 dest: /192.168.6.249:50010
2015-11-23 04:24:56,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33165, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742030_1206, duration: 12004922945
2015-11-23 04:24:56,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742030_1206, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:24:56,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742031_1207 src: /192.168.6.248:33169 dest: /192.168.6.249:50010
2015-11-23 04:25:09,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33169, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742031_1207, duration: 12281301412
2015-11-23 04:25:09,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742031_1207, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:25:09,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742032_1208 src: /192.168.6.248:33175 dest: /192.168.6.249:50010
2015-11-23 04:25:22,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33175, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742032_1208, duration: 13279143594
2015-11-23 04:25:22,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742032_1208, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:25:22,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742033_1209 src: /192.168.6.248:33181 dest: /192.168.6.249:50010
2015-11-23 04:25:35,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33181, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742033_1209, duration: 12631158287
2015-11-23 04:25:35,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742033_1209, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:25:35,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742034_1210 src: /192.168.6.248:33187 dest: /192.168.6.249:50010
2015-11-23 04:25:47,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33187, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742034_1210, duration: 12039236528
2015-11-23 04:25:47,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742034_1210, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:25:47,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742035_1211 src: /192.168.6.248:33192 dest: /192.168.6.249:50010
2015-11-23 04:26:00,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33192, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742035_1211, duration: 12278199836
2015-11-23 04:26:00,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742035_1211, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:26:01,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742036_1212 src: /192.168.6.248:33198 dest: /192.168.6.249:50010
2015-11-23 04:26:13,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33198, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742036_1212, duration: 12522985642
2015-11-23 04:26:13,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742036_1212, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:26:13,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742037_1213 src: /192.168.6.248:33203 dest: /192.168.6.249:50010
2015-11-23 04:26:25,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33203, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742037_1213, duration: 12036938627
2015-11-23 04:26:25,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742037_1213, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:26:26,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742038_1214 src: /192.168.6.248:33209 dest: /192.168.6.249:50010
2015-11-23 04:26:38,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33209, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742038_1214, duration: 11911077677
2015-11-23 04:26:38,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742038_1214, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:26:38,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742039_1215 src: /192.168.6.248:33214 dest: /192.168.6.249:50010
2015-11-23 04:26:40,946 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:958ms (threshold=300ms)
2015-11-23 04:26:51,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33214, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742039_1215, duration: 13047883640
2015-11-23 04:26:51,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742039_1215, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:26:52,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742040_1216 src: /192.168.6.248:33220 dest: /192.168.6.249:50010
2015-11-23 04:27:04,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33220, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742040_1216, duration: 11909579625
2015-11-23 04:27:04,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742040_1216, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:27:04,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742041_1217 src: /192.168.6.248:33224 dest: /192.168.6.249:50010
2015-11-23 04:27:17,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33224, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742041_1217, duration: 12466449339
2015-11-23 04:27:17,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742041_1217, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:27:20,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742042_1218 src: /192.168.6.248:33230 dest: /192.168.6.249:50010
2015-11-23 04:27:32,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33230, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742042_1218, duration: 11785668171
2015-11-23 04:27:32,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742042_1218, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:27:32,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742043_1219 src: /192.168.6.248:33243 dest: /192.168.6.249:50010
2015-11-23 04:27:44,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33243, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742043_1219, duration: 12150131727
2015-11-23 04:27:44,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742043_1219, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:27:45,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742044_1220 src: /192.168.6.248:33249 dest: /192.168.6.249:50010
2015-11-23 04:27:57,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33249, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742044_1220, duration: 11874417584
2015-11-23 04:27:57,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742044_1220, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:27:57,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742045_1221 src: /192.168.6.248:33255 dest: /192.168.6.249:50010
2015-11-23 04:28:02,076 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1159ms (threshold=300ms)
2015-11-23 04:28:11,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33255, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742045_1221, duration: 13575406213
2015-11-23 04:28:11,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742045_1221, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:28:12,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742046_1222 src: /192.168.6.248:33260 dest: /192.168.6.249:50010
2015-11-23 04:28:24,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33260, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742046_1222, duration: 11937975856
2015-11-23 04:28:24,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742046_1222, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:28:24,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742047_1223 src: /192.168.6.248:33266 dest: /192.168.6.249:50010
2015-11-23 04:28:36,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33266, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742047_1223, duration: 12300416533
2015-11-23 04:28:36,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742047_1223, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:28:40,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742048_1224 src: /192.168.6.248:33271 dest: /192.168.6.249:50010
2015-11-23 04:28:52,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33271, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742048_1224, duration: 11902684123
2015-11-23 04:28:52,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742048_1224, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:28:52,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742049_1225 src: /192.168.6.248:33277 dest: /192.168.6.249:50010
2015-11-23 04:29:04,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33277, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742049_1225, duration: 12029342909
2015-11-23 04:29:04,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742049_1225, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:29:05,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742050_1226 src: /192.168.6.248:33281 dest: /192.168.6.249:50010
2015-11-23 04:29:17,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33281, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742050_1226, duration: 11810386053
2015-11-23 04:29:17,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742050_1226, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:29:17,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742051_1227 src: /192.168.6.248:33287 dest: /192.168.6.249:50010
2015-11-23 04:29:22,268 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1263ms (threshold=300ms)
2015-11-23 04:29:31,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33287, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742051_1227, duration: 13887708426
2015-11-23 04:29:31,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742051_1227, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:29:31,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742052_1228 src: /192.168.6.248:33294 dest: /192.168.6.249:50010
2015-11-23 04:29:43,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33294, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742052_1228, duration: 11903443605
2015-11-23 04:29:43,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742052_1228, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:29:43,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742053_1229 src: /192.168.6.248:33299 dest: /192.168.6.249:50010
2015-11-23 04:29:57,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33299, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742053_1229, duration: 13424024030
2015-11-23 04:29:57,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742053_1229, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:29:57,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742054_1230 src: /192.168.6.248:33304 dest: /192.168.6.249:50010
2015-11-23 04:30:09,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33304, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742054_1230, duration: 11887640018
2015-11-23 04:30:09,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742054_1230, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:30:09,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742055_1231 src: /192.168.6.248:33311 dest: /192.168.6.249:50010
2015-11-23 04:30:21,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33311, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742055_1231, duration: 12373856166
2015-11-23 04:30:21,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742055_1231, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:30:21,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742056_1232 src: /192.168.6.248:33317 dest: /192.168.6.249:50010
2015-11-23 04:30:34,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33317, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742056_1232, duration: 12783395024
2015-11-23 04:30:34,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742056_1232, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:30:34,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742057_1233 src: /192.168.6.248:33323 dest: /192.168.6.249:50010
2015-11-23 04:30:46,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33323, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742057_1233, duration: 12345457325
2015-11-23 04:30:46,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742057_1233, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:30:46,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742058_1234 src: /192.168.6.248:33329 dest: /192.168.6.249:50010
2015-11-23 04:30:58,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33329, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742058_1234, duration: 11915262390
2015-11-23 04:30:58,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742058_1234, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:30:58,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742059_1235 src: /192.168.6.248:33335 dest: /192.168.6.249:50010
2015-11-23 04:31:02,855 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:807ms (threshold=300ms)
2015-11-23 04:31:13,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33335, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742059_1235, duration: 14377499833
2015-11-23 04:31:13,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742059_1235, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:31:13,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742060_1236 src: /192.168.6.248:33340 dest: /192.168.6.249:50010
2015-11-23 04:31:25,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33340, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742060_1236, duration: 12219456917
2015-11-23 04:31:25,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742060_1236, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:31:25,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742061_1237 src: /192.168.6.248:33346 dest: /192.168.6.249:50010
2015-11-23 04:31:38,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33346, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742061_1237, duration: 12559803473
2015-11-23 04:31:38,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742061_1237, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:31:38,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742062_1238 src: /192.168.6.248:33351 dest: /192.168.6.249:50010
2015-11-23 04:31:50,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33351, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742062_1238, duration: 12373968663
2015-11-23 04:31:50,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742062_1238, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:31:50,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742063_1239 src: /192.168.6.248:33356 dest: /192.168.6.249:50010
2015-11-23 04:32:03,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33356, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742063_1239, duration: 13428707364
2015-11-23 04:32:03,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742063_1239, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:32:03,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742064_1240 src: /192.168.6.248:33361 dest: /192.168.6.249:50010
2015-11-23 04:32:16,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33361, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742064_1240, duration: 12284840843
2015-11-23 04:32:16,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742064_1240, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:32:16,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742065_1241 src: /192.168.6.248:33367 dest: /192.168.6.249:50010
2015-11-23 04:32:28,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33367, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742065_1241, duration: 12441660752
2015-11-23 04:32:28,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742065_1241, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:32:28,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742066_1242 src: /192.168.6.248:33374 dest: /192.168.6.249:50010
2015-11-23 04:32:41,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33374, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742066_1242, duration: 12645520785
2015-11-23 04:32:41,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742066_1242, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:32:41,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742067_1243 src: /192.168.6.248:33378 dest: /192.168.6.249:50010
2015-11-23 04:32:53,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33378, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742067_1243, duration: 12076703006
2015-11-23 04:32:53,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742067_1243, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:32:53,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742068_1244 src: /192.168.6.248:33384 dest: /192.168.6.249:50010
2015-11-23 04:33:05,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33384, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742068_1244, duration: 12389856604
2015-11-23 04:33:05,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742068_1244, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:33:05,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742069_1245 src: /192.168.6.248:33389 dest: /192.168.6.249:50010
2015-11-23 04:33:18,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33389, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742069_1245, duration: 12108146226
2015-11-23 04:33:18,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742069_1245, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:33:18,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742070_1246 src: /192.168.6.248:33394 dest: /192.168.6.249:50010
2015-11-23 04:33:31,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33394, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742070_1246, duration: 13052155291
2015-11-23 04:33:31,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742070_1246, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:33:31,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742071_1247 src: /192.168.6.248:33401 dest: /192.168.6.249:50010
2015-11-23 04:33:43,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33401, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742071_1247, duration: 12204017743
2015-11-23 04:33:43,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742071_1247, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:33:43,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742072_1248 src: /192.168.6.248:33406 dest: /192.168.6.249:50010
2015-11-23 04:33:55,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33406, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742072_1248, duration: 12249403784
2015-11-23 04:33:55,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742072_1248, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:33:55,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742073_1249 src: /192.168.6.248:33411 dest: /192.168.6.249:50010
2015-11-23 04:33:58,776 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:908ms (threshold=300ms)
2015-11-23 04:34:08,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33411, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742073_1249, duration: 13207025891
2015-11-23 04:34:08,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742073_1249, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:34:08,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742074_1250 src: /192.168.6.248:33416 dest: /192.168.6.249:50010
2015-11-23 04:34:21,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33416, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742074_1250, duration: 12237189984
2015-11-23 04:34:21,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742074_1250, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:34:21,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742075_1251 src: /192.168.6.248:33421 dest: /192.168.6.249:50010
2015-11-23 04:34:33,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33421, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742075_1251, duration: 12215847358
2015-11-23 04:34:33,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742075_1251, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:34:33,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742076_1252 src: /192.168.6.248:33427 dest: /192.168.6.249:50010
2015-11-23 04:34:49,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33427, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742076_1252, duration: 16005334368
2015-11-23 04:34:49,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742076_1252, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:34:49,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742077_1253 src: /192.168.6.248:33433 dest: /192.168.6.249:50010
2015-11-23 04:35:01,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33433, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742077_1253, duration: 12226082339
2015-11-23 04:35:01,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742077_1253, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:35:01,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742078_1254 src: /192.168.6.248:33438 dest: /192.168.6.249:50010
2015-11-23 04:35:14,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33438, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742078_1254, duration: 12580162453
2015-11-23 04:35:14,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742078_1254, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:35:14,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742079_1255 src: /192.168.6.248:33444 dest: /192.168.6.249:50010
2015-11-23 04:35:26,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33444, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742079_1255, duration: 12481833395
2015-11-23 04:35:26,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742079_1255, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:35:26,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742080_1256 src: /192.168.6.248:33449 dest: /192.168.6.249:50010
2015-11-23 04:35:28,926 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:951ms (threshold=300ms)
2015-11-23 04:35:40,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33449, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742080_1256, duration: 13081185378
2015-11-23 04:35:40,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742080_1256, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:35:40,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742081_1257 src: /192.168.6.248:33455 dest: /192.168.6.249:50010
2015-11-23 04:35:52,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33455, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742081_1257, duration: 12352668577
2015-11-23 04:35:52,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742081_1257, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:35:52,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742082_1258 src: /192.168.6.248:33461 dest: /192.168.6.249:50010
2015-11-23 04:36:05,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33461, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742082_1258, duration: 12685992374
2015-11-23 04:36:05,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742082_1258, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:36:05,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742083_1259 src: /192.168.6.248:33465 dest: /192.168.6.249:50010
2015-11-23 04:36:17,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33465, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742083_1259, duration: 12305503401
2015-11-23 04:36:17,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742083_1259, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:36:17,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742084_1260 src: /192.168.6.248:33471 dest: /192.168.6.249:50010
2015-11-23 04:36:29,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33471, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742084_1260, duration: 12155927397
2015-11-23 04:36:29,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742084_1260, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:36:29,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742085_1261 src: /192.168.6.248:33478 dest: /192.168.6.249:50010
2015-11-23 04:36:41,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33478, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742085_1261, duration: 12204660878
2015-11-23 04:36:41,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742085_1261, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:36:41,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742086_1262 src: /192.168.6.248:33483 dest: /192.168.6.249:50010
2015-11-23 04:36:44,398 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:1271ms (threshold=300ms)
2015-11-23 04:36:55,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33483, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742086_1262, duration: 13436908641
2015-11-23 04:36:55,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742086_1262, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:36:55,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742087_1263 src: /192.168.6.248:33488 dest: /192.168.6.249:50010
2015-11-23 04:37:07,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33488, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742087_1263, duration: 12189866879
2015-11-23 04:37:07,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742087_1263, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:37:07,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742088_1264 src: /192.168.6.248:33493 dest: /192.168.6.249:50010
2015-11-23 04:37:19,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33493, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742088_1264, duration: 12252592899
2015-11-23 04:37:19,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742088_1264, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:37:19,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742089_1265 src: /192.168.6.248:33498 dest: /192.168.6.249:50010
2015-11-23 04:37:24,068 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:895ms (threshold=300ms)
2015-11-23 04:37:33,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33498, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742089_1265, duration: 13472022009
2015-11-23 04:37:33,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742089_1265, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:37:33,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742090_1266 src: /192.168.6.248:33504 dest: /192.168.6.249:50010
2015-11-23 04:37:38,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:33504, dest: /192.168.6.249:50010, bytes: 51875046, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660002807_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742090_1266, duration: 4700498989
2015-11-23 04:37:38,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742090_1266, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:39:40,040 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-23 04:40:03,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); maxRetries=45
2015-11-23 04:40:05,699 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-23 04:40:05,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-23 04:43:00,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-23 04:43:00,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-23 04:43:02,236 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-23 04:43:02,326 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-23 04:43:02,326 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-23 04:43:02,331 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-23 04:43:02,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-23 04:43:02,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-23 04:43:02,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-23 04:43:02,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-23 04:43:02,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-23 04:43:02,754 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-23 04:43:02,768 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-23 04:43:02,793 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-23 04:43:02,801 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-23 04:43:02,804 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-23 04:43:02,804 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-23 04:43:02,804 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-23 04:43:02,837 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 53114
2015-11-23 04:43:02,837 INFO org.mortbay.log: jetty-6.1.26
2015-11-23 04:43:03,167 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:53114
2015-11-23 04:43:03,315 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-23 04:43:03,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-23 04:43:03,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-23 04:43:03,498 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-23 04:43:03,535 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-23 04:43:03,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-23 04:43:03,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-23 04:43:03,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-23 04:43:03,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-23 04:43:03,847 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-23 04:43:03,847 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-23 04:43:04,327 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3152@rushikesh2
2015-11-23 04:43:04,406 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-23 04:43:04,406 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-23 04:43:04,408 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-23 04:43:04,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-23 04:43:04,519 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-23 04:43:04,519 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-23 04:43:04,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-23 04:43:04,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-23 04:43:04,563 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-23 04:43:04,592 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35143766016
2015-11-23 04:43:04,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 31ms
2015-11-23 04:43:04,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 39ms
2015-11-23 04:43:04,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-23 04:43:04,636 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 42ms
2015-11-23 04:43:04,636 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 42ms
2015-11-23 04:43:04,972 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1274121087 ms.
2015-11-23 04:43:04,975 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448249387975 with interval 21600000
2015-11-23 04:43:04,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-23 04:43:05,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-23 04:43:05,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-23 04:43:05,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=994
2015-11-23 04:43:05,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-23 04:43:05,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2746904981,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 7 msec to generate and 262 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-23 04:43:05,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-23 04:43:30,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741900_1076 to 192.168.6.237:50010 
2015-11-23 04:43:30,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741901_1077 to 192.168.6.237:50010 
2015-11-23 04:44:24,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741900_1076 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 04:44:24,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741898_1074 to 192.168.6.237:50010 
2015-11-23 04:44:26,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741901_1077 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 04:44:27,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741892_1068 to 192.168.6.237:50010 
2015-11-23 04:45:13,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741898_1074 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 04:45:15,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741892_1068 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 04:45:15,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742061_1237 to 192.168.6.237:50010 
2015-11-23 04:45:15,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742059_1235 to 192.168.6.237:50010 
2015-11-23 04:46:02,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742059_1235 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 04:46:03,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741987_1163 to 192.168.6.237:50010 
2015-11-23 04:46:08,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742061_1237 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 04:46:09,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741986_1162 to 192.168.6.237:50010 
2015-11-23 04:46:55,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741987_1163 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 04:46:57,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741854_1030 to 192.168.6.237:50010 
2015-11-23 04:46:59,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741986_1162 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 04:47:00,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741843_1019 to 192.168.6.237:50010 
2015-11-23 04:47:12,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742080_1256 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742080 for deletion
2015-11-23 04:47:12,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742081_1257 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742081 for deletion
2015-11-23 04:47:12,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742082_1258 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742082 for deletion
2015-11-23 04:47:12,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742083_1259 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742083 for deletion
2015-11-23 04:47:12,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742084_1260 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742084 for deletion
2015-11-23 04:47:12,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742085_1261 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742085 for deletion
2015-11-23 04:47:12,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742086_1262 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742086 for deletion
2015-11-23 04:47:12,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742087_1263 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742087 for deletion
2015-11-23 04:47:12,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742088_1264 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742088 for deletion
2015-11-23 04:47:12,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742089_1265 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742089 for deletion
2015-11-23 04:47:12,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742090_1266 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742090 for deletion
2015-11-23 04:47:12,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741961_1137 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741961 for deletion
2015-11-23 04:47:12,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741962_1138 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741962 for deletion
2015-11-23 04:47:12,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741963_1139 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741963 for deletion
2015-11-23 04:47:12,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741964_1140 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741964 for deletion
2015-11-23 04:47:12,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741965_1141 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741965 for deletion
2015-11-23 04:47:12,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741966_1142 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741966 for deletion
2015-11-23 04:47:12,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741967_1143 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741967 for deletion
2015-11-23 04:47:12,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741968_1144 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741968 for deletion
2015-11-23 04:47:12,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741969_1145 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741969 for deletion
2015-11-23 04:47:12,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741970_1146 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741970 for deletion
2015-11-23 04:47:12,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741971_1147 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741971 for deletion
2015-11-23 04:47:12,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741972_1148 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741972 for deletion
2015-11-23 04:47:12,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741973_1149 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741973 for deletion
2015-11-23 04:47:12,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741974_1150 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741974 for deletion
2015-11-23 04:47:12,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741975_1151 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741975 for deletion
2015-11-23 04:47:12,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741976_1152 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741976 for deletion
2015-11-23 04:47:12,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741977_1153 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741977 for deletion
2015-11-23 04:47:12,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741978_1154 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741978 for deletion
2015-11-23 04:47:12,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741979_1155 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741979 for deletion
2015-11-23 04:47:12,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741980_1156 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741980 for deletion
2015-11-23 04:47:12,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741981_1157 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741981 for deletion
2015-11-23 04:47:12,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741982_1158 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741982 for deletion
2015-11-23 04:47:12,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741983_1159 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741983 for deletion
2015-11-23 04:47:12,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741984_1160 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741984 for deletion
2015-11-23 04:47:12,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741985_1161 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741985 for deletion
2015-11-23 04:47:12,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741986_1162 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741986 for deletion
2015-11-23 04:47:12,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741987_1163 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741987 for deletion
2015-11-23 04:47:12,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741988_1164 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741988 for deletion
2015-11-23 04:47:12,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741989_1165 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741989 for deletion
2015-11-23 04:47:12,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741990_1166 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741990 for deletion
2015-11-23 04:47:12,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741991_1167 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741991 for deletion
2015-11-23 04:47:12,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741992_1168 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741992 for deletion
2015-11-23 04:47:12,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741993_1169 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741993 for deletion
2015-11-23 04:47:12,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741994_1170 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741994 for deletion
2015-11-23 04:47:12,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741995_1171 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741995 for deletion
2015-11-23 04:47:12,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741996_1172 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741996 for deletion
2015-11-23 04:47:12,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741997_1173 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741997 for deletion
2015-11-23 04:47:12,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741998_1174 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741998 for deletion
2015-11-23 04:47:12,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741999_1175 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741999 for deletion
2015-11-23 04:47:12,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742000_1176 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742000 for deletion
2015-11-23 04:47:12,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742001_1177 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742001 for deletion
2015-11-23 04:47:12,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742002_1178 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742002 for deletion
2015-11-23 04:47:12,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742003_1179 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742003 for deletion
2015-11-23 04:47:12,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742004_1180 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742004 for deletion
2015-11-23 04:47:12,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742005_1181 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742005 for deletion
2015-11-23 04:47:12,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742006_1182 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742006 for deletion
2015-11-23 04:47:12,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742007_1183 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742007 for deletion
2015-11-23 04:47:12,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742008_1184 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742008 for deletion
2015-11-23 04:47:12,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742009_1185 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742009 for deletion
2015-11-23 04:47:12,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742010_1186 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742010 for deletion
2015-11-23 04:47:12,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742011_1187 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742011 for deletion
2015-11-23 04:47:12,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742012_1188 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742012 for deletion
2015-11-23 04:47:12,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742013_1189 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742013 for deletion
2015-11-23 04:47:12,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742014_1190 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742014 for deletion
2015-11-23 04:47:12,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742015_1191 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742015 for deletion
2015-11-23 04:47:12,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742016_1192 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742016 for deletion
2015-11-23 04:47:12,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742017_1193 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742017 for deletion
2015-11-23 04:47:12,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742018_1194 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742018 for deletion
2015-11-23 04:47:12,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742019_1195 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742019 for deletion
2015-11-23 04:47:12,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742020_1196 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742020 for deletion
2015-11-23 04:47:12,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742021_1197 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742021 for deletion
2015-11-23 04:47:12,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742022_1198 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742022 for deletion
2015-11-23 04:47:12,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742023_1199 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742023 for deletion
2015-11-23 04:47:12,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742024_1200 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742024 for deletion
2015-11-23 04:47:12,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742025_1201 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742025 for deletion
2015-11-23 04:47:12,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742026_1202 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742026 for deletion
2015-11-23 04:47:12,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742027_1203 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742027 for deletion
2015-11-23 04:47:12,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742028_1204 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742028 for deletion
2015-11-23 04:47:12,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742029_1205 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742029 for deletion
2015-11-23 04:47:12,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742030_1206 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742030 for deletion
2015-11-23 04:47:12,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742031_1207 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742031 for deletion
2015-11-23 04:47:12,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742032_1208 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742032 for deletion
2015-11-23 04:47:12,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742033_1209 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742033 for deletion
2015-11-23 04:47:12,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742034_1210 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742034 for deletion
2015-11-23 04:47:12,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742035_1211 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742035 for deletion
2015-11-23 04:47:12,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742036_1212 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742036 for deletion
2015-11-23 04:47:12,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742037_1213 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742037 for deletion
2015-11-23 04:47:12,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742038_1214 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742038 for deletion
2015-11-23 04:47:12,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742039_1215 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742039 for deletion
2015-11-23 04:47:12,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742040_1216 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742040 for deletion
2015-11-23 04:47:12,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742041_1217 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742041 for deletion
2015-11-23 04:47:12,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742042_1218 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742042 for deletion
2015-11-23 04:47:12,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742043_1219 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742043 for deletion
2015-11-23 04:47:12,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742044_1220 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742044 for deletion
2015-11-23 04:47:12,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742045_1221 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742045 for deletion
2015-11-23 04:47:12,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742046_1222 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742046 for deletion
2015-11-23 04:47:12,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742047_1223 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742047 for deletion
2015-11-23 04:47:12,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742048_1224 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742048 for deletion
2015-11-23 04:47:12,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742049_1225 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742049 for deletion
2015-11-23 04:47:12,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742050_1226 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742050 for deletion
2015-11-23 04:47:12,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742051_1227 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742051 for deletion
2015-11-23 04:47:12,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742052_1228 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742052 for deletion
2015-11-23 04:47:12,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742053_1229 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742053 for deletion
2015-11-23 04:47:12,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742054_1230 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742054 for deletion
2015-11-23 04:47:12,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742055_1231 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742055 for deletion
2015-11-23 04:47:12,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742056_1232 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742056 for deletion
2015-11-23 04:47:12,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742057_1233 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742057 for deletion
2015-11-23 04:47:12,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742058_1234 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742058 for deletion
2015-11-23 04:47:12,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742059_1235 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742059 for deletion
2015-11-23 04:47:12,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742060_1236 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742060 for deletion
2015-11-23 04:47:12,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742061_1237 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742061 for deletion
2015-11-23 04:47:12,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742062_1238 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742062 for deletion
2015-11-23 04:47:12,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742063_1239 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742063 for deletion
2015-11-23 04:47:12,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742064_1240 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742064 for deletion
2015-11-23 04:47:12,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742065_1241 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742065 for deletion
2015-11-23 04:47:12,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742066_1242 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742066 for deletion
2015-11-23 04:47:12,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742067_1243 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742067 for deletion
2015-11-23 04:47:12,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742068_1244 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742068 for deletion
2015-11-23 04:47:12,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742069_1245 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742069 for deletion
2015-11-23 04:47:12,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742070_1246 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742070 for deletion
2015-11-23 04:47:12,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742071_1247 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742071 for deletion
2015-11-23 04:47:12,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742072_1248 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742072 for deletion
2015-11-23 04:47:12,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742073_1249 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742073 for deletion
2015-11-23 04:47:12,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742074_1250 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742074 for deletion
2015-11-23 04:47:12,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742075_1251 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742075 for deletion
2015-11-23 04:47:12,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742076_1252 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742076 for deletion
2015-11-23 04:47:12,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742077_1253 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742077 for deletion
2015-11-23 04:47:12,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742078_1254 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742078 for deletion
2015-11-23 04:47:12,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742079_1255 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742079 for deletion
2015-11-23 04:47:12,831 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742080_1256 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742080
2015-11-23 04:47:12,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742081_1257 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742081
2015-11-23 04:47:12,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742082_1258 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742082
2015-11-23 04:47:12,833 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742083_1259 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742083
2015-11-23 04:47:12,833 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742084_1260 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742084
2015-11-23 04:47:12,833 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742085_1261 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742085
2015-11-23 04:47:12,834 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742086_1262 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742086
2015-11-23 04:47:12,834 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742087_1263 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742087
2015-11-23 04:47:12,834 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742088_1264 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742088
2015-11-23 04:47:12,835 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742089_1265 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742089
2015-11-23 04:47:12,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742090_1266 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir1/blk_1073742090
2015-11-23 04:47:12,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741961_1137 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741961
2015-11-23 04:47:12,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741962_1138 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741962
2015-11-23 04:47:12,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741963_1139 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741963
2015-11-23 04:47:12,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741964_1140 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741964
2015-11-23 04:47:12,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741965_1141 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741965
2015-11-23 04:47:12,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741966_1142 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741966
2015-11-23 04:47:12,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741967_1143 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741967
2015-11-23 04:47:12,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741968_1144 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741968
2015-11-23 04:47:12,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741969_1145 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741969
2015-11-23 04:47:12,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741970_1146 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741970
2015-11-23 04:47:12,857 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741971_1147 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741971
2015-11-23 04:47:12,857 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741972_1148 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741972
2015-11-23 04:47:12,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741973_1149 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741973
2015-11-23 04:47:12,863 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741974_1150 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741974
2015-11-23 04:47:12,863 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741975_1151 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741975
2015-11-23 04:47:12,864 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741976_1152 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741976
2015-11-23 04:47:12,864 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741977_1153 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741977
2015-11-23 04:47:12,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741978_1154 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741978
2015-11-23 04:47:12,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741979_1155 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741979
2015-11-23 04:47:12,866 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741980_1156 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741980
2015-11-23 04:47:12,866 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741981_1157 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741981
2015-11-23 04:47:12,866 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741982_1158 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741982
2015-11-23 04:47:12,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741983_1159 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741983
2015-11-23 04:47:12,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741984_1160 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741984
2015-11-23 04:47:12,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741985_1161 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741985
2015-11-23 04:47:12,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741986_1162 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741986
2015-11-23 04:47:12,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741987_1163 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741987
2015-11-23 04:47:12,878 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741988_1164 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741988
2015-11-23 04:47:12,878 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741989_1165 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741989
2015-11-23 04:47:12,878 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741990_1166 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741990
2015-11-23 04:47:12,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741991_1167 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741991
2015-11-23 04:47:12,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741992_1168 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741992
2015-11-23 04:47:12,880 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741993_1169 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741993
2015-11-23 04:47:12,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741994_1170 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741994
2015-11-23 04:47:12,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741995_1171 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741995
2015-11-23 04:47:12,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741996_1172 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741996
2015-11-23 04:47:12,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741997_1173 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741997
2015-11-23 04:47:12,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741998_1174 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741998
2015-11-23 04:47:12,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741999_1175 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741999
2015-11-23 04:47:12,883 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742000_1176 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742000
2015-11-23 04:47:12,883 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742001_1177 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742001
2015-11-23 04:47:12,883 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742002_1178 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742002
2015-11-23 04:47:12,919 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742003_1179 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742003
2015-11-23 04:47:12,919 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742004_1180 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742004
2015-11-23 04:47:12,919 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742005_1181 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742005
2015-11-23 04:47:12,920 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742006_1182 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742006
2015-11-23 04:47:12,920 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742007_1183 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742007
2015-11-23 04:47:12,921 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742008_1184 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742008
2015-11-23 04:47:12,921 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742009_1185 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742009
2015-11-23 04:47:12,922 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742010_1186 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742010
2015-11-23 04:47:12,922 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742011_1187 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742011
2015-11-23 04:47:12,922 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742012_1188 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742012
2015-11-23 04:47:12,923 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742013_1189 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742013
2015-11-23 04:47:12,923 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742014_1190 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742014
2015-11-23 04:47:12,924 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742015_1191 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742015
2015-11-23 04:47:12,924 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742016_1192 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742016
2015-11-23 04:47:12,925 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742017_1193 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742017
2015-11-23 04:47:12,925 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742018_1194 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742018
2015-11-23 04:47:12,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742019_1195 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742019
2015-11-23 04:47:12,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742020_1196 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742020
2015-11-23 04:47:12,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742021_1197 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742021
2015-11-23 04:47:12,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742022_1198 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742022
2015-11-23 04:47:12,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742023_1199 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742023
2015-11-23 04:47:12,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742024_1200 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742024
2015-11-23 04:47:12,932 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742025_1201 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742025
2015-11-23 04:47:12,932 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742026_1202 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742026
2015-11-23 04:47:12,932 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742027_1203 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742027
2015-11-23 04:47:12,933 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742028_1204 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742028
2015-11-23 04:47:12,933 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742029_1205 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742029
2015-11-23 04:47:12,934 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742030_1206 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742030
2015-11-23 04:47:12,934 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742031_1207 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742031
2015-11-23 04:47:12,934 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742032_1208 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742032
2015-11-23 04:47:12,944 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742033_1209 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742033
2015-11-23 04:47:12,944 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742034_1210 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742034
2015-11-23 04:47:12,962 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742035_1211 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742035
2015-11-23 04:47:12,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742036_1212 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742036
2015-11-23 04:47:12,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742037_1213 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742037
2015-11-23 04:47:12,963 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742038_1214 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742038
2015-11-23 04:47:12,964 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742039_1215 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742039
2015-11-23 04:47:12,964 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742040_1216 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742040
2015-11-23 04:47:12,965 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742041_1217 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742041
2015-11-23 04:47:12,966 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742042_1218 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742042
2015-11-23 04:47:12,966 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742043_1219 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742043
2015-11-23 04:47:12,966 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742044_1220 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742044
2015-11-23 04:47:12,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742045_1221 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742045
2015-11-23 04:47:12,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742046_1222 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742046
2015-11-23 04:47:12,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742047_1223 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742047
2015-11-23 04:47:13,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742048_1224 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742048
2015-11-23 04:47:13,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742049_1225 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742049
2015-11-23 04:47:13,070 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742050_1226 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742050
2015-11-23 04:47:13,122 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742051_1227 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742051
2015-11-23 04:47:13,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742052_1228 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742052
2015-11-23 04:47:13,124 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742053_1229 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742053
2015-11-23 04:47:13,124 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742054_1230 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742054
2015-11-23 04:47:13,124 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742055_1231 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742055
2015-11-23 04:47:13,125 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742056_1232 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742056
2015-11-23 04:47:13,125 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742057_1233 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742057
2015-11-23 04:47:13,125 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742058_1234 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742058
2015-11-23 04:47:13,126 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742059_1235 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742059
2015-11-23 04:47:13,126 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742060_1236 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742060
2015-11-23 04:47:13,127 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742061_1237 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742061
2015-11-23 04:47:13,128 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742062_1238 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742062
2015-11-23 04:47:13,128 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742063_1239 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742063
2015-11-23 04:47:13,144 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742064_1240 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742064
2015-11-23 04:47:13,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742065_1241 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742065
2015-11-23 04:47:13,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742066_1242 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742066
2015-11-23 04:47:13,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742067_1243 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742067
2015-11-23 04:47:13,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742068_1244 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742068
2015-11-23 04:47:13,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742069_1245 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742069
2015-11-23 04:47:13,147 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742070_1246 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742070
2015-11-23 04:47:13,147 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742071_1247 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742071
2015-11-23 04:47:13,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742072_1248 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742072
2015-11-23 04:47:13,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742073_1249 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742073
2015-11-23 04:47:13,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742074_1250 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742074
2015-11-23 04:47:13,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742075_1251 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742075
2015-11-23 04:47:13,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742076_1252 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742076
2015-11-23 04:47:13,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742077_1253 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742077
2015-11-23 04:47:13,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742078_1254 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742078
2015-11-23 04:47:13,171 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742079_1255 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073742079
2015-11-23 04:47:39,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2015-11-23 04:47:39,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2015-11-23 04:47:39,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2015-11-23 04:47:39,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741834_1010 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741834 for deletion
2015-11-23 04:47:39,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741835_1011 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741835 for deletion
2015-11-23 04:47:39,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2015-11-23 04:47:39,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2015-11-23 04:47:39,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2015-11-23 04:47:39,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2015-11-23 04:47:39,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2015-11-23 04:47:39,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2015-11-23 04:47:39,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2015-11-23 04:47:39,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2015-11-23 04:47:39,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2015-11-23 04:47:39,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2015-11-23 04:47:39,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2015-11-23 04:47:39,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2015-11-23 04:47:39,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2015-11-23 04:47:39,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741849_1025 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741849 for deletion
2015-11-23 04:47:39,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2015-11-23 04:47:39,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2015-11-23 04:47:39,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2015-11-23 04:47:39,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2015-11-23 04:47:39,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2015-11-23 04:47:39,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2015-11-23 04:47:39,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2015-11-23 04:47:39,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2015-11-23 04:47:39,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2015-11-23 04:47:39,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741859_1035 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741859 for deletion
2015-11-23 04:47:39,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2015-11-23 04:47:39,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2015-11-23 04:47:39,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2015-11-23 04:47:39,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2015-11-23 04:47:39,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2015-11-23 04:47:39,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2015-11-23 04:47:39,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741866_1042 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741866 for deletion
2015-11-23 04:47:39,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741867_1043 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741867 for deletion
2015-11-23 04:47:39,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2015-11-23 04:47:39,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741869_1045 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741869 for deletion
2015-11-23 04:47:39,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2015-11-23 04:47:39,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741871_1047 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741871 for deletion
2015-11-23 04:47:39,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2015-11-23 04:47:39,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2015-11-23 04:47:39,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2015-11-23 04:47:39,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2015-11-23 04:47:39,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2015-11-23 04:47:39,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741877_1053 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741877 for deletion
2015-11-23 04:47:39,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2015-11-23 04:47:39,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2015-11-23 04:47:39,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741880_1056 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741880 for deletion
2015-11-23 04:47:39,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741881_1057 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741881 for deletion
2015-11-23 04:47:39,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741882_1058 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741882 for deletion
2015-11-23 04:47:39,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741883_1059 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741883 for deletion
2015-11-23 04:47:39,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741884_1060 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741884 for deletion
2015-11-23 04:47:39,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2015-11-23 04:47:39,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741886_1062 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741886 for deletion
2015-11-23 04:47:39,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741887_1063 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741887 for deletion
2015-11-23 04:47:39,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741888_1064 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741888 for deletion
2015-11-23 04:47:39,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2015-11-23 04:47:39,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741890_1066 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741890 for deletion
2015-11-23 04:47:39,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2015-11-23 04:47:39,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2015-11-23 04:47:39,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2015-11-23 04:47:39,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741894_1070 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741894 for deletion
2015-11-23 04:47:39,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741895_1071 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741895 for deletion
2015-11-23 04:47:39,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741896_1072 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741896 for deletion
2015-11-23 04:47:39,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741897_1073 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741897 for deletion
2015-11-23 04:47:39,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741898_1074 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741898 for deletion
2015-11-23 04:47:39,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741899_1075 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741899 for deletion
2015-11-23 04:47:39,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741900_1076 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741900 for deletion
2015-11-23 04:47:39,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741901_1077 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741901 for deletion
2015-11-23 04:47:39,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741902_1078 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741902 for deletion
2015-11-23 04:47:39,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741903_1079 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741903 for deletion
2015-11-23 04:47:39,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741904_1080 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741904 for deletion
2015-11-23 04:47:39,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741905_1081 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741905 for deletion
2015-11-23 04:47:39,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741906_1082 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741906 for deletion
2015-11-23 04:47:39,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741907_1083 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741907 for deletion
2015-11-23 04:47:39,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741908_1084 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741908 for deletion
2015-11-23 04:47:39,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741909_1085 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741909 for deletion
2015-11-23 04:47:39,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741910_1086 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741910 for deletion
2015-11-23 04:47:39,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741911_1087 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741911 for deletion
2015-11-23 04:47:39,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741912_1088 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741912 for deletion
2015-11-23 04:47:39,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741913_1089 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741913 for deletion
2015-11-23 04:47:39,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741914_1090 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741914 for deletion
2015-11-23 04:47:39,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741915_1091 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741915 for deletion
2015-11-23 04:47:39,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741916_1092 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741916 for deletion
2015-11-23 04:47:39,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741917_1093 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741917 for deletion
2015-11-23 04:47:39,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741918_1094 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741918 for deletion
2015-11-23 04:47:39,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741919_1095 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741919 for deletion
2015-11-23 04:47:39,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741920_1096 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741920 for deletion
2015-11-23 04:47:39,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741921_1097 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741921 for deletion
2015-11-23 04:47:39,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741922_1098 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741922 for deletion
2015-11-23 04:47:39,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741923_1099 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741923 for deletion
2015-11-23 04:47:39,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741924_1100 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741924 for deletion
2015-11-23 04:47:39,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741925_1101 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741925 for deletion
2015-11-23 04:47:39,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741926_1102 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741926 for deletion
2015-11-23 04:47:39,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741927_1103 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741927 for deletion
2015-11-23 04:47:39,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741928_1104 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741928 for deletion
2015-11-23 04:47:39,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741929_1105 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741929 for deletion
2015-11-23 04:47:39,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741930_1106 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741930 for deletion
2015-11-23 04:47:39,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2015-11-23 04:47:39,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741932_1108 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741932 for deletion
2015-11-23 04:47:39,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741933_1109 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741933 for deletion
2015-11-23 04:47:39,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741934_1110 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741934 for deletion
2015-11-23 04:47:39,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741935_1111 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741935 for deletion
2015-11-23 04:47:39,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741936_1112 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741936 for deletion
2015-11-23 04:47:39,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741937_1113 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741937 for deletion
2015-11-23 04:47:39,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741938_1114 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741938 for deletion
2015-11-23 04:47:39,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741939_1115 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741939 for deletion
2015-11-23 04:47:39,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741940_1116 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741940 for deletion
2015-11-23 04:47:39,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741941_1117 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741941 for deletion
2015-11-23 04:47:39,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741942_1118 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741942 for deletion
2015-11-23 04:47:39,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741943_1119 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741943 for deletion
2015-11-23 04:47:39,801 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741944_1120 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741944 for deletion
2015-11-23 04:47:39,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741945_1121 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741945 for deletion
2015-11-23 04:47:39,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741946_1122 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741946 for deletion
2015-11-23 04:47:39,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741947_1123 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741947 for deletion
2015-11-23 04:47:39,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741948_1124 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741948 for deletion
2015-11-23 04:47:39,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741949_1125 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741949 for deletion
2015-11-23 04:47:39,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741950_1126 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741950 for deletion
2015-11-23 04:47:39,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741951_1127 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741951 for deletion
2015-11-23 04:47:39,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741952_1128 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741952 for deletion
2015-11-23 04:47:39,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741953_1129 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741953 for deletion
2015-11-23 04:47:39,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741954_1130 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741954 for deletion
2015-11-23 04:47:39,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741955_1131 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741955 for deletion
2015-11-23 04:47:39,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741956_1132 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741956 for deletion
2015-11-23 04:47:39,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741957_1133 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741957 for deletion
2015-11-23 04:47:39,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741958_1134 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741958 for deletion
2015-11-23 04:47:39,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741831_1007 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741831
2015-11-23 04:47:39,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741959_1135 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741959 for deletion
2015-11-23 04:47:39,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741960_1136 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741960 for deletion
2015-11-23 04:47:39,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741832_1008 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741832
2015-11-23 04:47:39,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741833_1009 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741833
2015-11-23 04:47:39,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741834_1010 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741834
2015-11-23 04:47:39,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741835_1011 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741835
2015-11-23 04:47:39,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741836_1012 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741836
2015-11-23 04:47:39,806 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741837_1013 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741837
2015-11-23 04:47:39,817 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741838_1014 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741838
2015-11-23 04:47:39,817 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741839_1015 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741839
2015-11-23 04:47:39,818 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741840_1016 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741840
2015-11-23 04:47:39,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741841_1017 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741841
2015-11-23 04:47:39,830 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741842_1018 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741842
2015-11-23 04:47:39,830 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741843_1019 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741843
2015-11-23 04:47:39,830 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741844_1020 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741844
2015-11-23 04:47:39,831 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741845_1021 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741845
2015-11-23 04:47:39,831 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741846_1022 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741846
2015-11-23 04:47:39,831 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741847_1023 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741847
2015-11-23 04:47:39,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741848_1024 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741848
2015-11-23 04:47:39,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741849_1025 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741849
2015-11-23 04:47:39,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741850_1026 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741850
2015-11-23 04:47:39,833 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741851_1027 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741851
2015-11-23 04:47:39,833 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741852_1028 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741852
2015-11-23 04:47:39,834 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741853_1029 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741853
2015-11-23 04:47:39,834 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741854_1030 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741854
2015-11-23 04:47:39,835 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741855_1031 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741855
2015-11-23 04:47:39,835 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741856_1032 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741856
2015-11-23 04:47:39,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741857_1033 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741857
2015-11-23 04:47:39,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741858_1034 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741858
2015-11-23 04:47:39,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741859_1035 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741859
2015-11-23 04:47:39,837 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741860_1036 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741860
2015-11-23 04:47:39,837 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741861_1037 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741861
2015-11-23 04:47:39,838 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741862_1038 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741862
2015-11-23 04:47:39,838 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741863_1039 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741863
2015-11-23 04:47:39,839 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741864_1040 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741864
2015-11-23 04:47:39,839 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741865_1041 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741865
2015-11-23 04:47:39,840 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741866_1042 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741866
2015-11-23 04:47:39,840 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741867_1043 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741867
2015-11-23 04:47:39,840 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741868_1044 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741868
2015-11-23 04:47:39,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741869_1045 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741869
2015-11-23 04:47:39,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741870_1046 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741870
2015-11-23 04:47:39,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741871_1047 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741871
2015-11-23 04:47:39,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741872_1048 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741872
2015-11-23 04:47:39,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741873_1049 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741873
2015-11-23 04:47:39,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741874_1050 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741874
2015-11-23 04:47:39,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741875_1051 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741875
2015-11-23 04:47:39,857 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741876_1052 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741876
2015-11-23 04:47:39,857 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741877_1053 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741877
2015-11-23 04:47:39,858 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741878_1054 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741878
2015-11-23 04:47:39,858 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741879_1055 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741879
2015-11-23 04:47:39,859 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741880_1056 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741880
2015-11-23 04:47:39,859 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741881_1057 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741881
2015-11-23 04:47:39,859 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741882_1058 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741882
2015-11-23 04:47:39,864 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741883_1059 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741883
2015-11-23 04:47:39,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741884_1060 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741884
2015-11-23 04:47:39,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741885_1061 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741885
2015-11-23 04:47:39,866 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741886_1062 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741886
2015-11-23 04:47:39,866 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741887_1063 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741887
2015-11-23 04:47:39,866 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741888_1064 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741888
2015-11-23 04:47:39,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741889_1065 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741889
2015-11-23 04:47:39,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741890_1066 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741890
2015-11-23 04:47:39,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741891_1067 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741891
2015-11-23 04:47:39,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741892_1068 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741892
2015-11-23 04:47:39,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741893_1069 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741893
2015-11-23 04:47:39,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741894_1070 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741894
2015-11-23 04:47:39,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741895_1071 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741895
2015-11-23 04:47:39,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741896_1072 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741896
2015-11-23 04:47:39,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741897_1073 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741897
2015-11-23 04:47:39,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741898_1074 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741898
2015-11-23 04:47:39,878 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741899_1075 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741899
2015-11-23 04:47:39,878 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741900_1076 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741900
2015-11-23 04:47:39,887 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741901_1077 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741901
2015-11-23 04:47:39,888 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741902_1078 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741902
2015-11-23 04:47:39,888 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741903_1079 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741903
2015-11-23 04:47:39,888 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741904_1080 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741904
2015-11-23 04:47:39,889 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741905_1081 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741905
2015-11-23 04:47:39,889 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741906_1082 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741906
2015-11-23 04:47:39,889 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741907_1083 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741907
2015-11-23 04:47:39,890 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741908_1084 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741908
2015-11-23 04:47:39,890 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741909_1085 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741909
2015-11-23 04:47:39,890 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741910_1086 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741910
2015-11-23 04:47:39,891 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741911_1087 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741911
2015-11-23 04:47:39,891 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741912_1088 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741912
2015-11-23 04:47:39,896 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741913_1089 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741913
2015-11-23 04:47:39,897 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741914_1090 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741914
2015-11-23 04:47:39,897 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741915_1091 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741915
2015-11-23 04:47:39,898 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741916_1092 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741916
2015-11-23 04:47:39,898 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741917_1093 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741917
2015-11-23 04:47:39,898 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741918_1094 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741918
2015-11-23 04:47:39,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741919_1095 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741919
2015-11-23 04:47:39,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741920_1096 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741920
2015-11-23 04:47:39,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741921_1097 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741921
2015-11-23 04:47:39,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741922_1098 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741922
2015-11-23 04:47:39,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741923_1099 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741923
2015-11-23 04:47:39,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741924_1100 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741924
2015-11-23 04:47:39,901 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741925_1101 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741925
2015-11-23 04:47:39,901 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741926_1102 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741926
2015-11-23 04:47:39,901 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741927_1103 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741927
2015-11-23 04:47:39,908 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741928_1104 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741928
2015-11-23 04:47:39,908 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741929_1105 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741929
2015-11-23 04:47:39,909 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741930_1106 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741930
2015-11-23 04:47:39,909 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741931_1107 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741931
2015-11-23 04:47:39,910 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741932_1108 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741932
2015-11-23 04:47:39,910 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741933_1109 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741933
2015-11-23 04:47:39,910 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741934_1110 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741934
2015-11-23 04:47:39,911 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741935_1111 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741935
2015-11-23 04:47:39,911 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741936_1112 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741936
2015-11-23 04:47:39,911 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741937_1113 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741937
2015-11-23 04:47:39,912 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741938_1114 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741938
2015-11-23 04:47:39,912 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741939_1115 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741939
2015-11-23 04:47:39,912 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741940_1116 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741940
2015-11-23 04:47:39,913 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741941_1117 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741941
2015-11-23 04:47:39,913 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741942_1118 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741942
2015-11-23 04:47:39,913 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741943_1119 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741943
2015-11-23 04:47:39,920 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741944_1120 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741944
2015-11-23 04:47:39,921 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741945_1121 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741945
2015-11-23 04:47:39,921 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741946_1122 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741946
2015-11-23 04:47:39,922 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741947_1123 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741947
2015-11-23 04:47:39,922 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741948_1124 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741948
2015-11-23 04:47:39,922 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741949_1125 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741949
2015-11-23 04:47:39,923 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741950_1126 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741950
2015-11-23 04:47:39,923 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741951_1127 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741951
2015-11-23 04:47:39,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741952_1128 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741952
2015-11-23 04:47:39,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741953_1129 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741953
2015-11-23 04:47:39,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741954_1130 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741954
2015-11-23 04:47:39,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741955_1131 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741955
2015-11-23 04:47:39,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741956_1132 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741956
2015-11-23 04:47:39,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741957_1133 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741957
2015-11-23 04:47:39,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741958_1134 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741958
2015-11-23 04:47:39,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741959_1135 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741959
2015-11-23 04:47:39,932 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073741960_1136 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir0/blk_1073741960
2015-11-23 04:47:43,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741854_1030 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 04:47:43,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741843_1019 (numBytes=134217728) to /192.168.6.237:50010
2015-11-23 04:47:45,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Can't send invalid block BP-1750158012-192.168.6.248-1444037565733:blk_1073742068_1244
2015-11-23 04:47:54,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742091_1267 src: /192.168.6.237:33186 dest: /192.168.6.249:50010
2015-11-23 04:48:06,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33186, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742091_1267, duration: 12097321329
2015-11-23 04:48:06,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742091_1267, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:48:06,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742092_1268 src: /192.168.6.248:35732 dest: /192.168.6.249:50010
2015-11-23 04:48:18,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35732, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742092_1268, duration: 11867411156
2015-11-23 04:48:18,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742092_1268, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:48:18,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742093_1269 src: /192.168.6.248:35736 dest: /192.168.6.249:50010
2015-11-23 04:48:26,852 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:640ms (threshold=300ms)
2015-11-23 04:48:33,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35736, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742093_1269, duration: 14701299533
2015-11-23 04:48:33,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742093_1269, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:48:33,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742094_1270 src: /192.168.6.237:33197 dest: /192.168.6.249:50010
2015-11-23 04:48:45,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33197, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742094_1270, duration: 11919009660
2015-11-23 04:48:45,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742094_1270, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:48:45,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742095_1271 src: /192.168.6.237:33207 dest: /192.168.6.249:50010
2015-11-23 04:48:57,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33207, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742095_1271, duration: 11869235141
2015-11-23 04:48:57,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742095_1271, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:48:57,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742096_1272 src: /192.168.6.248:35751 dest: /192.168.6.249:50010
2015-11-23 04:49:11,842 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:561ms (threshold=300ms)
2015-11-23 04:49:11,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35751, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742096_1272, duration: 14686298943
2015-11-23 04:49:11,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742096_1272, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:49:11,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742097_1273 src: /192.168.6.248:35758 dest: /192.168.6.249:50010
2015-11-23 04:49:23,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35758, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742097_1273, duration: 11929869527
2015-11-23 04:49:23,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742097_1273, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:49:23,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742098_1274 src: /192.168.6.237:33219 dest: /192.168.6.249:50010
2015-11-23 04:49:35,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33219, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742098_1274, duration: 11858055648
2015-11-23 04:49:35,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742098_1274, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:49:35,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742099_1275 src: /192.168.6.248:35770 dest: /192.168.6.249:50010
2015-11-23 04:49:49,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35770, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742099_1275, duration: 13780106689
2015-11-23 04:49:49,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742099_1275, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:49:49,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742100_1276 src: /192.168.6.248:35774 dest: /192.168.6.249:50010
2015-11-23 04:49:58,436 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 491ms (threshold=300ms)
2015-11-23 04:50:02,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35774, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742100_1276, duration: 12467216035
2015-11-23 04:50:02,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742100_1276, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:50:02,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742101_1277 src: /192.168.6.248:35779 dest: /192.168.6.249:50010
2015-11-23 04:50:08,567 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 578ms (threshold=300ms)
2015-11-23 04:50:14,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35779, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742101_1277, duration: 12553740032
2015-11-23 04:50:14,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742101_1277, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:50:14,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742102_1278 src: /192.168.6.248:35785 dest: /192.168.6.249:50010
2015-11-23 04:50:28,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35785, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742102_1278, duration: 14138393757
2015-11-23 04:50:28,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742102_1278, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:50:28,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742103_1279 src: /192.168.6.237:33231 dest: /192.168.6.249:50010
2015-11-23 04:50:40,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33231, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742103_1279, duration: 11857878797
2015-11-23 04:50:40,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742103_1279, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:50:40,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742104_1280 src: /192.168.6.248:35796 dest: /192.168.6.249:50010
2015-11-23 04:50:52,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35796, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742104_1280, duration: 11859802856
2015-11-23 04:50:52,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742104_1280, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:50:52,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742105_1281 src: /192.168.6.237:33243 dest: /192.168.6.249:50010
2015-11-23 04:51:02,271 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:862ms (threshold=300ms)
2015-11-23 04:51:07,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33243, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742105_1281, duration: 14861318894
2015-11-23 04:51:07,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742105_1281, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:51:07,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742106_1282 src: /192.168.6.248:35809 dest: /192.168.6.249:50010
2015-11-23 04:51:19,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35809, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742106_1282, duration: 11862393790
2015-11-23 04:51:19,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742106_1282, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:51:19,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742107_1283 src: /192.168.6.248:35813 dest: /192.168.6.249:50010
2015-11-23 04:51:31,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35813, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742107_1283, duration: 12206682152
2015-11-23 04:51:31,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742107_1283, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:51:31,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742108_1284 src: /192.168.6.248:35818 dest: /192.168.6.249:50010
2015-11-23 04:51:45,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35818, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742108_1284, duration: 13913304099
2015-11-23 04:51:45,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742108_1284, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:51:45,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742109_1285 src: /192.168.6.237:33256 dest: /192.168.6.249:50010
2015-11-23 04:51:57,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33256, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742109_1285, duration: 11928666811
2015-11-23 04:51:57,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742109_1285, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:51:57,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742110_1286 src: /192.168.6.237:33264 dest: /192.168.6.249:50010
2015-11-23 04:52:01,904 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:487ms (threshold=300ms)
2015-11-23 04:52:10,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33264, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742110_1286, duration: 12326797921
2015-11-23 04:52:10,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742110_1286, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:52:10,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742111_1287 src: /192.168.6.248:35835 dest: /192.168.6.249:50010
2015-11-23 04:52:24,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35835, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742111_1287, duration: 13750362648
2015-11-23 04:52:24,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742111_1287, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:52:24,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742112_1288 src: /192.168.6.237:33269 dest: /192.168.6.249:50010
2015-11-23 04:52:35,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33269, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742112_1288, duration: 11857243513
2015-11-23 04:52:35,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742112_1288, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:52:35,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742113_1289 src: /192.168.6.237:33275 dest: /192.168.6.249:50010
2015-11-23 04:52:47,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33275, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742113_1289, duration: 11857558487
2015-11-23 04:52:47,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742113_1289, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:52:47,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742114_1290 src: /192.168.6.248:35851 dest: /192.168.6.249:50010
2015-11-23 04:53:02,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35851, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742114_1290, duration: 14771172327
2015-11-23 04:53:02,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742114_1290, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:53:02,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742115_1291 src: /192.168.6.248:35856 dest: /192.168.6.249:50010
2015-11-23 04:53:11,210 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 464ms (threshold=300ms)
2015-11-23 04:53:15,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35856, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742115_1291, duration: 12398821782
2015-11-23 04:53:15,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742115_1291, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:53:15,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742116_1292 src: /192.168.6.248:35862 dest: /192.168.6.249:50010
2015-11-23 04:53:21,272 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 526ms (threshold=300ms)
2015-11-23 04:53:27,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35862, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742116_1292, duration: 12583256103
2015-11-23 04:53:27,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742116_1292, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:53:28,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742117_1293 src: /192.168.6.237:33293 dest: /192.168.6.249:50010
2015-11-23 04:53:42,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33293, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742117_1293, duration: 13594753285
2015-11-23 04:53:42,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742117_1293, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:53:42,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742118_1294 src: /192.168.6.248:35873 dest: /192.168.6.249:50010
2015-11-23 04:53:54,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35873, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742118_1294, duration: 11858641917
2015-11-23 04:53:54,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742118_1294, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:53:54,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742119_1295 src: /192.168.6.248:35879 dest: /192.168.6.249:50010
2015-11-23 04:54:06,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35879, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742119_1295, duration: 11860318715
2015-11-23 04:54:06,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742119_1295, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:54:06,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742120_1296 src: /192.168.6.248:35886 dest: /192.168.6.249:50010
2015-11-23 04:54:20,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35886, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742120_1296, duration: 13971586238
2015-11-23 04:54:20,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742120_1296, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:54:20,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742121_1297 src: /192.168.6.237:33305 dest: /192.168.6.249:50010
2015-11-23 04:54:22,559 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:593ms (threshold=300ms)
2015-11-23 04:54:32,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33305, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742121_1297, duration: 12434054192
2015-11-23 04:54:32,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742121_1297, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:54:32,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742122_1298 src: /192.168.6.237:33307 dest: /192.168.6.249:50010
2015-11-23 04:54:44,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33307, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742122_1298, duration: 11859831326
2015-11-23 04:54:44,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742122_1298, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:54:44,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742123_1299 src: /192.168.6.248:35900 dest: /192.168.6.249:50010
2015-11-23 04:54:58,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35900, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742123_1299, duration: 14001221348
2015-11-23 04:54:58,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742123_1299, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:54:58,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742124_1300 src: /192.168.6.237:33318 dest: /192.168.6.249:50010
2015-11-23 04:55:10,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33318, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742124_1300, duration: 11857705299
2015-11-23 04:55:10,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742124_1300, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:55:10,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742125_1301 src: /192.168.6.248:35912 dest: /192.168.6.249:50010
2015-11-23 04:55:12,709 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:677ms (threshold=300ms)
2015-11-23 04:55:23,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35912, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742125_1301, duration: 12535146494
2015-11-23 04:55:23,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742125_1301, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:55:23,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742126_1302 src: /192.168.6.237:33330 dest: /192.168.6.249:50010
2015-11-23 04:55:36,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33330, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742126_1302, duration: 13669641508
2015-11-23 04:55:36,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742126_1302, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:55:36,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742127_1303 src: /192.168.6.237:33331 dest: /192.168.6.249:50010
2015-11-23 04:55:49,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33331, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742127_1303, duration: 12959764209
2015-11-23 04:55:49,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742127_1303, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:55:49,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742128_1304 src: /192.168.6.237:33332 dest: /192.168.6.249:50010
2015-11-23 04:56:01,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33332, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742128_1304, duration: 11858826455
2015-11-23 04:56:01,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742128_1304, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:56:01,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742129_1305 src: /192.168.6.248:35933 dest: /192.168.6.249:50010
2015-11-23 04:56:15,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35933, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742129_1305, duration: 13455290398
2015-11-23 04:56:15,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742129_1305, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:56:15,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742130_1306 src: /192.168.6.237:33344 dest: /192.168.6.249:50010
2015-11-23 04:56:27,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33344, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742130_1306, duration: 11857406474
2015-11-23 04:56:27,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742130_1306, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:56:27,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742131_1307 src: /192.168.6.248:35944 dest: /192.168.6.249:50010
2015-11-23 04:56:36,422 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 427ms (threshold=300ms)
2015-11-23 04:56:41,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35944, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742131_1307, duration: 14361457503
2015-11-23 04:56:41,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742131_1307, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:56:41,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742132_1308 src: /192.168.6.237:33355 dest: /192.168.6.249:50010
2015-11-23 04:56:53,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33355, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742132_1308, duration: 11860014118
2015-11-23 04:56:53,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742132_1308, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:56:53,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742133_1309 src: /192.168.6.237:33357 dest: /192.168.6.249:50010
2015-11-23 04:57:05,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33357, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742133_1309, duration: 11858922850
2015-11-23 04:57:05,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742133_1309, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:57:05,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742134_1310 src: /192.168.6.237:33358 dest: /192.168.6.249:50010
2015-11-23 04:57:19,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33358, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742134_1310, duration: 13531259653
2015-11-23 04:57:19,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742134_1310, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:57:19,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742135_1311 src: /192.168.6.237:33366 dest: /192.168.6.249:50010
2015-11-23 04:57:31,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33366, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742135_1311, duration: 11862017557
2015-11-23 04:57:31,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742135_1311, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:57:31,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742136_1312 src: /192.168.6.248:35971 dest: /192.168.6.249:50010
2015-11-23 04:57:32,922 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:487ms (threshold=300ms)
2015-11-23 04:57:43,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35971, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742136_1312, duration: 12340226346
2015-11-23 04:57:43,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742136_1312, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:57:43,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742137_1313 src: /192.168.6.248:35977 dest: /192.168.6.249:50010
2015-11-23 04:57:56,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35977, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742137_1313, duration: 13368238472
2015-11-23 04:57:56,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742137_1313, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:57:56,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742138_1314 src: /192.168.6.248:35983 dest: /192.168.6.249:50010
2015-11-23 04:58:09,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35983, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742138_1314, duration: 12814474198
2015-11-23 04:58:09,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742138_1314, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:58:09,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742139_1315 src: /192.168.6.237:33382 dest: /192.168.6.249:50010
2015-11-23 04:58:21,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33382, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742139_1315, duration: 12130862181
2015-11-23 04:58:21,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742139_1315, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:58:22,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742140_1316 src: /192.168.6.248:35996 dest: /192.168.6.249:50010
2015-11-23 04:58:31,582 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 611ms (threshold=300ms)
2015-11-23 04:58:36,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:35996, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742140_1316, duration: 13704419175
2015-11-23 04:58:36,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742140_1316, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:58:36,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742141_1317 src: /192.168.6.237:33388 dest: /192.168.6.249:50010
2015-11-23 04:58:48,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33388, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742141_1317, duration: 11858292846
2015-11-23 04:58:48,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742141_1317, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:58:48,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742142_1318 src: /192.168.6.248:36006 dest: /192.168.6.249:50010
2015-11-23 04:59:02,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36006, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742142_1318, duration: 13772696771
2015-11-23 04:59:02,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742142_1318, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:59:02,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742143_1319 src: /192.168.6.237:33395 dest: /192.168.6.249:50010
2015-11-23 04:59:03,588 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:650ms (threshold=300ms)
2015-11-23 04:59:14,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33395, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742143_1319, duration: 12489713764
2015-11-23 04:59:14,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742143_1319, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:59:14,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742144_1320 src: /192.168.6.248:36017 dest: /192.168.6.249:50010
2015-11-23 04:59:26,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36017, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742144_1320, duration: 11858916778
2015-11-23 04:59:26,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742144_1320, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 04:59:26,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742145_1321 src: /192.168.6.237:33407 dest: /192.168.6.249:50010
2015-11-23 04:59:41,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33407, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742145_1321, duration: 14554101418
2015-11-23 04:59:41,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742145_1321, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:59:41,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742146_1322 src: /192.168.6.237:33408 dest: /192.168.6.249:50010
2015-11-23 04:59:53,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33408, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742146_1322, duration: 11866877623
2015-11-23 04:59:53,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742146_1322, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 04:59:53,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742147_1323 src: /192.168.6.248:36040 dest: /192.168.6.249:50010
2015-11-23 04:59:56,688 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 500ms (threshold=300ms)
2015-11-23 05:00:05,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36040, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742147_1323, duration: 12498424247
2015-11-23 05:00:05,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742147_1323, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:00:05,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742148_1324 src: /192.168.6.248:36047 dest: /192.168.6.249:50010
2015-11-23 05:00:19,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36047, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742148_1324, duration: 13401392586
2015-11-23 05:00:19,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742148_1324, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:00:19,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742149_1325 src: /192.168.6.248:36051 dest: /192.168.6.249:50010
2015-11-23 05:00:31,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36051, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742149_1325, duration: 11859585965
2015-11-23 05:00:31,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742149_1325, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:00:31,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742150_1326 src: /192.168.6.248:36055 dest: /192.168.6.249:50010
2015-11-23 05:00:43,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36055, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742150_1326, duration: 11859806002
2015-11-23 05:00:43,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742150_1326, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:00:44,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742151_1327 src: /192.168.6.248:36061 dest: /192.168.6.249:50010
2015-11-23 05:00:57,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36061, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742151_1327, duration: 12941509011
2015-11-23 05:00:57,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742151_1327, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:00:57,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742152_1328 src: /192.168.6.237:33431 dest: /192.168.6.249:50010
2015-11-23 05:01:08,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33431, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742152_1328, duration: 11857095143
2015-11-23 05:01:08,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742152_1328, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:01:08,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742153_1329 src: /192.168.6.248:36074 dest: /192.168.6.249:50010
2015-11-23 05:01:23,944 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:693ms (threshold=300ms)
2015-11-23 05:01:24,548 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:587ms (threshold=300ms)
2015-11-23 05:01:24,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36074, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742153_1329, duration: 15596161883
2015-11-23 05:01:24,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742153_1329, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:01:24,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742154_1330 src: /192.168.6.248:36080 dest: /192.168.6.249:50010
2015-11-23 05:01:36,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36080, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742154_1330, duration: 11861553255
2015-11-23 05:01:36,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742154_1330, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:01:36,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742155_1331 src: /192.168.6.248:36086 dest: /192.168.6.249:50010
2015-11-23 05:01:41,844 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 488ms (threshold=300ms)
2015-11-23 05:01:48,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36086, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742155_1331, duration: 12481109598
2015-11-23 05:01:48,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742155_1331, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:01:49,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742156_1332 src: /192.168.6.237:33443 dest: /192.168.6.249:50010
2015-11-23 05:02:03,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33443, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742156_1332, duration: 14476713262
2015-11-23 05:02:03,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742156_1332, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:02:03,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742157_1333 src: /192.168.6.248:36095 dest: /192.168.6.249:50010
2015-11-23 05:02:15,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36095, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742157_1333, duration: 11858742669
2015-11-23 05:02:15,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742157_1333, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:02:15,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742158_1334 src: /192.168.6.248:36101 dest: /192.168.6.249:50010
2015-11-23 05:02:27,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36101, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742158_1334, duration: 11865026915
2015-11-23 05:02:27,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742158_1334, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:02:27,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742159_1335 src: /192.168.6.248:36106 dest: /192.168.6.249:50010
2015-11-23 05:02:40,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36106, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742159_1335, duration: 13599963702
2015-11-23 05:02:40,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742159_1335, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:02:41,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742160_1336 src: /192.168.6.248:36112 dest: /192.168.6.249:50010
2015-11-23 05:02:52,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36112, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742160_1336, duration: 11858734819
2015-11-23 05:02:52,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742160_1336, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:02:52,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742161_1337 src: /192.168.6.237:33466 dest: /192.168.6.249:50010
2015-11-23 05:03:05,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33466, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742161_1337, duration: 12504994576
2015-11-23 05:03:05,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742161_1337, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:03:05,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742162_1338 src: /192.168.6.237:33467 dest: /192.168.6.249:50010
2015-11-23 05:03:19,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33467, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742162_1338, duration: 14104050094
2015-11-23 05:03:19,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742162_1338, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:03:19,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742163_1339 src: /192.168.6.237:33475 dest: /192.168.6.249:50010
2015-11-23 05:03:31,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33475, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742163_1339, duration: 11857946377
2015-11-23 05:03:31,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742163_1339, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:03:31,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742164_1340 src: /192.168.6.237:33480 dest: /192.168.6.249:50010
2015-11-23 05:03:34,048 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:605ms (threshold=300ms)
2015-11-23 05:03:43,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33480, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742164_1340, duration: 12450936517
2015-11-23 05:03:43,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742164_1340, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:03:43,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742165_1341 src: /192.168.6.237:33481 dest: /192.168.6.249:50010
2015-11-23 05:03:58,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33481, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742165_1341, duration: 14249253078
2015-11-23 05:03:58,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742165_1341, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:03:58,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742166_1342 src: /192.168.6.248:36151 dest: /192.168.6.249:50010
2015-11-23 05:04:10,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36151, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742166_1342, duration: 11868083897
2015-11-23 05:04:10,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742166_1342, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:04:10,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742167_1343 src: /192.168.6.237:33482 dest: /192.168.6.249:50010
2015-11-23 05:04:22,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33482, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742167_1343, duration: 12037910569
2015-11-23 05:04:22,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742167_1343, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:04:22,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742168_1344 src: /192.168.6.237:33483 dest: /192.168.6.249:50010
2015-11-23 05:04:35,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33483, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742168_1344, duration: 13182160965
2015-11-23 05:04:35,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742168_1344, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:04:35,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742169_1345 src: /192.168.6.248:36170 dest: /192.168.6.249:50010
2015-11-23 05:04:39,138 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:616ms (threshold=300ms)
2015-11-23 05:04:48,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36170, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742169_1345, duration: 12472407325
2015-11-23 05:04:48,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742169_1345, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:04:48,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742170_1346 src: /192.168.6.237:33484 dest: /192.168.6.249:50010
2015-11-23 05:05:01,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33484, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742170_1346, duration: 13563631491
2015-11-23 05:05:01,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742170_1346, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:05:02,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742171_1347 src: /192.168.6.237:33485 dest: /192.168.6.249:50010
2015-11-23 05:05:15,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33485, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742171_1347, duration: 13413624031
2015-11-23 05:05:15,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742171_1347, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:05:15,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742172_1348 src: /192.168.6.248:36185 dest: /192.168.6.249:50010
2015-11-23 05:05:22,107 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 509ms (threshold=300ms)
2015-11-23 05:05:27,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36185, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742172_1348, duration: 12505545027
2015-11-23 05:05:27,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742172_1348, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:05:28,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742173_1349 src: /192.168.6.237:33486 dest: /192.168.6.249:50010
2015-11-23 05:05:41,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33486, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742173_1349, duration: 13733574507
2015-11-23 05:05:41,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742173_1349, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:05:42,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742174_1350 src: /192.168.6.237:33487 dest: /192.168.6.249:50010
2015-11-23 05:05:53,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33487, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742174_1350, duration: 11857594833
2015-11-23 05:05:53,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742174_1350, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:05:53,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742175_1351 src: /192.168.6.237:33488 dest: /192.168.6.249:50010
2015-11-23 05:06:05,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33488, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742175_1351, duration: 11859023813
2015-11-23 05:06:05,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742175_1351, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:06:05,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742176_1352 src: /192.168.6.248:36209 dest: /192.168.6.249:50010
2015-11-23 05:06:19,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36209, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742176_1352, duration: 13540750674
2015-11-23 05:06:19,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742176_1352, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:06:19,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742177_1353 src: /192.168.6.248:36213 dest: /192.168.6.249:50010
2015-11-23 05:06:31,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36213, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742177_1353, duration: 11859936310
2015-11-23 05:06:31,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742177_1353, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:06:31,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742178_1354 src: /192.168.6.248:36217 dest: /192.168.6.249:50010
2015-11-23 05:06:39,624 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:351ms (threshold=300ms)
2015-11-23 05:06:43,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36217, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742178_1354, duration: 12206597171
2015-11-23 05:06:43,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742178_1354, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:06:43,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742179_1355 src: /192.168.6.237:33489 dest: /192.168.6.249:50010
2015-11-23 05:06:57,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33489, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742179_1355, duration: 14350919736
2015-11-23 05:06:57,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742179_1355, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:06:57,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742180_1356 src: /192.168.6.248:36229 dest: /192.168.6.249:50010
2015-11-23 05:07:09,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36229, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742180_1356, duration: 11860613771
2015-11-23 05:07:09,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742180_1356, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:07:09,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742181_1357 src: /192.168.6.237:33490 dest: /192.168.6.249:50010
2015-11-23 05:07:21,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33490, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742181_1357, duration: 11936341657
2015-11-23 05:07:21,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742181_1357, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:07:22,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742182_1358 src: /192.168.6.237:33491 dest: /192.168.6.249:50010
2015-11-23 05:07:35,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33491, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742182_1358, duration: 13023722486
2015-11-23 05:07:35,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742182_1358, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:07:35,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742183_1359 src: /192.168.6.237:33492 dest: /192.168.6.249:50010
2015-11-23 05:07:47,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33492, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742183_1359, duration: 11858342319
2015-11-23 05:07:47,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742183_1359, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:07:47,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742184_1360 src: /192.168.6.237:33493 dest: /192.168.6.249:50010
2015-11-23 05:08:01,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33493, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742184_1360, duration: 14073185665
2015-11-23 05:08:01,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742184_1360, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:08:01,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742185_1361 src: /192.168.6.237:33494 dest: /192.168.6.249:50010
2015-11-23 05:08:10,345 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:646ms (threshold=300ms)
2015-11-23 05:08:14,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33494, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742185_1361, duration: 12487346482
2015-11-23 05:08:14,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742185_1361, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:08:14,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742186_1362 src: /192.168.6.248:36263 dest: /192.168.6.249:50010
2015-11-23 05:08:26,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36263, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742186_1362, duration: 11858966744
2015-11-23 05:08:26,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742186_1362, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:08:26,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742187_1363 src: /192.168.6.237:33495 dest: /192.168.6.249:50010
2015-11-23 05:08:40,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33495, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742187_1363, duration: 13768746014
2015-11-23 05:08:40,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742187_1363, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:08:40,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742188_1364 src: /192.168.6.248:36274 dest: /192.168.6.249:50010
2015-11-23 05:08:52,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36274, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742188_1364, duration: 11859103030
2015-11-23 05:08:52,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742188_1364, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:08:52,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742189_1365 src: /192.168.6.248:36280 dest: /192.168.6.249:50010
2015-11-23 05:09:03,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36280, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742189_1365, duration: 11862601602
2015-11-23 05:09:03,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742189_1365, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:09:04,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742190_1366 src: /192.168.6.248:36283 dest: /192.168.6.249:50010
2015-11-23 05:09:18,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36283, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742190_1366, duration: 13991685114
2015-11-23 05:09:18,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742190_1366, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:09:18,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742191_1367 src: /192.168.6.237:33496 dest: /192.168.6.249:50010
2015-11-23 05:09:29,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33496, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742191_1367, duration: 11858505913
2015-11-23 05:09:29,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742191_1367, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:09:29,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742192_1368 src: /192.168.6.248:36294 dest: /192.168.6.249:50010
2015-11-23 05:09:39,076 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 981ms (threshold=300ms)
2015-11-23 05:09:42,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36294, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742192_1368, duration: 12976787849
2015-11-23 05:09:42,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742192_1368, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:09:42,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742193_1369 src: /192.168.6.248:36300 dest: /192.168.6.249:50010
2015-11-23 05:09:50,498 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:688ms (threshold=300ms)
2015-11-23 05:09:58,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36300, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742193_1369, duration: 15068146334
2015-11-23 05:09:58,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742193_1369, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:09:58,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742194_1370 src: /192.168.6.248:36306 dest: /192.168.6.249:50010
2015-11-23 05:10:09,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36306, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742194_1370, duration: 11874296384
2015-11-23 05:10:09,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742194_1370, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:10:09,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742195_1371 src: /192.168.6.248:36313 dest: /192.168.6.249:50010
2015-11-23 05:10:21,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36313, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742195_1371, duration: 11860479402
2015-11-23 05:10:21,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742195_1371, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:10:21,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742196_1372 src: /192.168.6.237:33497 dest: /192.168.6.249:50010
2015-11-23 05:10:36,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33497, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742196_1372, duration: 14367267353
2015-11-23 05:10:36,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742196_1372, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:10:36,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742197_1373 src: /192.168.6.237:33498 dest: /192.168.6.249:50010
2015-11-23 05:10:48,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33498, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742197_1373, duration: 11858594801
2015-11-23 05:10:48,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742197_1373, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:10:48,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742198_1374 src: /192.168.6.248:36329 dest: /192.168.6.249:50010
2015-11-23 05:11:00,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36329, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742198_1374, duration: 12214598205
2015-11-23 05:11:00,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742198_1374, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:11:00,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742199_1375 src: /192.168.6.248:36333 dest: /192.168.6.249:50010
2015-11-23 05:11:13,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36333, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742199_1375, duration: 13295023299
2015-11-23 05:11:13,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742199_1375, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:11:13,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742200_1376 src: /192.168.6.237:33499 dest: /192.168.6.249:50010
2015-11-23 05:11:25,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33499, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742200_1376, duration: 11858727384
2015-11-23 05:11:25,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742200_1376, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:11:25,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742201_1377 src: /192.168.6.237:33500 dest: /192.168.6.249:50010
2015-11-23 05:11:39,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33500, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742201_1377, duration: 13625568141
2015-11-23 05:11:39,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742201_1377, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:11:39,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742202_1378 src: /192.168.6.248:36352 dest: /192.168.6.249:50010
2015-11-23 05:11:40,624 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:612ms (threshold=300ms)
2015-11-23 05:11:51,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36352, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742202_1378, duration: 12470687931
2015-11-23 05:11:51,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742202_1378, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:11:51,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742203_1379 src: /192.168.6.237:33501 dest: /192.168.6.249:50010
2015-11-23 05:12:03,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33501, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742203_1379, duration: 11859979868
2015-11-23 05:12:03,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742203_1379, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:12:03,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742204_1380 src: /192.168.6.237:33502 dest: /192.168.6.249:50010
2015-11-23 05:12:17,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33502, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742204_1380, duration: 14056615537
2015-11-23 05:12:17,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742204_1380, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:12:17,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742205_1381 src: /192.168.6.248:36374 dest: /192.168.6.249:50010
2015-11-23 05:12:29,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36374, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742205_1381, duration: 11858292643
2015-11-23 05:12:29,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742205_1381, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:12:29,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742206_1382 src: /192.168.6.248:36378 dest: /192.168.6.249:50010
2015-11-23 05:12:41,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36378, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742206_1382, duration: 11862135001
2015-11-23 05:12:41,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742206_1382, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:12:41,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742207_1383 src: /192.168.6.237:33503 dest: /192.168.6.249:50010
2015-11-23 05:12:56,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33503, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742207_1383, duration: 14404592901
2015-11-23 05:12:56,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742207_1383, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:12:56,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742208_1384 src: /192.168.6.237:33504 dest: /192.168.6.249:50010
2015-11-23 05:13:08,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33504, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742208_1384, duration: 11863698105
2015-11-23 05:13:08,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742208_1384, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:13:08,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742209_1385 src: /192.168.6.248:36397 dest: /192.168.6.249:50010
2015-11-23 05:13:19,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36397, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742209_1385, duration: 11861142323
2015-11-23 05:13:19,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742209_1385, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:13:20,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742210_1386 src: /192.168.6.248:36401 dest: /192.168.6.249:50010
2015-11-23 05:13:33,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36401, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742210_1386, duration: 12964521494
2015-11-23 05:13:33,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742210_1386, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:13:33,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742211_1387 src: /192.168.6.237:33505 dest: /192.168.6.249:50010
2015-11-23 05:13:45,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33505, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742211_1387, duration: 11857739970
2015-11-23 05:13:45,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742211_1387, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:13:45,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742212_1388 src: /192.168.6.248:36411 dest: /192.168.6.249:50010
2015-11-23 05:13:59,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36411, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742212_1388, duration: 14346167223
2015-11-23 05:13:59,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742212_1388, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:13:59,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742213_1389 src: /192.168.6.248:36416 dest: /192.168.6.249:50010
2015-11-23 05:14:01,215 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:751ms (threshold=300ms)
2015-11-23 05:14:12,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36416, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742213_1389, duration: 12614873945
2015-11-23 05:14:12,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742213_1389, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:14:12,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742214_1390 src: /192.168.6.248:36423 dest: /192.168.6.249:50010
2015-11-23 05:14:24,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36423, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742214_1390, duration: 11860738103
2015-11-23 05:14:24,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742214_1390, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:14:24,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742215_1391 src: /192.168.6.248:36429 dest: /192.168.6.249:50010
2015-11-23 05:14:32,677 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 521ms (threshold=300ms)
2015-11-23 05:14:38,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36429, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742215_1391, duration: 13896606665
2015-11-23 05:14:38,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742215_1391, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:14:38,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742216_1392 src: /192.168.6.237:33506 dest: /192.168.6.249:50010
2015-11-23 05:14:50,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33506, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742216_1392, duration: 11858852532
2015-11-23 05:14:50,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742216_1392, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:14:50,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742217_1393 src: /192.168.6.248:36439 dest: /192.168.6.249:50010
2015-11-23 05:15:03,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36439, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742217_1393, duration: 13049574825
2015-11-23 05:15:03,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742217_1393, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:15:03,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742218_1394 src: /192.168.6.248:36444 dest: /192.168.6.249:50010
2015-11-23 05:15:17,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36444, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742218_1394, duration: 14112718785
2015-11-23 05:15:17,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742218_1394, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:15:17,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742219_1395 src: /192.168.6.237:33507 dest: /192.168.6.249:50010
2015-11-23 05:15:29,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:33507, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742219_1395, duration: 11857364524
2015-11-23 05:15:29,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742219_1395, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-23 05:15:29,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742220_1396 src: /192.168.6.248:36455 dest: /192.168.6.249:50010
2015-11-23 05:15:33,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:36455, dest: /192.168.6.249:50010, bytes: 51875046, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_381440227_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742220_1396, duration: 4589052961
2015-11-23 05:15:33,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742220_1396, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-23 05:35:45,774 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-23 05:35:49,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-23 05:35:50,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-23 05:35:51,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-23 05:35:51,892 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-23 05:35:51,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-24 04:52:24,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-24 04:52:24,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-24 04:52:25,829 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-24 04:52:25,950 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-24 04:52:25,950 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-24 04:52:25,955 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-24 04:52:25,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-24 04:52:25,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-24 04:52:26,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-24 04:52:26,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-24 04:52:26,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-24 04:52:26,216 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-24 04:52:26,225 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-24 04:52:26,259 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-24 04:52:26,264 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-24 04:52:26,267 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-24 04:52:26,267 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-24 04:52:26,267 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-24 04:52:26,297 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37274
2015-11-24 04:52:26,297 INFO org.mortbay.log: jetty-6.1.26
2015-11-24 04:52:26,501 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37274
2015-11-24 04:52:26,642 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-24 04:52:26,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-24 04:52:26,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-24 04:52:27,165 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-24 04:52:27,207 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-24 04:52:27,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-24 04:52:27,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-24 04:52:27,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-24 04:52:27,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-24 04:52:27,550 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-24 04:52:27,550 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-24 04:52:28,027 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 11981@rushikesh2
2015-11-24 04:52:28,105 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-24 04:52:28,105 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-24 04:52:28,106 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-24 04:52:28,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-24 04:52:28,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-24 04:52:28,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-24 04:52:28,217 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-24 04:52:28,217 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-24 04:52:28,218 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-24 04:52:28,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 39ms
2015-11-24 04:52:28,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 40ms
2015-11-24 04:52:28,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-24 04:52:28,282 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 25ms
2015-11-24 04:52:28,282 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 25ms
2015-11-24 04:52:28,497 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1187157562 ms.
2015-11-24 04:52:28,499 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448331944499 with interval 21600000
2015-11-24 04:52:28,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-24 04:52:28,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-24 04:52:28,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-24 04:52:28,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1391
2015-11-24 04:52:28,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-24 04:52:28,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe333596c893,  containing 1 storage report(s), of which we sent 1. The reports had 132 total blocks and used 1 RPC(s). This took 4 msec to generate and 43 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-24 04:52:28,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-24 04:58:31,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397 src: /192.168.6.237:41220 dest: /192.168.6.249:50010
2015-11-24 04:58:43,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41220, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397, duration: 12505598747
2015-11-24 04:58:43,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742221_1397, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 04:58:43,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398 src: /192.168.6.248:54937 dest: /192.168.6.249:50010
2015-11-24 04:58:55,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:54937, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398, duration: 11986611673
2015-11-24 04:58:55,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742222_1398, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 04:58:55,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399 src: /192.168.6.237:41232 dest: /192.168.6.249:50010
2015-11-24 04:59:08,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41232, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399, duration: 12854682572
2015-11-24 04:59:08,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742223_1399, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 04:59:08,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400 src: /192.168.6.248:54954 dest: /192.168.6.249:50010
2015-11-24 04:59:20,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:54954, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400, duration: 11942038734
2015-11-24 04:59:20,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742224_1400, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 04:59:20,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401 src: /192.168.6.248:54960 dest: /192.168.6.249:50010
2015-11-24 04:59:32,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:54960, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401, duration: 11883668907
2015-11-24 04:59:32,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742225_1401, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 04:59:32,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402 src: /192.168.6.237:41244 dest: /192.168.6.249:50010
2015-11-24 04:59:46,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41244, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402, duration: 13654525927
2015-11-24 04:59:46,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742226_1402, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 04:59:46,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403 src: /192.168.6.237:41245 dest: /192.168.6.249:50010
2015-11-24 04:59:58,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41245, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403, duration: 12004646302
2015-11-24 04:59:58,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742227_1403, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 04:59:58,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404 src: /192.168.6.248:54975 dest: /192.168.6.249:50010
2015-11-24 05:00:10,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:54975, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404, duration: 12008658894
2015-11-24 05:00:10,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742228_1404, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:00:10,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405 src: /192.168.6.237:41257 dest: /192.168.6.249:50010
2015-11-24 05:00:23,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41257, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405, duration: 11990190231
2015-11-24 05:00:23,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742229_1405, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:00:23,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406 src: /192.168.6.248:54986 dest: /192.168.6.249:50010
2015-11-24 05:00:35,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:54986, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406, duration: 11858229773
2015-11-24 05:00:35,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742230_1406, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:00:35,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407 src: /192.168.6.248:54991 dest: /192.168.6.249:50010
2015-11-24 05:00:48,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:54991, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407, duration: 13145276498
2015-11-24 05:00:48,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742231_1407, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:00:48,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408 src: /192.168.6.237:41268 dest: /192.168.6.249:50010
2015-11-24 05:01:01,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41268, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408, duration: 12619554741
2015-11-24 05:01:01,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742232_1408, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:01:01,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409 src: /192.168.6.248:55002 dest: /192.168.6.249:50010
2015-11-24 05:01:13,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55002, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409, duration: 12292449195
2015-11-24 05:01:13,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742233_1409, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:01:13,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410 src: /192.168.6.237:41270 dest: /192.168.6.249:50010
2015-11-24 05:01:25,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41270, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410, duration: 12412194649
2015-11-24 05:01:25,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742234_1410, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:01:25,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411 src: /192.168.6.237:41278 dest: /192.168.6.249:50010
2015-11-24 05:01:37,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41278, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411, duration: 11886030297
2015-11-24 05:01:37,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742235_1411, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:01:37,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412 src: /192.168.6.237:41283 dest: /192.168.6.249:50010
2015-11-24 05:01:41,335 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:453ms (threshold=300ms)
2015-11-24 05:01:50,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41283, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412, duration: 12682016903
2015-11-24 05:01:50,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742236_1412, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:01:50,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413 src: /192.168.6.248:55025 dest: /192.168.6.249:50010
2015-11-24 05:02:03,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55025, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413, duration: 13292573253
2015-11-24 05:02:03,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742237_1413, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:02:03,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414 src: /192.168.6.248:55029 dest: /192.168.6.249:50010
2015-11-24 05:02:15,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55029, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414, duration: 12064276446
2015-11-24 05:02:15,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742238_1414, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:02:15,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415 src: /192.168.6.248:55034 dest: /192.168.6.249:50010
2015-11-24 05:02:27,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55034, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415, duration: 12025213940
2015-11-24 05:02:27,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742239_1415, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:02:27,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416 src: /192.168.6.248:55045 dest: /192.168.6.249:50010
2015-11-24 05:02:37,154 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:623ms (threshold=300ms)
2015-11-24 05:02:40,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55045, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416, duration: 12897491119
2015-11-24 05:02:40,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742240_1416, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:02:40,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417 src: /192.168.6.237:41297 dest: /192.168.6.249:50010
2015-11-24 05:02:52,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41297, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417, duration: 11989070448
2015-11-24 05:02:52,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742241_1417, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:02:52,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418 src: /192.168.6.248:55058 dest: /192.168.6.249:50010
2015-11-24 05:03:04,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55058, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418, duration: 12059487355
2015-11-24 05:03:04,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742242_1418, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:03:05,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419 src: /192.168.6.248:55062 dest: /192.168.6.249:50010
2015-11-24 05:03:17,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55062, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419, duration: 11871164021
2015-11-24 05:03:17,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742243_1419, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:03:17,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420 src: /192.168.6.248:55067 dest: /192.168.6.249:50010
2015-11-24 05:03:29,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55067, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420, duration: 11858954244
2015-11-24 05:03:29,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742244_1420, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:03:29,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421 src: /192.168.6.237:41317 dest: /192.168.6.249:50010
2015-11-24 05:03:32,458 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:378ms (threshold=300ms)
2015-11-24 05:03:38,085 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:626ms (threshold=300ms)
2015-11-24 05:03:42,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41317, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421, duration: 13693493459
2015-11-24 05:03:42,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742245_1421, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:03:42,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422 src: /192.168.6.237:41319 dest: /192.168.6.249:50010
2015-11-24 05:03:54,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41319, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422, duration: 11880357371
2015-11-24 05:03:54,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742246_1422, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:03:54,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423 src: /192.168.6.237:41320 dest: /192.168.6.249:50010
2015-11-24 05:04:06,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41320, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423, duration: 11985355053
2015-11-24 05:04:06,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742247_1423, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:04:07,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424 src: /192.168.6.248:55092 dest: /192.168.6.249:50010
2015-11-24 05:04:20,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55092, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424, duration: 12548945540
2015-11-24 05:04:20,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742248_1424, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:04:20,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425 src: /192.168.6.248:55099 dest: /192.168.6.249:50010
2015-11-24 05:04:32,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55099, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425, duration: 12014704857
2015-11-24 05:04:32,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742249_1425, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:04:32,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426 src: /192.168.6.237:41332 dest: /192.168.6.249:50010
2015-11-24 05:04:44,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41332, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426, duration: 12641239369
2015-11-24 05:04:44,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742250_1426, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:04:44,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427 src: /192.168.6.248:55110 dest: /192.168.6.249:50010
2015-11-24 05:04:56,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55110, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427, duration: 11898972076
2015-11-24 05:04:56,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742251_1427, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:04:56,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428 src: /192.168.6.248:55116 dest: /192.168.6.249:50010
2015-11-24 05:05:08,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55116, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428, duration: 12056269995
2015-11-24 05:05:08,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742252_1428, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:05:08,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429 src: /192.168.6.237:41344 dest: /192.168.6.249:50010
2015-11-24 05:05:21,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41344, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429, duration: 12735705981
2015-11-24 05:05:21,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742253_1429, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:05:21,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430 src: /192.168.6.237:41348 dest: /192.168.6.249:50010
2015-11-24 05:05:33,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41348, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430, duration: 12001334737
2015-11-24 05:05:33,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742254_1430, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:05:33,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431 src: /192.168.6.237:41357 dest: /192.168.6.249:50010
2015-11-24 05:05:45,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41357, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431, duration: 12058004905
2015-11-24 05:05:45,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742255_1431, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:05:45,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432 src: /192.168.6.248:55140 dest: /192.168.6.249:50010
2015-11-24 05:05:57,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55140, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432, duration: 11905374816
2015-11-24 05:05:57,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742256_1432, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:05:57,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 src: /192.168.6.237:41358 dest: /192.168.6.249:50010
2015-11-24 05:06:09,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41358, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433, duration: 11890285567
2015-11-24 05:06:09,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:06:09,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434 src: /192.168.6.248:55150 dest: /192.168.6.249:50010
2015-11-24 05:06:10,673 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 456ms (threshold=300ms)
2015-11-24 05:06:23,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55150, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434, duration: 13839497870
2015-11-24 05:06:23,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742258_1434, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:06:23,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435 src: /192.168.6.248:55156 dest: /192.168.6.249:50010
2015-11-24 05:06:35,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55156, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435, duration: 11945193279
2015-11-24 05:06:35,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742259_1435, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:06:35,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436 src: /192.168.6.237:41371 dest: /192.168.6.249:50010
2015-11-24 05:06:47,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41371, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436, duration: 11880209796
2015-11-24 05:06:47,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742260_1436, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:06:48,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437 src: /192.168.6.248:55166 dest: /192.168.6.249:50010
2015-11-24 05:07:09,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55166, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437, duration: 21745701514
2015-11-24 05:07:09,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742261_1437, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:07:09,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438 src: /192.168.6.237:41383 dest: /192.168.6.249:50010
2015-11-24 05:07:33,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41383, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438, duration: 23311102424
2015-11-24 05:07:33,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742262_1438, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:07:33,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439 src: /192.168.6.248:55184 dest: /192.168.6.249:50010
2015-11-24 05:07:46,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55184, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439, duration: 12916626563
2015-11-24 05:07:46,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742263_1439, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:07:46,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440 src: /192.168.6.237:41395 dest: /192.168.6.249:50010
2015-11-24 05:07:58,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41395, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440, duration: 12043112756
2015-11-24 05:07:58,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742264_1440, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:07:58,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441 src: /192.168.6.237:41396 dest: /192.168.6.249:50010
2015-11-24 05:08:08,924 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:760ms (threshold=300ms)
2015-11-24 05:08:12,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41396, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441, duration: 13724787388
2015-11-24 05:08:12,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742265_1441, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:08:12,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442 src: /192.168.6.237:41408 dest: /192.168.6.249:50010
2015-11-24 05:08:35,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41408, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442, duration: 23265179443
2015-11-24 05:08:35,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742266_1442, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:08:35,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443 src: /192.168.6.237:41409 dest: /192.168.6.249:50010
2015-11-24 05:08:59,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41409, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443, duration: 24199358570
2015-11-24 05:08:59,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742267_1443, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:08:59,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444 src: /192.168.6.248:55219 dest: /192.168.6.249:50010
2015-11-24 05:09:11,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55219, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444, duration: 12169683266
2015-11-24 05:09:11,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742268_1444, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:09:12,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445 src: /192.168.6.248:55225 dest: /192.168.6.249:50010
2015-11-24 05:09:24,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55225, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445, duration: 12204111684
2015-11-24 05:09:24,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742269_1445, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:09:24,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446 src: /192.168.6.237:41428 dest: /192.168.6.249:50010
2015-11-24 05:09:48,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41428, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446, duration: 24121181405
2015-11-24 05:09:48,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742270_1446, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:09:48,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447 src: /192.168.6.248:55239 dest: /192.168.6.249:50010
2015-11-24 05:10:01,725 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 380ms (threshold=300ms)
2015-11-24 05:10:02,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55239, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447, duration: 13490130407
2015-11-24 05:10:02,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742271_1447, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:10:02,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448 src: /192.168.6.248:55246 dest: /192.168.6.249:50010
2015-11-24 05:10:25,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55246, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448, duration: 23310425066
2015-11-24 05:10:25,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742272_1448, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:10:25,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449 src: /192.168.6.248:55254 dest: /192.168.6.249:50010
2015-11-24 05:10:40,756 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 702ms (threshold=300ms)
2015-11-24 05:10:50,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55254, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449, duration: 24242658179
2015-11-24 05:10:50,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742273_1449, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:10:50,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450 src: /192.168.6.248:55264 dest: /192.168.6.249:50010
2015-11-24 05:11:13,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55264, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450, duration: 23035779184
2015-11-24 05:11:13,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742274_1450, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:11:13,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451 src: /192.168.6.237:41456 dest: /192.168.6.249:50010
2015-11-24 05:11:37,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41456, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451, duration: 24024310170
2015-11-24 05:11:37,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742275_1451, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:11:37,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452 src: /192.168.6.248:55282 dest: /192.168.6.249:50010
2015-11-24 05:11:43,149 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 467ms (threshold=300ms)
2015-11-24 05:11:51,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55282, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452, duration: 14571557263
2015-11-24 05:11:51,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742276_1452, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:11:51,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453 src: /192.168.6.237:41468 dest: /192.168.6.249:50010
2015-11-24 05:12:17,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41468, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453, duration: 25720258302
2015-11-24 05:12:17,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742277_1453, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:12:17,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454 src: /192.168.6.237:41482 dest: /192.168.6.249:50010
2015-11-24 05:12:30,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41482, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454, duration: 13397096717
2015-11-24 05:12:30,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742278_1454, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:12:31,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455 src: /192.168.6.248:55302 dest: /192.168.6.249:50010
2015-11-24 05:12:54,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55302, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455, duration: 23567923909
2015-11-24 05:12:54,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742279_1455, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:12:54,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456 src: /192.168.6.248:55311 dest: /192.168.6.249:50010
2015-11-24 05:13:17,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55311, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456, duration: 23180138307
2015-11-24 05:13:17,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742280_1456, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:13:17,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457 src: /192.168.6.237:41494 dest: /192.168.6.249:50010
2015-11-24 05:13:30,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41494, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457, duration: 12931444323
2015-11-24 05:13:30,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742281_1457, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:13:30,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458 src: /192.168.6.248:55324 dest: /192.168.6.249:50010
2015-11-24 05:13:52,653 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 567ms (threshold=300ms)
2015-11-24 05:13:55,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55324, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458, duration: 24222936953
2015-11-24 05:13:55,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742282_1458, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:13:55,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459 src: /192.168.6.248:55333 dest: /192.168.6.249:50010
2015-11-24 05:14:14,555 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 577ms (threshold=300ms)
2015-11-24 05:14:19,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55333, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459, duration: 24109142276
2015-11-24 05:14:19,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742283_1459, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:14:19,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460 src: /192.168.6.237:41495 dest: /192.168.6.249:50010
2015-11-24 05:14:43,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41495, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460, duration: 23900654494
2015-11-24 05:14:43,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742284_1460, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:14:43,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461 src: /192.168.6.248:55351 dest: /192.168.6.249:50010
2015-11-24 05:14:51,790 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 596ms (threshold=300ms)
2015-11-24 05:14:52,736 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 374ms (threshold=300ms)
2015-11-24 05:14:54,530 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 1052ms (threshold=300ms)
2015-11-24 05:15:00,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55351, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461, duration: 17173406033
2015-11-24 05:15:00,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742285_1461, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:15:00,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462 src: /192.168.6.237:41496 dest: /192.168.6.249:50010
2015-11-24 05:15:15,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41496, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462, duration: 14728529167
2015-11-24 05:15:15,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742286_1462, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:15:15,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463 src: /192.168.6.237:41497 dest: /192.168.6.249:50010
2015-11-24 05:15:39,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41497, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463, duration: 24273185709
2015-11-24 05:15:39,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742287_1463, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:15:39,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464 src: /192.168.6.237:41498 dest: /192.168.6.249:50010
2015-11-24 05:16:04,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41498, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464, duration: 25010377923
2015-11-24 05:16:04,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742288_1464, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:16:04,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465 src: /192.168.6.237:41499 dest: /192.168.6.249:50010
2015-11-24 05:16:27,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41499, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465, duration: 22960910856
2015-11-24 05:16:27,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742289_1465, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:16:27,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466 src: /192.168.6.237:41500 dest: /192.168.6.249:50010
2015-11-24 05:16:39,131 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:935ms (threshold=300ms)
2015-11-24 05:16:40,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41500, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466, duration: 13012708586
2015-11-24 05:16:40,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742290_1466, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:16:40,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467 src: /192.168.6.237:41501 dest: /192.168.6.249:50010
2015-11-24 05:16:53,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41501, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467, duration: 12432870542
2015-11-24 05:16:53,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742291_1467, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:16:53,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468 src: /192.168.6.237:41502 dest: /192.168.6.249:50010
2015-11-24 05:17:04,020 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:596ms (threshold=300ms)
2015-11-24 05:17:06,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41502, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468, duration: 13316974844
2015-11-24 05:17:06,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742292_1468, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:17:06,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469 src: /192.168.6.237:41503 dest: /192.168.6.249:50010
2015-11-24 05:17:30,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41503, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469, duration: 24443597233
2015-11-24 05:17:30,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742293_1469, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:17:31,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470 src: /192.168.6.248:55416 dest: /192.168.6.249:50010
2015-11-24 05:17:39,067 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 756ms (threshold=300ms)
2015-11-24 05:17:39,919 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 332ms (threshold=300ms)
2015-11-24 05:17:47,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55416, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470, duration: 16013578483
2015-11-24 05:17:47,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742294_1470, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:17:47,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471 src: /192.168.6.248:55424 dest: /192.168.6.249:50010
2015-11-24 05:17:59,948 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 937ms (threshold=300ms)
2015-11-24 05:18:11,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55424, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471, duration: 24213945186
2015-11-24 05:18:11,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742295_1471, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:18:11,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472 src: /192.168.6.237:41504 dest: /192.168.6.249:50010
2015-11-24 05:18:35,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41504, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472, duration: 24266121226
2015-11-24 05:18:35,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742296_1472, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:18:35,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473 src: /192.168.6.237:41505 dest: /192.168.6.249:50010
2015-11-24 05:19:00,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41505, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473, duration: 24819967345
2015-11-24 05:19:00,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742297_1473, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:19:00,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474 src: /192.168.6.248:55451 dest: /192.168.6.249:50010
2015-11-24 05:19:07,213 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 377ms (threshold=300ms)
2015-11-24 05:19:07,792 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:578ms (threshold=300ms)
2015-11-24 05:19:13,710 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 698ms (threshold=300ms)
2015-11-24 05:19:16,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55451, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474, duration: 16382144340
2015-11-24 05:19:16,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742298_1474, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:19:16,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475 src: /192.168.6.237:41506 dest: /192.168.6.249:50010
2015-11-24 05:19:29,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41506, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475, duration: 12077608746
2015-11-24 05:19:29,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742299_1475, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:19:29,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476 src: /192.168.6.237:41507 dest: /192.168.6.249:50010
2015-11-24 05:19:42,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41507, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476, duration: 13409258540
2015-11-24 05:19:42,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742300_1476, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:19:42,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477 src: /192.168.6.237:41508 dest: /192.168.6.249:50010
2015-11-24 05:20:05,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41508, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477, duration: 23207982992
2015-11-24 05:20:05,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742301_1477, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:20:05,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478 src: /192.168.6.248:55479 dest: /192.168.6.249:50010
2015-11-24 05:20:18,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55479, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478, duration: 12405330886
2015-11-24 05:20:18,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742302_1478, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:20:18,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479 src: /192.168.6.237:41509 dest: /192.168.6.249:50010
2015-11-24 05:20:30,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41509, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479, duration: 12029886343
2015-11-24 05:20:30,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742303_1479, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:20:30,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480 src: /192.168.6.237:41510 dest: /192.168.6.249:50010
2015-11-24 05:20:53,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41510, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480, duration: 23501948412
2015-11-24 05:20:53,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742304_1480, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:20:53,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481 src: /192.168.6.248:55501 dest: /192.168.6.249:50010
2015-11-24 05:21:03,302 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 438ms (threshold=300ms)
2015-11-24 05:21:07,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55501, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481, duration: 13484820066
2015-11-24 05:21:07,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742305_1481, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:21:07,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482 src: /192.168.6.237:41517 dest: /192.168.6.249:50010
2015-11-24 05:21:31,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41517, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482, duration: 24201788627
2015-11-24 05:21:31,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742306_1482, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:21:31,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483 src: /192.168.6.237:41518 dest: /192.168.6.249:50010
2015-11-24 05:21:43,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41518, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483, duration: 12028009506
2015-11-24 05:21:43,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742307_1483, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:21:43,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484 src: /192.168.6.237:41519 dest: /192.168.6.249:50010
2015-11-24 05:22:06,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41519, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484, duration: 23110836643
2015-11-24 05:22:06,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742308_1484, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:22:06,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485 src: /192.168.6.248:55532 dest: /192.168.6.249:50010
2015-11-24 05:22:15,293 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:356ms (threshold=300ms)
2015-11-24 05:22:30,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55532, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485, duration: 23553193446
2015-11-24 05:22:30,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742309_1485, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:22:30,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486 src: /192.168.6.237:41520 dest: /192.168.6.249:50010
2015-11-24 05:22:42,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41520, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486, duration: 12114742746
2015-11-24 05:22:42,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742310_1486, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:22:42,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487 src: /192.168.6.248:55546 dest: /192.168.6.249:50010
2015-11-24 05:22:54,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55546, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487, duration: 12062749319
2015-11-24 05:22:54,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742311_1487, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:22:54,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488 src: /192.168.6.237:41521 dest: /192.168.6.249:50010
2015-11-24 05:23:05,576 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:577ms (threshold=300ms)
2015-11-24 05:23:08,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41521, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488, duration: 13284276647
2015-11-24 05:23:08,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742312_1488, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:23:08,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489 src: /192.168.6.248:55559 dest: /192.168.6.249:50010
2015-11-24 05:23:20,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55559, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489, duration: 11986933726
2015-11-24 05:23:20,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742313_1489, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:23:20,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490 src: /192.168.6.248:55564 dest: /192.168.6.249:50010
2015-11-24 05:23:32,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55564, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490, duration: 12268514114
2015-11-24 05:23:32,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742314_1490, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:23:33,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491 src: /192.168.6.237:41522 dest: /192.168.6.249:50010
2015-11-24 05:23:56,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41522, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491, duration: 22994858846
2015-11-24 05:23:56,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742315_1491, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:23:56,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492 src: /192.168.6.237:41523 dest: /192.168.6.249:50010
2015-11-24 05:24:19,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41523, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492, duration: 23107530207
2015-11-24 05:24:19,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742316_1492, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:24:19,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493 src: /192.168.6.248:55585 dest: /192.168.6.249:50010
2015-11-24 05:24:32,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55585, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493, duration: 12819173001
2015-11-24 05:24:32,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742317_1493, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:24:32,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494 src: /192.168.6.237:41524 dest: /192.168.6.249:50010
2015-11-24 05:24:44,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41524, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494, duration: 12298486303
2015-11-24 05:24:44,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742318_1494, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:24:44,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495 src: /192.168.6.248:55599 dest: /192.168.6.249:50010
2015-11-24 05:24:55,703 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:536ms (threshold=300ms)
2015-11-24 05:24:57,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55599, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495, duration: 13059350716
2015-11-24 05:24:57,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742319_1495, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:24:57,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496 src: /192.168.6.248:55606 dest: /192.168.6.249:50010
2015-11-24 05:25:20,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55606, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496, duration: 23226926592
2015-11-24 05:25:20,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742320_1496, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:25:21,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497 src: /192.168.6.248:55615 dest: /192.168.6.249:50010
2015-11-24 05:25:44,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55615, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497, duration: 23164516752
2015-11-24 05:25:44,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742321_1497, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:25:44,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498 src: /192.168.6.248:55623 dest: /192.168.6.249:50010
2015-11-24 05:25:56,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55623, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498, duration: 12127288802
2015-11-24 05:25:56,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742322_1498, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:25:56,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499 src: /192.168.6.237:41525 dest: /192.168.6.249:50010
2015-11-24 05:26:09,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41525, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499, duration: 12512237121
2015-11-24 05:26:09,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742323_1499, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:26:09,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500 src: /192.168.6.237:41526 dest: /192.168.6.249:50010
2015-11-24 05:26:32,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41526, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500, duration: 23234870497
2015-11-24 05:26:32,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:26:32,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501 src: /192.168.6.248:55643 dest: /192.168.6.249:50010
2015-11-24 05:26:44,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55643, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501, duration: 12259196377
2015-11-24 05:26:44,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742325_1501, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:26:44,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502 src: /192.168.6.237:41527 dest: /192.168.6.249:50010
2015-11-24 05:27:07,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41527, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502, duration: 22594021182
2015-11-24 05:27:07,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742326_1502, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:27:08,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503 src: /192.168.6.237:41528 dest: /192.168.6.249:50010
2015-11-24 05:27:20,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41528, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503, duration: 12135001031
2015-11-24 05:27:20,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742327_1503, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:27:20,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504 src: /192.168.6.248:55666 dest: /192.168.6.249:50010
2015-11-24 05:27:32,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55666, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504, duration: 12462961012
2015-11-24 05:27:32,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742328_1504, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:27:32,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505 src: /192.168.6.237:41529 dest: /192.168.6.249:50010
2015-11-24 05:27:56,073 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:538ms (threshold=300ms)
2015-11-24 05:27:57,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41529, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505, duration: 24098051915
2015-11-24 05:27:57,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742329_1505, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:27:57,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506 src: /192.168.6.237:41530 dest: /192.168.6.249:50010
2015-11-24 05:28:20,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41530, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506, duration: 23086280604
2015-11-24 05:28:20,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742330_1506, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:28:20,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507 src: /192.168.6.248:55688 dest: /192.168.6.249:50010
2015-11-24 05:28:30,956 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write packet to mirror took 307ms (threshold=300ms)
2015-11-24 05:28:32,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55688, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507, duration: 12360429220
2015-11-24 05:28:32,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742331_1507, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:28:32,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508 src: /192.168.6.237:41531 dest: /192.168.6.249:50010
2015-11-24 05:28:55,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41531, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508, duration: 22720802012
2015-11-24 05:28:55,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742332_1508, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:28:55,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509 src: /192.168.6.248:55702 dest: /192.168.6.249:50010
2015-11-24 05:29:18,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55702, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509, duration: 23152033879
2015-11-24 05:29:18,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742333_1509, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:29:18,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510 src: /192.168.6.237:41532 dest: /192.168.6.249:50010
2015-11-24 05:29:31,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41532, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510, duration: 13359574226
2015-11-24 05:29:31,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742334_1510, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:29:32,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511 src: /192.168.6.237:41533 dest: /192.168.6.249:50010
2015-11-24 05:29:55,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41533, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511, duration: 23185869511
2015-11-24 05:29:55,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742335_1511, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:29:55,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512 src: /192.168.6.248:55726 dest: /192.168.6.249:50010
2015-11-24 05:30:18,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55726, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512, duration: 23101895125
2015-11-24 05:30:18,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742336_1512, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:30:18,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513 src: /192.168.6.248:55734 dest: /192.168.6.249:50010
2015-11-24 05:30:26,169 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:498ms (threshold=300ms)
2015-11-24 05:30:30,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55734, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513, duration: 12441619080
2015-11-24 05:30:30,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742337_1513, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:30:30,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514 src: /192.168.6.237:41536 dest: /192.168.6.249:50010
2015-11-24 05:30:54,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41536, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514, duration: 23249247841
2015-11-24 05:30:54,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742338_1514, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:30:54,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515 src: /192.168.6.248:55749 dest: /192.168.6.249:50010
2015-11-24 05:31:06,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55749, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515, duration: 12538048828
2015-11-24 05:31:06,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742339_1515, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:31:06,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516 src: /192.168.6.237:41537 dest: /192.168.6.249:50010
2015-11-24 05:31:29,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41537, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516, duration: 23039061116
2015-11-24 05:31:29,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742340_1516, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:31:29,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517 src: /192.168.6.237:41538 dest: /192.168.6.249:50010
2015-11-24 05:31:52,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41538, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517, duration: 22694855412
2015-11-24 05:31:52,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742341_1517, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:31:52,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518 src: /192.168.6.248:55771 dest: /192.168.6.249:50010
2015-11-24 05:32:15,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55771, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518, duration: 23110574072
2015-11-24 05:32:15,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742342_1518, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:32:15,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519 src: /192.168.6.248:55777 dest: /192.168.6.249:50010
2015-11-24 05:32:38,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55777, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519, duration: 22667265293
2015-11-24 05:32:38,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742343_1519, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:32:38,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520 src: /192.168.6.248:55788 dest: /192.168.6.249:50010
2015-11-24 05:32:50,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55788, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520, duration: 12476638874
2015-11-24 05:32:50,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742344_1520, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:32:50,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521 src: /192.168.6.248:55795 dest: /192.168.6.249:50010
2015-11-24 05:33:11,533 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:726ms (threshold=300ms)
2015-11-24 05:33:14,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55795, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521, duration: 23736161239
2015-11-24 05:33:14,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742345_1521, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:33:14,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522 src: /192.168.6.237:41539 dest: /192.168.6.249:50010
2015-11-24 05:33:27,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41539, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522, duration: 12277944096
2015-11-24 05:33:27,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742346_1522, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:33:27,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523 src: /192.168.6.237:41540 dest: /192.168.6.249:50010
2015-11-24 05:33:49,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41540, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523, duration: 22805554797
2015-11-24 05:33:49,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742347_1523, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:33:50,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524 src: /192.168.6.237:41541 dest: /192.168.6.249:50010
2015-11-24 05:34:03,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41541, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524, duration: 12399829723
2015-11-24 05:34:03,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742348_1524, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:34:03,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525 src: /192.168.6.248:55823 dest: /192.168.6.249:50010
2015-11-24 05:34:26,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55823, dest: /192.168.6.249:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525, duration: 23707773261
2015-11-24 05:34:26,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742349_1525, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:34:26,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526 src: /192.168.6.248:55831 dest: /192.168.6.249:50010
2015-11-24 05:34:35,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:55831, dest: /192.168.6.249:50010, bytes: 51875046, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-129450144_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526, duration: 8245008339
2015-11-24 05:34:35,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742350_1526, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:52:57,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742351_1527 src: /192.168.6.248:56088 dest: /192.168.6.249:50010
2015-11-24 05:56:25,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:56088, dest: /192.168.6.249:50010, bytes: 63918210, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742351_1527, duration: 208597971265
2015-11-24 05:56:25,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742351_1527, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 05:56:26,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742352_1528 src: /192.168.6.237:41544 dest: /192.168.6.249:50010
2015-11-24 05:59:56,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41544, dest: /192.168.6.249:50010, bytes: 63959903, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742352_1528, duration: 210713711112
2015-11-24 05:59:56,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742352_1528, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 05:59:57,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742353_1529 src: /192.168.6.237:41545 dest: /192.168.6.249:50010
2015-11-24 06:03:27,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41545, dest: /192.168.6.249:50010, bytes: 63944909, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742353_1529, duration: 210760124256
2015-11-24 06:03:27,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742353_1529, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 06:03:28,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742354_1530 src: /192.168.6.237:41546 dest: /192.168.6.249:50010
2015-11-24 06:06:58,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41546, dest: /192.168.6.249:50010, bytes: 63970677, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742354_1530, duration: 210369902576
2015-11-24 06:06:58,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742354_1530, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 06:06:59,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742355_1531 src: /192.168.6.237:41547 dest: /192.168.6.249:50010
2015-11-24 06:10:27,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41547, dest: /192.168.6.249:50010, bytes: 63892089, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742355_1531, duration: 208557695565
2015-11-24 06:10:27,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742355_1531, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 06:10:27,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742356_1532 src: /192.168.6.248:56400 dest: /192.168.6.249:50010
2015-11-24 06:13:56,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:56400, dest: /192.168.6.249:50010, bytes: 64089316, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742356_1532, duration: 208139313252
2015-11-24 06:13:56,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742356_1532, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 06:13:56,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742357_1533 src: /192.168.6.237:41550 dest: /192.168.6.249:50010
2015-11-24 06:17:24,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41550, dest: /192.168.6.249:50010, bytes: 64090454, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742357_1533, duration: 207949294693
2015-11-24 06:17:24,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742357_1533, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 06:17:24,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742358_1534 src: /192.168.6.248:56516 dest: /192.168.6.249:50010
2015-11-24 06:20:52,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:56516, dest: /192.168.6.249:50010, bytes: 64177702, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742358_1534, duration: 208049402214
2015-11-24 06:20:52,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742358_1534, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 06:20:53,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742359_1535 src: /192.168.6.237:41551 dest: /192.168.6.249:50010
2015-11-24 06:24:21,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41551, dest: /192.168.6.249:50010, bytes: 64037038, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742359_1535, duration: 207976276391
2015-11-24 06:24:21,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742359_1535, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 06:24:21,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742360_1536 src: /192.168.6.248:56631 dest: /192.168.6.249:50010
2015-11-24 06:27:49,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:56631, dest: /192.168.6.249:50010, bytes: 64093563, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742360_1536, duration: 208363584721
2015-11-24 06:27:49,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742360_1536, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 06:27:50,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742361_1537 src: /192.168.6.237:41552 dest: /192.168.6.249:50010
2015-11-24 06:31:19,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41552, dest: /192.168.6.249:50010, bytes: 64115246, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742361_1537, duration: 209236210740
2015-11-24 06:31:19,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742361_1537, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 06:31:19,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742362_1538 src: /192.168.6.248:56750 dest: /192.168.6.249:50010
2015-11-24 06:34:48,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:56750, dest: /192.168.6.249:50010, bytes: 64121965, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742362_1538, duration: 208292553993
2015-11-24 06:34:48,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742362_1538, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 06:34:48,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742363_1539 src: /192.168.6.237:41553 dest: /192.168.6.249:50010
2015-11-24 06:38:16,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41553, dest: /192.168.6.249:50010, bytes: 64023361, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742363_1539, duration: 207871265244
2015-11-24 06:38:16,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742363_1539, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 06:38:16,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742364_1540 src: /192.168.6.248:56863 dest: /192.168.6.249:50010
2015-11-24 06:41:44,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:56863, dest: /192.168.6.249:50010, bytes: 64095447, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742364_1540, duration: 207380662669
2015-11-24 06:41:44,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742364_1540, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 06:41:44,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742365_1541 src: /192.168.6.237:41556 dest: /192.168.6.249:50010
2015-11-24 06:45:12,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41556, dest: /192.168.6.249:50010, bytes: 64018975, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742365_1541, duration: 208194930190
2015-11-24 06:45:12,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742365_1541, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 06:45:13,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742366_1542 src: /192.168.6.248:56977 dest: /192.168.6.249:50010
2015-11-24 06:48:42,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:56977, dest: /192.168.6.249:50010, bytes: 64084413, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742366_1542, duration: 209208600906
2015-11-24 06:48:42,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742366_1542, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 06:48:42,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742367_1543 src: /192.168.6.237:41557 dest: /192.168.6.249:50010
2015-11-24 06:52:10,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41557, dest: /192.168.6.249:50010, bytes: 64070613, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742367_1543, duration: 207681040865
2015-11-24 06:52:10,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742367_1543, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 06:52:10,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742368_1544 src: /192.168.6.237:41558 dest: /192.168.6.249:50010
2015-11-24 06:55:38,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:41558, dest: /192.168.6.249:50010, bytes: 64084139, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742368_1544, duration: 208069654604
2015-11-24 06:55:38,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742368_1544, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-24 06:55:39,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742369_1545 src: /192.168.6.248:57151 dest: /192.168.6.249:50010
2015-11-24 06:59:07,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:57151, dest: /192.168.6.249:50010, bytes: 64004195, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742369_1545, duration: 208034622200
2015-11-24 06:59:07,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742369_1545, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 06:59:07,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742370_1546 src: /192.168.6.248:57214 dest: /192.168.6.249:50010
2015-11-24 07:02:35,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:57214, dest: /192.168.6.249:50010, bytes: 63987327, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742370_1546, duration: 207889108648
2015-11-24 07:02:35,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742370_1546, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 07:02:36,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742371_1547 src: /192.168.6.248:57270 dest: /192.168.6.249:50010
2015-11-24 07:06:04,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:57270, dest: /192.168.6.249:50010, bytes: 64009289, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742371_1547, duration: 208257333272
2015-11-24 07:06:04,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742371_1547, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 07:06:04,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742372_1548 src: /192.168.6.248:57325 dest: /192.168.6.249:50010
2015-11-24 07:09:31,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:57325, dest: /192.168.6.249:50010, bytes: 64186338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973570138_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742372_1548, duration: 207367390551
2015-11-24 07:09:31,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742372_1548, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-24 07:09:32,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742373_1549 src: /192.168.6.248:57381 dest: /192.168.6.249:50010
2015-11-24 07:09:49,394 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-24 07:09:49,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-25 02:10:49,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-25 02:10:50,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-25 02:10:51,410 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-25 02:10:51,500 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-25 02:10:51,500 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-25 02:10:51,504 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-25 02:10:51,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-25 02:10:51,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-25 02:10:51,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-25 02:10:51,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-25 02:10:51,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-25 02:10:51,819 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-25 02:10:51,848 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-25 02:10:51,857 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-25 02:10:51,864 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-25 02:10:51,868 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-25 02:10:51,868 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-25 02:10:51,868 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-25 02:10:51,888 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 49656
2015-11-25 02:10:51,888 INFO org.mortbay.log: jetty-6.1.26
2015-11-25 02:10:52,191 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:49656
2015-11-25 02:10:52,337 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-25 02:10:52,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-25 02:10:52,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-25 02:10:52,550 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-25 02:10:52,567 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-25 02:10:52,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-25 02:10:52,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-25 02:10:52,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-25 02:10:52,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-25 02:10:52,880 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-25 02:10:52,880 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-25 02:10:53,253 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 12428@rushikesh2
2015-11-25 02:10:53,333 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-25 02:10:53,333 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-25 02:10:53,334 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-25 02:10:53,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-25 02:10:53,443 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-25 02:10:53,443 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-25 02:10:53,490 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-25 02:10:53,490 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 02:10:53,491 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 02:10:53,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 43ms
2015-11-25 02:10:53,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 43ms
2015-11-25 02:10:53,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 02:10:53,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 96ms
2015-11-25 02:10:53,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 97ms
2015-11-25 02:10:53,989 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1110452071 ms.
2015-11-25 02:10:53,991 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448418053991 with interval 21600000
2015-11-25 02:10:53,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-25 02:10:54,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-25 02:10:54,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-25 02:10:54,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1964
2015-11-25 02:10:54,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-25 02:10:54,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x556674ebe3d,  containing 1 storage report(s), of which we sent 1. The reports had 285 total blocks and used 1 RPC(s). This took 5 msec to generate and 62 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-25 02:10:54,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 03:10:45,004 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742257_1433 for rescanning.
2015-11-25 03:10:45,005 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169) exiting because of exception 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:539)
	at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:619)
2015-11-25 03:10:45,007 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169) exiting.
2015-11-25 04:28:55,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xcdea413ddf3,  containing 1 storage report(s), of which we sent 1. The reports had 285 total blocks and used 1 RPC(s). This took 1 msec to generate and 9 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-25 04:28:55,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 04:28:58,842 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742368_1544 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742368 for deletion
2015-11-25 04:28:58,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742369_1545 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742369 for deletion
2015-11-25 04:28:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742370_1546 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742370 for deletion
2015-11-25 04:28:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742371_1547 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742371 for deletion
2015-11-25 04:28:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742372_1548 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742372 for deletion
2015-11-25 04:28:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742373_1549 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/rbw/blk_1073742373 for deletion
2015-11-25 04:28:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742351_1527 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742351 for deletion
2015-11-25 04:28:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742352_1528 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742352 for deletion
2015-11-25 04:28:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742353_1529 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742353 for deletion
2015-11-25 04:28:58,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742354_1530 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742354 for deletion
2015-11-25 04:28:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742355_1531 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742355 for deletion
2015-11-25 04:28:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742356_1532 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742356 for deletion
2015-11-25 04:28:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742357_1533 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742357 for deletion
2015-11-25 04:28:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742358_1534 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742358 for deletion
2015-11-25 04:28:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742359_1535 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742359 for deletion
2015-11-25 04:28:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742360_1536 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742360 for deletion
2015-11-25 04:28:58,845 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742361_1537 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742361 for deletion
2015-11-25 04:28:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742362_1538 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742362 for deletion
2015-11-25 04:28:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742363_1539 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742363 for deletion
2015-11-25 04:28:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742364_1540 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742364 for deletion
2015-11-25 04:28:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742365_1541 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742365 for deletion
2015-11-25 04:28:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742366_1542 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742366 for deletion
2015-11-25 04:28:58,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742367_1543 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742367 for deletion
2015-11-25 04:28:58,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742368_1544 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742368
2015-11-25 04:28:58,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742369_1545 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742369
2015-11-25 04:28:58,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742370_1546 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742370
2015-11-25 04:28:58,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742371_1547 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742371
2015-11-25 04:28:58,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742372_1548 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742372
2015-11-25 04:28:58,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742373_1549 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/rbw/blk_1073742373
2015-11-25 04:28:58,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742351_1527 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742351
2015-11-25 04:28:58,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742352_1528 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742352
2015-11-25 04:28:58,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742353_1529 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742353
2015-11-25 04:28:58,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742354_1530 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742354
2015-11-25 04:28:58,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742355_1531 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742355
2015-11-25 04:28:58,871 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742356_1532 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742356
2015-11-25 04:28:58,871 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742357_1533 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742357
2015-11-25 04:28:58,871 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742358_1534 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742358
2015-11-25 04:28:58,872 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742359_1535 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742359
2015-11-25 04:28:58,872 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742360_1536 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742360
2015-11-25 04:28:58,872 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742361_1537 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742361
2015-11-25 04:28:58,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742362_1538 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742362
2015-11-25 04:28:58,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742363_1539 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742363
2015-11-25 04:28:58,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742364_1540 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742364
2015-11-25 04:28:58,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742365_1541 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742365
2015-11-25 04:28:58,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742366_1542 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742366
2015-11-25 04:28:58,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742367_1543 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742367
2015-11-25 04:59:08,543 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): Scheduling suspect block BP-1750158012-192.168.6.248-1444037565733:blk_1073742324_1500 for rescanning.
2015-11-25 07:50:54,065 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 262, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-25 08:02:19,864 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:515)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-25 08:02:23,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-25 08:02:24,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-25 08:02:25,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-25 08:02:26,277 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-25 08:02:26,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-25 08:04:18,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-25 08:04:18,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-25 08:04:19,386 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-25 08:04:19,450 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-25 08:04:19,450 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-25 08:04:19,455 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-25 08:04:19,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-25 08:04:19,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-25 08:04:19,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-25 08:04:19,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-25 08:04:19,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-25 08:04:19,574 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-25 08:04:19,582 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-25 08:04:19,587 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-25 08:04:19,592 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-25 08:04:19,594 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-25 08:04:19,594 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-25 08:04:19,595 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-25 08:04:19,604 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 51201
2015-11-25 08:04:19,605 INFO org.mortbay.log: jetty-6.1.26
2015-11-25 08:04:19,756 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:51201
2015-11-25 08:04:19,839 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-25 08:04:19,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-25 08:04:19,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-25 08:04:19,879 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-25 08:04:19,890 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-25 08:04:19,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-25 08:04:19,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-25 08:04:19,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-25 08:04:19,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-25 08:04:19,970 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-25 08:04:19,970 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-25 08:04:20,228 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 6414@rushikesh2
2015-11-25 08:04:20,302 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:04:20,302 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:04:20,303 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-25 08:04:20,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-25 08:04:20,368 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-25 08:04:20,368 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-25 08:04:20,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-25 08:04:20,402 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:04:20,402 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 08:04:20,410 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35143770112
2015-11-25 08:04:20,411 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-25 08:04:20,411 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-25 08:04:20,412 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 08:04:20,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 34ms
2015-11-25 08:04:20,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 36ms
2015-11-25 08:04:20,632 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1089245427 ms.
2015-11-25 08:04:20,634 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448431683634 with interval 21600000
2015-11-25 08:04:20,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-25 08:04:20,648 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-25 08:04:20,648 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-25 08:04:20,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1984
2015-11-25 08:04:20,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-25 08:04:20,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x189ff2f8da8b,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 6 msec to generate and 46 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-25 08:04:20,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:10:19,964 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-25 08:10:23,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-25 08:10:24,454 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-25 08:10:24,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-25 08:12:26,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-25 08:12:26,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-25 08:12:27,249 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-25 08:12:27,311 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-25 08:12:27,311 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-25 08:12:27,316 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-25 08:12:27,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-25 08:12:27,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-25 08:12:27,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-25 08:12:27,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-25 08:12:27,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-25 08:12:27,435 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-25 08:12:27,443 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-25 08:12:27,448 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-25 08:12:27,453 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-25 08:12:27,455 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-25 08:12:27,455 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-25 08:12:27,455 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-25 08:12:27,465 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 59563
2015-11-25 08:12:27,465 INFO org.mortbay.log: jetty-6.1.26
2015-11-25 08:12:27,619 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:59563
2015-11-25 08:12:27,702 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-25 08:12:27,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-25 08:12:27,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-25 08:12:27,741 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-25 08:12:27,752 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-25 08:12:27,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-25 08:12:27,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-25 08:12:27,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-25 08:12:27,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-25 08:12:27,831 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-25 08:12:27,831 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-25 08:12:28,063 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 7482@rushikesh2
2015-11-25 08:12:28,138 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:12:28,138 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:12:28,139 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-25 08:12:28,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-25 08:12:28,202 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-25 08:12:28,202 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-25 08:12:28,237 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-25 08:12:28,237 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:12:28,238 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 08:12:28,245 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35143770112
2015-11-25 08:12:28,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-25 08:12:28,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-25 08:12:28,247 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 08:12:28,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 34ms
2015-11-25 08:12:28,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 34ms
2015-11-25 08:12:28,441 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1088757618 ms.
2015-11-25 08:12:28,443 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448433219443 with interval 21600000
2015-11-25 08:12:28,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-25 08:12:28,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-25 08:12:28,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-25 08:12:28,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1987
2015-11-25 08:12:28,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-25 08:12:28,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1911850b46d8,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 4 msec to generate and 37 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-25 08:12:28,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:14:12,826 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-25 08:14:16,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-25 08:14:17,466 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-25 08:14:17,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-25 08:26:20,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-25 08:26:20,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-25 08:26:20,836 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-25 08:26:20,899 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-25 08:26:20,899 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-25 08:26:20,904 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-25 08:26:20,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-25 08:26:20,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-25 08:26:20,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-25 08:26:20,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-25 08:26:20,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-25 08:26:21,023 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-25 08:26:21,031 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-25 08:26:21,036 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-25 08:26:21,041 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-25 08:26:21,043 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-25 08:26:21,043 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-25 08:26:21,043 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-25 08:26:21,053 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36428
2015-11-25 08:26:21,053 INFO org.mortbay.log: jetty-6.1.26
2015-11-25 08:26:21,208 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36428
2015-11-25 08:26:21,291 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-25 08:26:21,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-25 08:26:21,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-25 08:26:21,331 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-25 08:26:21,342 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-25 08:26:21,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-25 08:26:21,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-25 08:26:21,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-25 08:26:21,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-25 08:26:21,422 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-25 08:26:21,422 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-25 08:26:21,672 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 9070@rushikesh2
2015-11-25 08:26:21,738 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:26:21,738 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:26:21,738 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-25 08:26:21,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-25 08:26:21,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-25 08:26:21,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-25 08:26:21,838 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-25 08:26:21,838 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:26:21,839 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 08:26:21,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 10ms
2015-11-25 08:26:21,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 11ms
2015-11-25 08:26:21,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-25 08:26:21,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 32ms
2015-11-25 08:26:21,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2015-11-25 08:26:22,040 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1087924019 ms.
2015-11-25 08:26:22,042 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448424624042 with interval 21600000
2015-11-25 08:26:22,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-25 08:26:22,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-25 08:26:22,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-25 08:26:22,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=1990
2015-11-25 08:26:22,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-25 08:26:22,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x19d39b6e4b8a,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 4 msec to generate and 56 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-25 08:26:22,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-25 08:31:37,354 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-25 08:31:37,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-26 06:00:58,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-26 06:00:58,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-26 06:00:59,348 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-26 06:00:59,527 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-26 06:00:59,527 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-26 06:00:59,534 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-26 06:00:59,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-26 06:00:59,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-26 06:00:59,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-26 06:00:59,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-26 06:00:59,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-26 06:00:59,886 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-26 06:00:59,897 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-26 06:00:59,922 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-26 06:00:59,930 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-26 06:00:59,934 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-26 06:00:59,934 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-26 06:00:59,934 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-26 06:00:59,966 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 49554
2015-11-26 06:00:59,966 INFO org.mortbay.log: jetty-6.1.26
2015-11-26 06:01:00,199 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:49554
2015-11-26 06:01:00,342 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-26 06:01:00,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-26 06:01:00,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-26 06:01:00,580 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-26 06:01:00,615 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-26 06:01:00,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-26 06:01:00,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-26 06:01:00,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-26 06:01:00,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-26 06:01:00,919 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-26 06:01:00,920 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-26 06:01:01,298 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 25069@rushikesh2
2015-11-26 06:01:01,400 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-26 06:01:01,400 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-26 06:01:01,401 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-26 06:01:01,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-26 06:01:01,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-26 06:01:01,494 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-26 06:01:01,540 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-26 06:01:01,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-26 06:01:01,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-26 06:01:01,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 55ms
2015-11-26 06:01:01,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 56ms
2015-11-26 06:01:01,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-26 06:01:01,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 53ms
2015-11-26 06:01:01,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 53ms
2015-11-26 06:01:02,069 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 1010243990 ms.
2015-11-26 06:01:02,071 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448516202071 with interval 21600000
2015-11-26 06:01:02,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-26 06:01:02,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-26 06:01:02,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-26 06:01:02,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2027
2015-11-26 06:01:02,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-26 06:01:02,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x11910f684c51,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 4 msec to generate and 65 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-26 06:01:02,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-26 06:15:09,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742377_1554 src: /192.168.6.248:53468 dest: /192.168.6.249:50010
2015-11-26 06:15:09,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:53468, dest: /192.168.6.249:50010, bytes: 287206, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1428792081_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742377_1554, duration: 89304419
2015-11-26 06:15:09,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742377_1554, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-26 06:15:36,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742377_1554 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742377 for deletion
2015-11-26 06:15:36,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742377_1554 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742377
2015-11-26 08:03:48,843 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-26 08:03:52,581 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-26 08:03:52,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-27 00:44:06,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 00:44:06,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 00:44:07,707 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 00:44:07,900 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 00:44:07,900 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 00:44:07,907 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 00:44:07,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-27 00:44:07,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 00:44:08,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 00:44:08,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 00:44:08,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 00:44:08,216 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 00:44:08,228 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 00:44:08,237 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 00:44:08,243 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 00:44:08,246 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 00:44:08,246 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 00:44:08,247 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 00:44:08,284 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 48152
2015-11-27 00:44:08,284 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 00:44:08,582 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:48152
2015-11-27 00:44:08,747 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 00:44:08,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 00:44:08,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 00:44:08,955 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 00:44:08,991 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 00:44:09,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 00:44:09,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 00:44:09,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 00:44:09,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 00:44:09,269 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 00:44:09,270 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 00:44:09,754 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3586@rushikesh2
2015-11-27 00:44:09,848 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 00:44:09,848 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 00:44:09,849 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 00:44:09,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-27 00:44:09,946 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-27 00:44:09,946 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 00:44:09,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 00:44:09,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 00:44:09,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 00:44:10,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 57ms
2015-11-27 00:44:10,054 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 58ms
2015-11-27 00:44:10,054 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 00:44:10,096 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 41ms
2015-11-27 00:44:10,096 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 42ms
2015-11-27 00:44:10,493 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 942855566 ms.
2015-11-27 00:44:10,495 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448566702495 with interval 21600000
2015-11-27 00:44:10,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 00:44:10,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 00:44:10,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 00:44:10,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2053
2015-11-27 00:44:10,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-27 00:44:10,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9f123e84bc,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 7 msec to generate and 111 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 00:44:10,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 01:08:22,572 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1750158012-192.168.6.248-1444037565733 Total blocks: 262, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-11-27 01:12:07,125 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh2:50010:DataXceiver error processing unknown operation  src: /192.168.6.254:60202 dst: /192.168.6.249:50010
java.io.IOException: Version Mismatch (Expected: 28, Received: 18245 )
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:60)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:227)
	at java.lang.Thread.run(Thread.java:745)
2015-11-27 01:12:07,131 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh2:50010:DataXceiver error processing unknown operation  src: /192.168.6.254:60203 dst: /192.168.6.249:50010
java.io.IOException: Version Mismatch (Expected: 28, Received: 18245 )
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:60)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:227)
	at java.lang.Thread.run(Thread.java:745)
2015-11-27 01:12:21,001 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh2:50010:DataXceiver error processing unknown operation  src: /192.168.6.254:60395 dst: /192.168.6.249:50010
java.io.IOException: Version Mismatch (Expected: 28, Received: 18245 )
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:60)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:227)
	at java.lang.Thread.run(Thread.java:745)
2015-11-27 01:12:21,005 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh2:50010:DataXceiver error processing unknown operation  src: /192.168.6.254:60396 dst: /192.168.6.249:50010
java.io.IOException: Version Mismatch (Expected: 28, Received: 18245 )
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:60)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:227)
	at java.lang.Thread.run(Thread.java:745)
2015-11-27 01:12:51,681 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: rushikesh2:50010:DataXceiver error processing unknown operation  src: /192.168.6.254:60799 dst: /192.168.6.249:50010
java.io.IOException: Version Mismatch (Expected: 28, Received: 18245 )
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:60)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:227)
	at java.lang.Thread.run(Thread.java:745)
2015-11-27 02:10:00,226 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-27 02:10:04,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-27 02:10:05,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-27 02:10:05,411 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 02:10:05,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-27 02:20:15,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 02:20:15,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 02:20:16,576 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 02:20:16,651 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 02:20:16,651 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 02:20:16,656 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 02:20:16,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-27 02:20:16,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 02:20:16,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 02:20:16,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 02:20:16,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 02:20:16,809 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 02:20:16,816 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 02:20:16,822 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 02:20:16,826 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 02:20:16,829 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 02:20:16,829 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 02:20:16,829 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 02:20:16,839 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 53199
2015-11-27 02:20:16,839 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 02:20:16,995 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:53199
2015-11-27 02:20:17,077 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 02:20:17,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 02:20:17,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 02:20:17,116 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 02:20:17,127 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 02:20:17,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 02:20:17,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 02:20:17,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 02:20:17,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 02:20:17,208 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 02:20:17,208 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 02:20:17,432 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 15927@rushikesh2
2015-11-27 02:20:17,506 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 02:20:17,506 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 02:20:17,506 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 02:20:17,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-27 02:20:17,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-27 02:20:17,572 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 02:20:17,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 02:20:17,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 02:20:17,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 02:20:17,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 11ms
2015-11-27 02:20:17,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 11ms
2015-11-27 02:20:17,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 02:20:17,652 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 35ms
2015-11-27 02:20:17,652 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 35ms
2015-11-27 02:20:17,810 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 937088249 ms.
2015-11-27 02:20:17,812 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448576240812 with interval 21600000
2015-11-27 02:20:17,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 02:20:17,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 02:20:17,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 02:20:17,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2067
2015-11-27 02:20:17,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-27 02:20:17,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5ddda38ace5,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 4 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 02:20:17,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 02:28:14,202 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-27 02:28:18,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-27 02:28:18,543 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 02:28:18,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-27 02:32:13,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 02:32:13,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 02:32:13,809 FATAL org.apache.hadoop.conf.Configuration: error parsing conf hdfs-site.xml
org.xml.sax.SAXParseException; systemId: file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml; lineNumber: 23; columnNumber: 3; The element type "property" must be terminated by the matching end-tag "</property>".
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2492)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1143)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1115)
	at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1451)
	at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:321)
	at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:487)
	at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:170)
	at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:153)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2248)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2307)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2484)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2508)
2015-11-27 02:32:13,810 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml; lineNumber: 23; columnNumber: 3; The element type "property" must be terminated by the matching end-tag "</property>".
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2645)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2492)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1143)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1115)
	at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1451)
	at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:321)
	at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:487)
	at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:170)
	at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:153)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2248)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2307)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2484)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2508)
Caused by: org.xml.sax.SAXParseException; systemId: file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml; lineNumber: 23; columnNumber: 3; The element type "property" must be terminated by the matching end-tag "</property>".
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	... 13 more
2015-11-27 02:32:13,812 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-11-27 02:32:13,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-27 02:35:03,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 02:35:03,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 02:35:03,661 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 02:35:03,724 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 02:35:03,724 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 02:35:03,729 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 02:35:03,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-27 02:35:03,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 02:35:03,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 02:35:03,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 02:35:03,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 02:35:03,850 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 02:35:03,858 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 02:35:03,863 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 02:35:03,868 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 02:35:03,870 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 02:35:03,870 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 02:35:03,870 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 02:35:03,880 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45173
2015-11-27 02:35:03,880 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 02:35:04,035 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45173
2015-11-27 02:35:04,119 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 02:35:04,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 02:35:04,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 02:35:04,159 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 02:35:04,170 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 02:35:04,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 02:35:04,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 02:35:04,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 02:35:04,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 02:35:04,251 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 02:35:04,252 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 02:35:04,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 18145@rushikesh2
2015-11-27 02:35:04,564 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 02:35:04,564 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 02:35:04,565 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 02:35:04,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-27 02:35:04,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-27 02:35:04,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 02:35:04,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 02:35:04,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 02:35:04,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 02:35:04,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35143770112
2015-11-27 02:35:04,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-27 02:35:04,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-27 02:35:04,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 02:35:04,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 33ms
2015-11-27 02:35:04,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2015-11-27 02:35:04,883 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 936201176 ms.
2015-11-27 02:35:04,885 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448585304885 with interval 21600000
2015-11-27 02:35:04,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 02:35:04,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 02:35:04,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 02:35:04,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2073
2015-11-27 02:35:04,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-27 02:35:04,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6ac64254966,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 5 msec to generate and 41 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 02:35:04,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 02:36:16,245 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-27 02:36:20,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-27 02:36:21,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-27 02:36:22,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-27 02:36:22,439 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 02:36:22,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-27 03:05:08,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 03:05:08,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 03:05:08,759 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 03:05:08,822 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 03:05:08,822 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 03:05:08,828 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 03:05:08,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-27 03:05:08,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 03:05:08,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 03:05:08,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 03:05:08,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 03:05:08,946 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 03:05:08,954 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 03:05:08,959 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 03:05:08,964 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 03:05:08,966 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 03:05:08,966 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 03:05:08,966 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 03:05:08,976 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 52962
2015-11-27 03:05:08,976 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 03:05:09,132 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:52962
2015-11-27 03:05:09,214 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 03:05:09,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 03:05:09,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 03:05:09,253 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 03:05:09,264 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 03:05:09,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 03:05:09,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 03:05:09,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 03:05:09,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 03:05:09,343 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 03:05:09,344 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 03:05:09,564 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 20624@rushikesh2
2015-11-27 03:05:09,638 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 03:05:09,638 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 03:05:09,639 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 03:05:09,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-27 03:05:09,712 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-27 03:05:09,712 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 03:05:09,745 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 03:05:09,746 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 03:05:09,746 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 03:05:09,757 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 11ms
2015-11-27 03:05:09,758 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 11ms
2015-11-27 03:05:09,758 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 03:05:09,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 32ms
2015-11-27 03:05:09,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 34ms
2015-11-27 03:05:09,967 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 934396092 ms.
2015-11-27 03:05:09,968 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448589790968 with interval 21600000
2015-11-27 03:05:09,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 03:05:09,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 03:05:09,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 03:05:10,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2074
2015-11-27 03:05:10,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-27 03:05:10,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x850ab387356,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 4 msec to generate and 38 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 03:05:10,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 03:19:36,338 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-27 03:19:40,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-27 03:19:41,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-27 03:19:42,195 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 03:19:42,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-27 04:00:38,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-27 04:00:38,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-27 04:00:39,047 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-27 04:00:39,109 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-27 04:00:39,110 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-27 04:00:39,114 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-27 04:00:39,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-27 04:00:39,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-27 04:00:39,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-27 04:00:39,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-27 04:00:39,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-27 04:00:39,234 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-27 04:00:39,241 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-27 04:00:39,247 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-27 04:00:39,251 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-27 04:00:39,254 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-27 04:00:39,254 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-27 04:00:39,254 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-27 04:00:39,264 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 48018
2015-11-27 04:00:39,264 INFO org.mortbay.log: jetty-6.1.26
2015-11-27 04:00:39,418 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:48018
2015-11-27 04:00:39,499 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-27 04:00:39,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-27 04:00:39,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-27 04:00:39,539 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-27 04:00:39,550 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-27 04:00:39,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-27 04:00:39,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-27 04:00:39,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-27 04:00:39,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-27 04:00:39,643 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-27 04:00:39,643 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-27 04:00:39,874 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 24815@rushikesh2
2015-11-27 04:00:39,955 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-27 04:00:39,955 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-27 04:00:39,956 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-27 04:00:39,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-27 04:00:40,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-27 04:00:40,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-27 04:00:40,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-27 04:00:40,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 04:00:40,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 04:00:40,066 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 10ms
2015-11-27 04:00:40,066 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 11ms
2015-11-27 04:00:40,067 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-27 04:00:40,099 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 32ms
2015-11-27 04:00:40,099 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2015-11-27 04:00:40,304 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 931065755 ms.
2015-11-27 04:00:40,305 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448581370305 with interval 21600000
2015-11-27 04:00:40,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-27 04:00:40,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-27 04:00:40,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-27 04:00:40,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=2081
2015-11-27 04:00:40,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-27 04:00:40,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xb5812c92cd9,  containing 1 storage report(s), of which we sent 1. The reports had 262 total blocks and used 1 RPC(s). This took 4 msec to generate and 38 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-27 04:00:40,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-27 04:06:29,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742378_1555 src: /192.168.6.248:40276 dest: /192.168.6.249:50010
2015-11-27 04:06:29,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40276, dest: /192.168.6.249:50010, bytes: 3363967, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-192053129_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742378_1555, duration: 354654983
2015-11-27 04:06:29,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742378_1555, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:09:10,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742380_1557 src: /192.168.6.237:53891 dest: /192.168.6.249:50010
2015-11-27 04:09:10,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53891, dest: /192.168.6.249:50010, bytes: 2254636, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_769393575_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742380_1557, duration: 250280825
2015-11-27 04:09:10,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742380_1557, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:09:12,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742381_1558 src: /192.168.6.248:40331 dest: /192.168.6.249:50010
2015-11-27 04:09:12,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40331, dest: /192.168.6.249:50010, bytes: 2255615, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2100031452_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742381_1558, duration: 248282393
2015-11-27 04:09:12,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742381_1558, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:09:15,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742382_1559 src: /192.168.6.238:47961 dest: /192.168.6.249:50010
2015-11-27 04:09:15,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:47961, dest: /192.168.6.249:50010, bytes: 2245077, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1091588624_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742382_1559, duration: 243185431
2015-11-27 04:09:15,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742382_1559, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:09:19,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742384_1561 src: /192.168.6.248:40340 dest: /192.168.6.249:50010
2015-11-27 04:09:20,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40340, dest: /192.168.6.249:50010, bytes: 2239839, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1089394258_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742384_1561, duration: 247066228
2015-11-27 04:09:20,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742384_1561, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:09:22,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742385_1562 src: /192.168.6.237:53893 dest: /192.168.6.249:50010
2015-11-27 04:09:22,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53893, dest: /192.168.6.249:50010, bytes: 2254636, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_708916130_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742385_1562, duration: 243592253
2015-11-27 04:09:22,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742385_1562, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:09:26,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742387_1564 src: /192.168.6.238:47963 dest: /192.168.6.249:50010
2015-11-27 04:09:27,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:47963, dest: /192.168.6.249:50010, bytes: 2245077, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1681932291_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742387_1564, duration: 242544280
2015-11-27 04:09:27,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742387_1564, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:09:29,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742388_1565 src: /192.168.6.238:47964 dest: /192.168.6.249:50010
2015-11-27 04:09:29,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:47964, dest: /192.168.6.249:50010, bytes: 2244091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1496412515_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742388_1565, duration: 249989414
2015-11-27 04:09:29,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742388_1565, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:09:31,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742389_1566 src: /192.168.6.237:53895 dest: /192.168.6.249:50010
2015-11-27 04:09:31,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53895, dest: /192.168.6.249:50010, bytes: 2268436, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1057581316_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742389_1566, duration: 246951673
2015-11-27 04:09:31,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742389_1566, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:09:36,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742391_1568 src: /192.168.6.248:40367 dest: /192.168.6.249:50010
2015-11-27 04:09:36,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40367, dest: /192.168.6.249:50010, bytes: 2269853, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1787146598_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742391_1568, duration: 249235173
2015-11-27 04:09:36,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742391_1568, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:09:41,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742393_1570 src: /192.168.6.248:40373 dest: /192.168.6.249:50010
2015-11-27 04:09:41,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40373, dest: /192.168.6.249:50010, bytes: 2389070, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1892248561_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742393_1570, duration: 265200903
2015-11-27 04:09:41,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742393_1570, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:09:43,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742394_1571 src: /192.168.6.237:53903 dest: /192.168.6.249:50010
2015-11-27 04:09:44,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53903, dest: /192.168.6.249:50010, bytes: 2261764, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1085163711_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742394_1571, duration: 243969189
2015-11-27 04:09:44,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742394_1571, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:09:46,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742395_1572 src: /192.168.6.248:40379 dest: /192.168.6.249:50010
2015-11-27 04:09:46,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40379, dest: /192.168.6.249:50010, bytes: 2269724, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1935583376_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742395_1572, duration: 244991667
2015-11-27 04:09:46,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742395_1572, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:09:50,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742397_1574 src: /192.168.6.248:40385 dest: /192.168.6.249:50010
2015-11-27 04:09:51,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40385, dest: /192.168.6.249:50010, bytes: 2272974, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-482657314_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742397_1574, duration: 246075794
2015-11-27 04:09:51,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742397_1574, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:09:53,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742398_1575 src: /192.168.6.238:47975 dest: /192.168.6.249:50010
2015-11-27 04:09:53,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:47975, dest: /192.168.6.249:50010, bytes: 2390295, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-375788784_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742398_1575, duration: 254595721
2015-11-27 04:09:53,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742398_1575, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:09:58,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742400_1577 src: /192.168.6.238:47979 dest: /192.168.6.249:50010
2015-11-27 04:09:58,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:47979, dest: /192.168.6.249:50010, bytes: 2273589, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_360155184_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742400_1577, duration: 243827418
2015-11-27 04:09:58,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742400_1577, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:10:00,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742401_1578 src: /192.168.6.248:40404 dest: /192.168.6.249:50010
2015-11-27 04:10:00,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40404, dest: /192.168.6.249:50010, bytes: 2267236, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-73119185_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742401_1578, duration: 245802834
2015-11-27 04:10:00,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742401_1578, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:10:02,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742402_1579 src: /192.168.6.248:40407 dest: /192.168.6.249:50010
2015-11-27 04:10:03,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40407, dest: /192.168.6.249:50010, bytes: 2267993, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1063814990_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742402_1579, duration: 244903861
2015-11-27 04:10:03,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742402_1579, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:10:05,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742403_1580 src: /192.168.6.248:40410 dest: /192.168.6.249:50010
2015-11-27 04:10:05,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40410, dest: /192.168.6.249:50010, bytes: 2387080, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1712759748_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742403_1580, duration: 256283344
2015-11-27 04:10:05,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742403_1580, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:10:07,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742404_1581 src: /192.168.6.248:40413 dest: /192.168.6.249:50010
2015-11-27 04:10:07,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40413, dest: /192.168.6.249:50010, bytes: 2273506, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_874066476_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742404_1581, duration: 244823006
2015-11-27 04:10:07,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742404_1581, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:10:09,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742405_1582 src: /192.168.6.237:53910 dest: /192.168.6.249:50010
2015-11-27 04:10:09,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53910, dest: /192.168.6.249:50010, bytes: 2269452, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1546805183_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742405_1582, duration: 243550376
2015-11-27 04:10:09,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742405_1582, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:10:12,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742406_1583 src: /192.168.6.238:47980 dest: /192.168.6.249:50010
2015-11-27 04:10:12,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:47980, dest: /192.168.6.249:50010, bytes: 2274079, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1313171196_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742406_1583, duration: 245710616
2015-11-27 04:10:12,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742406_1583, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:10:14,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742407_1584 src: /192.168.6.237:53911 dest: /192.168.6.249:50010
2015-11-27 04:10:14,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53911, dest: /192.168.6.249:50010, bytes: 2267787, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1491561210_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742407_1584, duration: 243736680
2015-11-27 04:10:14,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742407_1584, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:10:16,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742408_1585 src: /192.168.6.248:40425 dest: /192.168.6.249:50010
2015-11-27 04:10:16,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40425, dest: /192.168.6.249:50010, bytes: 2385603, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2122912683_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742408_1585, duration: 255580381
2015-11-27 04:10:16,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742408_1585, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:10:19,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742409_1586 src: /192.168.6.237:53914 dest: /192.168.6.249:50010
2015-11-27 04:10:19,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53914, dest: /192.168.6.249:50010, bytes: 2273388, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-994562952_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742409_1586, duration: 244480247
2015-11-27 04:10:19,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742409_1586, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:10:23,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742411_1588 src: /192.168.6.248:40434 dest: /192.168.6.249:50010
2015-11-27 04:10:24,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40434, dest: /192.168.6.249:50010, bytes: 2270531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1978097543_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742411_1588, duration: 245451956
2015-11-27 04:10:24,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742411_1588, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:10:26,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742412_1589 src: /192.168.6.238:47983 dest: /192.168.6.249:50010
2015-11-27 04:10:26,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:47983, dest: /192.168.6.249:50010, bytes: 2270886, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-433820038_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742412_1589, duration: 245815114
2015-11-27 04:10:26,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742412_1589, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:10:28,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742413_1590 src: /192.168.6.237:53924 dest: /192.168.6.249:50010
2015-11-27 04:10:29,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53924, dest: /192.168.6.249:50010, bytes: 2395800, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_444249520_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742413_1590, duration: 256034993
2015-11-27 04:10:29,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742413_1590, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:10:31,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742414_1591 src: /192.168.6.248:40449 dest: /192.168.6.249:50010
2015-11-27 04:10:31,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40449, dest: /192.168.6.249:50010, bytes: 2269527, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2107828724_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742414_1591, duration: 245599382
2015-11-27 04:10:31,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742414_1591, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:10:33,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742415_1592 src: /192.168.6.237:53925 dest: /192.168.6.249:50010
2015-11-27 04:10:33,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53925, dest: /192.168.6.249:50010, bytes: 2269234, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_954871241_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742415_1592, duration: 248557771
2015-11-27 04:10:33,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742415_1592, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:10:38,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742417_1594 src: /192.168.6.248:40458 dest: /192.168.6.249:50010
2015-11-27 04:10:38,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40458, dest: /192.168.6.249:50010, bytes: 2264843, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1322734136_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742417_1594, duration: 244836464
2015-11-27 04:10:38,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742417_1594, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:10:40,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742418_1595 src: /192.168.6.248:40461 dest: /192.168.6.249:50010
2015-11-27 04:10:41,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40461, dest: /192.168.6.249:50010, bytes: 2389217, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2040685034_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742418_1595, duration: 256991578
2015-11-27 04:10:41,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742418_1595, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:10:45,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742420_1597 src: /192.168.6.238:47994 dest: /192.168.6.249:50010
2015-11-27 04:10:45,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:47994, dest: /192.168.6.249:50010, bytes: 2271002, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-898202297_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742420_1597, duration: 243999684
2015-11-27 04:10:45,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742420_1597, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:10:50,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742422_1599 src: /192.168.6.248:40473 dest: /192.168.6.249:50010
2015-11-27 04:10:50,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40473, dest: /192.168.6.249:50010, bytes: 2272153, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_779290810_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742422_1599, duration: 244739015
2015-11-27 04:10:50,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742422_1599, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:10:54,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742424_1601 src: /192.168.6.248:40483 dest: /192.168.6.249:50010
2015-11-27 04:10:55,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40483, dest: /192.168.6.249:50010, bytes: 2272985, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1840817464_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742424_1601, duration: 245144401
2015-11-27 04:10:55,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742424_1601, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:10:57,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742425_1602 src: /192.168.6.237:53929 dest: /192.168.6.249:50010
2015-11-27 04:10:57,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53929, dest: /192.168.6.249:50010, bytes: 2275600, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_507254154_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742425_1602, duration: 244950936
2015-11-27 04:10:57,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742425_1602, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:10:59,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742426_1603 src: /192.168.6.238:47996 dest: /192.168.6.249:50010
2015-11-27 04:10:59,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:47996, dest: /192.168.6.249:50010, bytes: 2276131, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1237103931_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742426_1603, duration: 244471992
2015-11-27 04:10:59,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742426_1603, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:11:01,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742427_1604 src: /192.168.6.248:40495 dest: /192.168.6.249:50010
2015-11-27 04:11:02,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40495, dest: /192.168.6.249:50010, bytes: 2267440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-690563768_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742427_1604, duration: 248543164
2015-11-27 04:11:02,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742427_1604, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:11:04,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742428_1605 src: /192.168.6.237:53937 dest: /192.168.6.249:50010
2015-11-27 04:11:04,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53937, dest: /192.168.6.249:50010, bytes: 2390920, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-74694203_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742428_1605, duration: 255722130
2015-11-27 04:11:04,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742428_1605, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:11:11,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742431_1608 src: /192.168.6.238:48006 dest: /192.168.6.249:50010
2015-11-27 04:11:11,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48006, dest: /192.168.6.249:50010, bytes: 2274516, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-10474375_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742431_1608, duration: 253289032
2015-11-27 04:11:11,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742431_1608, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:11:16,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742433_1610 src: /192.168.6.238:48011 dest: /192.168.6.249:50010
2015-11-27 04:11:16,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48011, dest: /192.168.6.249:50010, bytes: 2389961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1672726040_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742433_1610, duration: 254169045
2015-11-27 04:11:16,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742433_1610, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:11:18,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742434_1611 src: /192.168.6.238:48012 dest: /192.168.6.249:50010
2015-11-27 04:11:19,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48012, dest: /192.168.6.249:50010, bytes: 2289411, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1242412575_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742434_1611, duration: 249179945
2015-11-27 04:11:19,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742434_1611, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:11:23,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742436_1613 src: /192.168.6.248:40522 dest: /192.168.6.249:50010
2015-11-27 04:11:23,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40522, dest: /192.168.6.249:50010, bytes: 2290782, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-943589859_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742436_1613, duration: 249486040
2015-11-27 04:11:23,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742436_1613, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:11:25,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742437_1614 src: /192.168.6.248:40530 dest: /192.168.6.249:50010
2015-11-27 04:11:26,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40530, dest: /192.168.6.249:50010, bytes: 2292847, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1660110734_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742437_1614, duration: 248478827
2015-11-27 04:11:26,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742437_1614, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:11:28,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742438_1615 src: /192.168.6.248:40534 dest: /192.168.6.249:50010
2015-11-27 04:11:28,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40534, dest: /192.168.6.249:50010, bytes: 2417443, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1905048515_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742438_1615, duration: 258241769
2015-11-27 04:11:28,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742438_1615, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:11:33,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742440_1617 src: /192.168.6.238:48013 dest: /192.168.6.249:50010
2015-11-27 04:11:33,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48013, dest: /192.168.6.249:50010, bytes: 2289573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1424057413_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742440_1617, duration: 246103261
2015-11-27 04:11:33,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742440_1617, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:11:35,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742441_1618 src: /192.168.6.248:40543 dest: /192.168.6.249:50010
2015-11-27 04:11:35,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40543, dest: /192.168.6.249:50010, bytes: 2293427, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1893650872_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742441_1618, duration: 247898282
2015-11-27 04:11:35,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742441_1618, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:11:37,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742442_1619 src: /192.168.6.238:48014 dest: /192.168.6.249:50010
2015-11-27 04:11:37,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48014, dest: /192.168.6.249:50010, bytes: 2288666, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1067645573_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742442_1619, duration: 245921901
2015-11-27 04:11:37,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742442_1619, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:11:40,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742443_1620 src: /192.168.6.248:40549 dest: /192.168.6.249:50010
2015-11-27 04:11:40,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40549, dest: /192.168.6.249:50010, bytes: 2411194, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2011779315_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742443_1620, duration: 256855915
2015-11-27 04:11:40,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742443_1620, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:11:42,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742444_1621 src: /192.168.6.237:53950 dest: /192.168.6.249:50010
2015-11-27 04:11:42,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53950, dest: /192.168.6.249:50010, bytes: 2285174, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-869177167_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742444_1621, duration: 244889685
2015-11-27 04:11:42,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742444_1621, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:11:44,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742445_1622 src: /192.168.6.248:40555 dest: /192.168.6.249:50010
2015-11-27 04:11:45,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40555, dest: /192.168.6.249:50010, bytes: 2292583, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_584425715_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742445_1622, duration: 246623273
2015-11-27 04:11:45,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742445_1622, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:11:47,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742446_1623 src: /192.168.6.248:40558 dest: /192.168.6.249:50010
2015-11-27 04:11:47,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40558, dest: /192.168.6.249:50010, bytes: 2294051, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-857097577_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742446_1623, duration: 247321222
2015-11-27 04:11:47,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742446_1623, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:11:51,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742448_1625 src: /192.168.6.248:40564 dest: /192.168.6.249:50010
2015-11-27 04:11:52,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40564, dest: /192.168.6.249:50010, bytes: 2405524, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_143882097_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742448_1625, duration: 258203949
2015-11-27 04:11:52,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742448_1625, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:11:54,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742449_1626 src: /192.168.6.248:40568 dest: /192.168.6.249:50010
2015-11-27 04:11:54,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40568, dest: /192.168.6.249:50010, bytes: 2289633, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1766073522_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742449_1626, duration: 246106277
2015-11-27 04:11:54,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742449_1626, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:11:56,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742450_1627 src: /192.168.6.237:53957 dest: /192.168.6.249:50010
2015-11-27 04:11:56,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53957, dest: /192.168.6.249:50010, bytes: 2293870, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1929397648_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742450_1627, duration: 246004880
2015-11-27 04:11:56,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742450_1627, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:11:58,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742451_1628 src: /192.168.6.248:40580 dest: /192.168.6.249:50010
2015-11-27 04:11:58,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40580, dest: /192.168.6.249:50010, bytes: 2294634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-59698746_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742451_1628, duration: 246751259
2015-11-27 04:11:58,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742451_1628, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:12:01,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742452_1629 src: /192.168.6.238:48027 dest: /192.168.6.249:50010
2015-11-27 04:12:01,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48027, dest: /192.168.6.249:50010, bytes: 2291622, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_880480733_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742452_1629, duration: 246180290
2015-11-27 04:12:01,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742452_1629, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:12:05,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742454_1631 src: /192.168.6.248:40589 dest: /192.168.6.249:50010
2015-11-27 04:12:06,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40589, dest: /192.168.6.249:50010, bytes: 2285973, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1404823623_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742454_1631, duration: 246914173
2015-11-27 04:12:06,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742454_1631, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:12:08,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742455_1632 src: /192.168.6.238:48028 dest: /192.168.6.249:50010
2015-11-27 04:12:08,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48028, dest: /192.168.6.249:50010, bytes: 2288342, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1222421418_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742455_1632, duration: 244968131
2015-11-27 04:12:08,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742455_1632, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:12:10,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742456_1633 src: /192.168.6.237:53959 dest: /192.168.6.249:50010
2015-11-27 04:12:10,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53959, dest: /192.168.6.249:50010, bytes: 2281696, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66515687_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742456_1633, duration: 247119271
2015-11-27 04:12:10,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742456_1633, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:12:13,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742457_1634 src: /192.168.6.237:53960 dest: /192.168.6.249:50010
2015-11-27 04:12:13,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53960, dest: /192.168.6.249:50010, bytes: 2292596, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_532562631_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742457_1634, duration: 247898716
2015-11-27 04:12:13,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742457_1634, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:12:22,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742461_1638 src: /192.168.6.248:40610 dest: /192.168.6.249:50010
2015-11-27 04:12:22,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40610, dest: /192.168.6.249:50010, bytes: 2282382, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1270462564_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742461_1638, duration: 246428515
2015-11-27 04:12:22,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742461_1638, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:12:24,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742462_1639 src: /192.168.6.248:40616 dest: /192.168.6.249:50010
2015-11-27 04:12:25,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40616, dest: /192.168.6.249:50010, bytes: 2294380, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1119755314_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742462_1639, duration: 247356444
2015-11-27 04:12:25,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742462_1639, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:12:29,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742464_1641 src: /192.168.6.248:40625 dest: /192.168.6.249:50010
2015-11-27 04:12:29,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40625, dest: /192.168.6.249:50010, bytes: 2290091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_587936907_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742464_1641, duration: 245903276
2015-11-27 04:12:29,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742464_1641, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:12:38,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742468_1645 src: /192.168.6.237:53975 dest: /192.168.6.249:50010
2015-11-27 04:12:39,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53975, dest: /192.168.6.249:50010, bytes: 2406340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548125119_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742468_1645, duration: 256695711
2015-11-27 04:12:39,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742468_1645, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:12:41,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742469_1646 src: /192.168.6.248:40640 dest: /192.168.6.249:50010
2015-11-27 04:12:41,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40640, dest: /192.168.6.249:50010, bytes: 2287959, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1670137118_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742469_1646, duration: 246724040
2015-11-27 04:12:41,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742469_1646, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:12:43,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742470_1647 src: /192.168.6.237:53976 dest: /192.168.6.249:50010
2015-11-27 04:12:44,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53976, dest: /192.168.6.249:50010, bytes: 2290934, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1673511722_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742470_1647, duration: 248297727
2015-11-27 04:12:44,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742470_1647, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:12:46,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742471_1648 src: /192.168.6.238:48044 dest: /192.168.6.249:50010
2015-11-27 04:12:46,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48044, dest: /192.168.6.249:50010, bytes: 2294640, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-231086332_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742471_1648, duration: 246495231
2015-11-27 04:12:46,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742471_1648, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:12:48,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742472_1649 src: /192.168.6.238:48045 dest: /192.168.6.249:50010
2015-11-27 04:12:48,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48045, dest: /192.168.6.249:50010, bytes: 2290170, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_588758593_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742472_1649, duration: 245187755
2015-11-27 04:12:48,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742472_1649, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:12:50,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742473_1650 src: /192.168.6.238:48046 dest: /192.168.6.249:50010
2015-11-27 04:12:51,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48046, dest: /192.168.6.249:50010, bytes: 2405103, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1288854922_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742473_1650, duration: 255562633
2015-11-27 04:12:51,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742473_1650, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:12:53,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742474_1651 src: /192.168.6.237:53977 dest: /192.168.6.249:50010
2015-11-27 04:12:53,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53977, dest: /192.168.6.249:50010, bytes: 2289531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1204476918_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742474_1651, duration: 245038818
2015-11-27 04:12:53,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742474_1651, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:12:57,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742476_1653 src: /192.168.6.237:53979 dest: /192.168.6.249:50010
2015-11-27 04:12:58,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53979, dest: /192.168.6.249:50010, bytes: 2293699, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1738006446_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742476_1653, duration: 246450589
2015-11-27 04:12:58,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742476_1653, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:00,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742477_1654 src: /192.168.6.237:53983 dest: /192.168.6.249:50010
2015-11-27 04:13:00,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53983, dest: /192.168.6.249:50010, bytes: 2287442, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1883509908_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742477_1654, duration: 245883857
2015-11-27 04:13:00,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742477_1654, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:02,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742478_1655 src: /192.168.6.238:48047 dest: /192.168.6.249:50010
2015-11-27 04:13:02,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48047, dest: /192.168.6.249:50010, bytes: 2410600, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-490365154_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742478_1655, duration: 255945492
2015-11-27 04:13:02,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742478_1655, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:07,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742480_1657 src: /192.168.6.237:53992 dest: /192.168.6.249:50010
2015-11-27 04:13:07,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53992, dest: /192.168.6.249:50010, bytes: 2290058, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1631830163_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742480_1657, duration: 245644936
2015-11-27 04:13:07,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742480_1657, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:09,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742481_1658 src: /192.168.6.248:40683 dest: /192.168.6.249:50010
2015-11-27 04:13:09,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40683, dest: /192.168.6.249:50010, bytes: 2297579, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_112176155_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742481_1658, duration: 247280870
2015-11-27 04:13:09,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742481_1658, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:13:11,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742482_1659 src: /192.168.6.248:40686 dest: /192.168.6.249:50010
2015-11-27 04:13:12,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40686, dest: /192.168.6.249:50010, bytes: 2288728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1539988541_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742482_1659, duration: 247680253
2015-11-27 04:13:12,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742482_1659, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:13:18,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742485_1662 src: /192.168.6.248:40695 dest: /192.168.6.249:50010
2015-11-27 04:13:19,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40695, dest: /192.168.6.249:50010, bytes: 2295070, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-595900108_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742485_1662, duration: 247570345
2015-11-27 04:13:19,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742485_1662, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:13:21,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742486_1663 src: /192.168.6.248:40698 dest: /192.168.6.249:50010
2015-11-27 04:13:21,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40698, dest: /192.168.6.249:50010, bytes: 2290339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1581369795_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742486_1663, duration: 245742508
2015-11-27 04:13:21,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742486_1663, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:13:23,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742487_1664 src: /192.168.6.248:40701 dest: /192.168.6.249:50010
2015-11-27 04:13:23,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40701, dest: /192.168.6.249:50010, bytes: 2290579, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1004951633_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742487_1664, duration: 246891437
2015-11-27 04:13:23,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742487_1664, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:13:25,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742488_1665 src: /192.168.6.238:48061 dest: /192.168.6.249:50010
2015-11-27 04:13:26,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48061, dest: /192.168.6.249:50010, bytes: 2290451, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2020991002_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742488_1665, duration: 245394300
2015-11-27 04:13:26,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742488_1665, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:28,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742489_1666 src: /192.168.6.238:48062 dest: /192.168.6.249:50010
2015-11-27 04:13:28,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48062, dest: /192.168.6.249:50010, bytes: 2291575, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1286189244_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742489_1666, duration: 248046810
2015-11-27 04:13:28,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742489_1666, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:30,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742490_1667 src: /192.168.6.248:40716 dest: /192.168.6.249:50010
2015-11-27 04:13:30,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40716, dest: /192.168.6.249:50010, bytes: 2293784, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_349574531_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742490_1667, duration: 246638923
2015-11-27 04:13:30,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742490_1667, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:13:32,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742491_1668 src: /192.168.6.237:53994 dest: /192.168.6.249:50010
2015-11-27 04:13:33,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:53994, dest: /192.168.6.249:50010, bytes: 2291618, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_264554633_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742491_1668, duration: 246320217
2015-11-27 04:13:33,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742491_1668, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:35,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742492_1669 src: /192.168.6.238:48063 dest: /192.168.6.249:50010
2015-11-27 04:13:35,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48063, dest: /192.168.6.249:50010, bytes: 2292379, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_347390061_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742492_1669, duration: 245463425
2015-11-27 04:13:35,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742492_1669, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:37,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742493_1670 src: /192.168.6.238:48064 dest: /192.168.6.249:50010
2015-11-27 04:13:37,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48064, dest: /192.168.6.249:50010, bytes: 2413436, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1372984698_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742493_1670, duration: 255454929
2015-11-27 04:13:37,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742493_1670, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:42,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742495_1672 src: /192.168.6.238:48065 dest: /192.168.6.249:50010
2015-11-27 04:13:42,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48065, dest: /192.168.6.249:50010, bytes: 2291028, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_731875983_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742495_1672, duration: 245250003
2015-11-27 04:13:42,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742495_1672, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:44,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742496_1673 src: /192.168.6.238:48066 dest: /192.168.6.249:50010
2015-11-27 04:13:45,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48066, dest: /192.168.6.249:50010, bytes: 2290339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1833442713_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742496_1673, duration: 245085148
2015-11-27 04:13:45,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742496_1673, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:54,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742500_1677 src: /192.168.6.248:40748 dest: /192.168.6.249:50010
2015-11-27 04:13:54,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40748, dest: /192.168.6.249:50010, bytes: 2287770, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_140349992_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742500_1677, duration: 247510612
2015-11-27 04:13:54,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742500_1677, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:13:56,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742501_1678 src: /192.168.6.238:48081 dest: /192.168.6.249:50010
2015-11-27 04:13:56,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48081, dest: /192.168.6.249:50010, bytes: 2281341, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1247573846_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742501_1678, duration: 245059056
2015-11-27 04:13:56,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742501_1678, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:13:58,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742502_1679 src: /192.168.6.248:40759 dest: /192.168.6.249:50010
2015-11-27 04:13:59,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40759, dest: /192.168.6.249:50010, bytes: 2293076, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2035452382_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742502_1679, duration: 246813804
2015-11-27 04:13:59,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742502_1679, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:14:01,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742503_1680 src: /192.168.6.238:48082 dest: /192.168.6.249:50010
2015-11-27 04:14:01,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48082, dest: /192.168.6.249:50010, bytes: 2409139, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1818995334_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742503_1680, duration: 255498590
2015-11-27 04:14:01,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742503_1680, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:14:03,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742504_1681 src: /192.168.6.248:40765 dest: /192.168.6.249:50010
2015-11-27 04:14:03,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40765, dest: /192.168.6.249:50010, bytes: 2293932, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1575312172_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742504_1681, duration: 246521325
2015-11-27 04:14:03,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742504_1681, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:14:08,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742506_1683 src: /192.168.6.238:48084 dest: /192.168.6.249:50010
2015-11-27 04:14:08,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48084, dest: /192.168.6.249:50010, bytes: 2285595, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-604575427_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742506_1683, duration: 245481392
2015-11-27 04:14:08,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742506_1683, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:14:10,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742507_1684 src: /192.168.6.237:54007 dest: /192.168.6.249:50010
2015-11-27 04:14:10,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54007, dest: /192.168.6.249:50010, bytes: 2286577, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1975174841_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742507_1684, duration: 245672511
2015-11-27 04:14:10,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742507_1684, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:14:12,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742508_1685 src: /192.168.6.248:40777 dest: /192.168.6.249:50010
2015-11-27 04:14:13,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40777, dest: /192.168.6.249:50010, bytes: 2410555, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-242709526_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742508_1685, duration: 257627581
2015-11-27 04:14:13,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742508_1685, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:14:15,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742509_1686 src: /192.168.6.237:54008 dest: /192.168.6.249:50010
2015-11-27 04:14:15,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54008, dest: /192.168.6.249:50010, bytes: 2289188, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_382709483_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742509_1686, duration: 248050830
2015-11-27 04:14:15,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742509_1686, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:14:17,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742510_1687 src: /192.168.6.248:40783 dest: /192.168.6.249:50010
2015-11-27 04:14:17,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40783, dest: /192.168.6.249:50010, bytes: 2294779, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1204064585_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742510_1687, duration: 247032055
2015-11-27 04:14:17,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742510_1687, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:14:19,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742511_1688 src: /192.168.6.248:40786 dest: /192.168.6.249:50010
2015-11-27 04:14:20,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40786, dest: /192.168.6.249:50010, bytes: 2291583, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_756168246_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742511_1688, duration: 246281146
2015-11-27 04:14:20,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742511_1688, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:14:22,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742512_1689 src: /192.168.6.248:40789 dest: /192.168.6.249:50010
2015-11-27 04:14:22,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40789, dest: /192.168.6.249:50010, bytes: 2290985, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_477747833_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742512_1689, duration: 247167933
2015-11-27 04:14:22,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742512_1689, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:14:24,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742513_1690 src: /192.168.6.238:48085 dest: /192.168.6.249:50010
2015-11-27 04:14:24,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48085, dest: /192.168.6.249:50010, bytes: 2404324, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2140082222_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742513_1690, duration: 255735750
2015-11-27 04:14:24,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742513_1690, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:14:26,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742514_1691 src: /192.168.6.248:40801 dest: /192.168.6.249:50010
2015-11-27 04:14:27,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40801, dest: /192.168.6.249:50010, bytes: 2291599, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1585879922_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742514_1691, duration: 246502295
2015-11-27 04:14:27,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742514_1691, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:14:29,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742515_1692 src: /192.168.6.248:40804 dest: /192.168.6.249:50010
2015-11-27 04:14:29,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40804, dest: /192.168.6.249:50010, bytes: 2289176, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_981701545_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742515_1692, duration: 247247905
2015-11-27 04:14:29,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742515_1692, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:14:31,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742516_1693 src: /192.168.6.238:48093 dest: /192.168.6.249:50010
2015-11-27 04:14:32,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48093, dest: /192.168.6.249:50010, bytes: 2288806, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2039814022_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742516_1693, duration: 245650364
2015-11-27 04:14:32,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742516_1693, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:14:36,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742518_1695 src: /192.168.6.238:48099 dest: /192.168.6.249:50010
2015-11-27 04:14:36,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48099, dest: /192.168.6.249:50010, bytes: 2409715, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_665135682_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742518_1695, duration: 252846618
2015-11-27 04:14:36,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742518_1695, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:14:38,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742519_1696 src: /192.168.6.237:54020 dest: /192.168.6.249:50010
2015-11-27 04:14:39,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54020, dest: /192.168.6.249:50010, bytes: 2291136, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1469125467_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742519_1696, duration: 246114254
2015-11-27 04:14:39,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742519_1696, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:14:43,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742521_1698 src: /192.168.6.238:48101 dest: /192.168.6.249:50010
2015-11-27 04:14:43,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48101, dest: /192.168.6.249:50010, bytes: 2280566, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_448519385_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742521_1698, duration: 244487082
2015-11-27 04:14:43,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742521_1698, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:14:48,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742523_1700 src: /192.168.6.248:40828 dest: /192.168.6.249:50010
2015-11-27 04:14:48,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40828, dest: /192.168.6.249:50010, bytes: 2415405, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1507235687_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742523_1700, duration: 257743582
2015-11-27 04:14:48,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742523_1700, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:14:55,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742526_1703 src: /192.168.6.248:40841 dest: /192.168.6.249:50010
2015-11-27 04:14:55,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40841, dest: /192.168.6.249:50010, bytes: 2285659, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-528799370_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742526_1703, duration: 245750329
2015-11-27 04:14:55,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742526_1703, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:14:57,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742527_1704 src: /192.168.6.248:40847 dest: /192.168.6.249:50010
2015-11-27 04:14:57,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40847, dest: /192.168.6.249:50010, bytes: 2286916, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1991565011_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742527_1704, duration: 246843183
2015-11-27 04:14:57,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742527_1704, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:15:02,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742529_1706 src: /192.168.6.248:40853 dest: /192.168.6.249:50010
2015-11-27 04:15:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40853, dest: /192.168.6.249:50010, bytes: 2295497, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-895026993_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742529_1706, duration: 246452457
2015-11-27 04:15:02,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742529_1706, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:15:04,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742530_1707 src: /192.168.6.248:40856 dest: /192.168.6.249:50010
2015-11-27 04:15:04,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40856, dest: /192.168.6.249:50010, bytes: 2295421, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1988921987_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742530_1707, duration: 247186627
2015-11-27 04:15:04,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742530_1707, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:15:06,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742531_1708 src: /192.168.6.237:54033 dest: /192.168.6.249:50010
2015-11-27 04:15:07,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54033, dest: /192.168.6.249:50010, bytes: 2290787, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1730978907_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742531_1708, duration: 244612326
2015-11-27 04:15:07,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742531_1708, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:15:09,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742532_1709 src: /192.168.6.238:48107 dest: /192.168.6.249:50010
2015-11-27 04:15:09,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48107, dest: /192.168.6.249:50010, bytes: 2290510, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2114056552_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742532_1709, duration: 246992932
2015-11-27 04:15:09,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742532_1709, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:15:13,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742534_1711 src: /192.168.6.248:40868 dest: /192.168.6.249:50010
2015-11-27 04:15:14,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40868, dest: /192.168.6.249:50010, bytes: 2286193, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-380614335_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742534_1711, duration: 246052713
2015-11-27 04:15:14,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742534_1711, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:15:18,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742536_1713 src: /192.168.6.237:54038 dest: /192.168.6.249:50010
2015-11-27 04:15:18,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54038, dest: /192.168.6.249:50010, bytes: 2293569, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66887566_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742536_1713, duration: 246058043
2015-11-27 04:15:18,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742536_1713, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:15:21,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742537_1714 src: /192.168.6.237:54039 dest: /192.168.6.249:50010
2015-11-27 04:15:21,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54039, dest: /192.168.6.249:50010, bytes: 2292344, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2140712317_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742537_1714, duration: 258856798
2015-11-27 04:15:21,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742537_1714, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:15:23,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742538_1715 src: /192.168.6.238:48115 dest: /192.168.6.249:50010
2015-11-27 04:15:23,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48115, dest: /192.168.6.249:50010, bytes: 2412010, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_534199632_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742538_1715, duration: 256621384
2015-11-27 04:15:23,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742538_1715, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:15:25,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742539_1716 src: /192.168.6.237:54040 dest: /192.168.6.249:50010
2015-11-27 04:15:25,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54040, dest: /192.168.6.249:50010, bytes: 2294494, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-388834464_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742539_1716, duration: 245878696
2015-11-27 04:15:25,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742539_1716, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:15:28,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742540_1717 src: /192.168.6.248:40892 dest: /192.168.6.249:50010
2015-11-27 04:15:28,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40892, dest: /192.168.6.249:50010, bytes: 2290858, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1867781145_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742540_1717, duration: 246609652
2015-11-27 04:15:28,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742540_1717, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:15:30,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742541_1718 src: /192.168.6.238:48116 dest: /192.168.6.249:50010
2015-11-27 04:15:30,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48116, dest: /192.168.6.249:50010, bytes: 2285050, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1403339187_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742541_1718, duration: 245473670
2015-11-27 04:15:30,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742541_1718, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:15:42,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742546_1723 src: /192.168.6.248:40910 dest: /192.168.6.249:50010
2015-11-27 04:15:42,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40910, dest: /192.168.6.249:50010, bytes: 2287033, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1288091342_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742546_1723, duration: 246038198
2015-11-27 04:15:42,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742546_1723, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:15:49,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742549_1726 src: /192.168.6.248:40919 dest: /192.168.6.249:50010
2015-11-27 04:15:49,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40919, dest: /192.168.6.249:50010, bytes: 2292936, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1596964327_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742549_1726, duration: 246422764
2015-11-27 04:15:49,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742549_1726, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:15:53,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742551_1728 src: /192.168.6.248:40926 dest: /192.168.6.249:50010
2015-11-27 04:15:54,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40926, dest: /192.168.6.249:50010, bytes: 2295193, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1064300927_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742551_1728, duration: 246538504
2015-11-27 04:15:54,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742551_1728, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:15:56,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742552_1729 src: /192.168.6.238:48132 dest: /192.168.6.249:50010
2015-11-27 04:15:56,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48132, dest: /192.168.6.249:50010, bytes: 2295340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1063432796_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742552_1729, duration: 246783600
2015-11-27 04:15:56,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742552_1729, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:16:00,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742554_1731 src: /192.168.6.248:40941 dest: /192.168.6.249:50010
2015-11-27 04:16:01,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40941, dest: /192.168.6.249:50010, bytes: 2288391, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1409549092_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742554_1731, duration: 246548223
2015-11-27 04:16:01,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742554_1731, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:16:05,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742556_1733 src: /192.168.6.248:40947 dest: /192.168.6.249:50010
2015-11-27 04:16:05,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40947, dest: /192.168.6.249:50010, bytes: 2291457, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_918250182_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742556_1733, duration: 246548342
2015-11-27 04:16:05,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742556_1733, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:16:07,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742557_1734 src: /192.168.6.248:40950 dest: /192.168.6.249:50010
2015-11-27 04:16:08,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40950, dest: /192.168.6.249:50010, bytes: 2296212, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1444621994_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742557_1734, duration: 247631259
2015-11-27 04:16:08,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742557_1734, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:16:10,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742558_1735 src: /192.168.6.248:40953 dest: /192.168.6.249:50010
2015-11-27 04:16:10,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40953, dest: /192.168.6.249:50010, bytes: 2410532, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1817662981_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742558_1735, duration: 257403657
2015-11-27 04:16:10,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742558_1735, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:16:12,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742559_1736 src: /192.168.6.237:54056 dest: /192.168.6.249:50010
2015-11-27 04:16:12,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54056, dest: /192.168.6.249:50010, bytes: 2285163, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_363775666_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742559_1736, duration: 245607398
2015-11-27 04:16:12,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742559_1736, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:16:14,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742560_1737 src: /192.168.6.248:40959 dest: /192.168.6.249:50010
2015-11-27 04:16:15,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40959, dest: /192.168.6.249:50010, bytes: 2285120, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1287098855_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742560_1737, duration: 246784995
2015-11-27 04:16:15,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742560_1737, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:16:17,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742561_1738 src: /192.168.6.248:40962 dest: /192.168.6.249:50010
2015-11-27 04:16:17,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40962, dest: /192.168.6.249:50010, bytes: 2289274, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_38346166_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742561_1738, duration: 249065014
2015-11-27 04:16:17,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742561_1738, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:16:19,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742562_1739 src: /192.168.6.248:40965 dest: /192.168.6.249:50010
2015-11-27 04:16:19,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40965, dest: /192.168.6.249:50010, bytes: 2292194, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_482988911_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742562_1739, duration: 246948888
2015-11-27 04:16:19,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742562_1739, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:16:24,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742564_1741 src: /192.168.6.248:40971 dest: /192.168.6.249:50010
2015-11-27 04:16:24,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40971, dest: /192.168.6.249:50010, bytes: 2291891, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_453490172_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742564_1741, duration: 247208861
2015-11-27 04:16:24,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742564_1741, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:16:26,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742565_1742 src: /192.168.6.238:48136 dest: /192.168.6.249:50010
2015-11-27 04:16:26,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48136, dest: /192.168.6.249:50010, bytes: 2291798, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1629623637_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742565_1742, duration: 246040806
2015-11-27 04:16:26,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742565_1742, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:16:31,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742567_1744 src: /192.168.6.248:40986 dest: /192.168.6.249:50010
2015-11-27 04:16:31,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40986, dest: /192.168.6.249:50010, bytes: 2290727, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-297124875_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742567_1744, duration: 246606639
2015-11-27 04:16:31,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742567_1744, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:16:33,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742568_1745 src: /192.168.6.248:40989 dest: /192.168.6.249:50010
2015-11-27 04:16:33,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40989, dest: /192.168.6.249:50010, bytes: 2411475, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-823563717_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742568_1745, duration: 257059834
2015-11-27 04:16:33,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742568_1745, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:16:35,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742569_1746 src: /192.168.6.238:48147 dest: /192.168.6.249:50010
2015-11-27 04:16:36,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48147, dest: /192.168.6.249:50010, bytes: 2289378, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1659635363_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742569_1746, duration: 245291492
2015-11-27 04:16:36,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742569_1746, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:16:38,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742570_1747 src: /192.168.6.248:40995 dest: /192.168.6.249:50010
2015-11-27 04:16:38,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:40995, dest: /192.168.6.249:50010, bytes: 2290641, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-325135083_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742570_1747, duration: 247020638
2015-11-27 04:16:38,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742570_1747, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:16:40,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742571_1748 src: /192.168.6.238:48148 dest: /192.168.6.249:50010
2015-11-27 04:16:40,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48148, dest: /192.168.6.249:50010, bytes: 2289353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2100375598_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742571_1748, duration: 245611706
2015-11-27 04:16:40,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742571_1748, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:16:42,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742572_1749 src: /192.168.6.238:48149 dest: /192.168.6.249:50010
2015-11-27 04:16:43,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48149, dest: /192.168.6.249:50010, bytes: 2284789, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-801357073_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742572_1749, duration: 244322960
2015-11-27 04:16:43,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742572_1749, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:16:45,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742573_1750 src: /192.168.6.238:48150 dest: /192.168.6.249:50010
2015-11-27 04:16:45,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48150, dest: /192.168.6.249:50010, bytes: 2405465, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_883669965_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742573_1750, duration: 258448785
2015-11-27 04:16:45,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742573_1750, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:16:49,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742575_1752 src: /192.168.6.237:54070 dest: /192.168.6.249:50010
2015-11-27 04:16:50,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54070, dest: /192.168.6.249:50010, bytes: 2294357, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1526895203_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742575_1752, duration: 246585415
2015-11-27 04:16:50,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742575_1752, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:16:52,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742576_1753 src: /192.168.6.238:48151 dest: /192.168.6.249:50010
2015-11-27 04:16:52,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48151, dest: /192.168.6.249:50010, bytes: 2286718, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1778170340_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742576_1753, duration: 246455344
2015-11-27 04:16:52,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742576_1753, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:16:54,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742577_1754 src: /192.168.6.238:48152 dest: /192.168.6.249:50010
2015-11-27 04:16:54,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48152, dest: /192.168.6.249:50010, bytes: 2291085, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-466440332_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742577_1754, duration: 245351390
2015-11-27 04:16:54,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742577_1754, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:16:57,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742578_1755 src: /192.168.6.237:54071 dest: /192.168.6.249:50010
2015-11-27 04:16:57,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54071, dest: /192.168.6.249:50010, bytes: 2406971, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-983385689_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742578_1755, duration: 256306312
2015-11-27 04:16:57,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742578_1755, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:16:59,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742579_1756 src: /192.168.6.238:48153 dest: /192.168.6.249:50010
2015-11-27 04:16:59,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48153, dest: /192.168.6.249:50010, bytes: 2284507, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_428444440_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742579_1756, duration: 287239155
2015-11-27 04:16:59,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742579_1756, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:01,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742580_1757 src: /192.168.6.248:41039 dest: /192.168.6.249:50010
2015-11-27 04:17:01,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:41039, dest: /192.168.6.249:50010, bytes: 2290973, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-256401522_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742580_1757, duration: 251271269
2015-11-27 04:17:01,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742580_1757, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:17:04,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742581_1758 src: /192.168.6.237:54078 dest: /192.168.6.249:50010
2015-11-27 04:17:04,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54078, dest: /192.168.6.249:50010, bytes: 2286440, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1600554755_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742581_1758, duration: 245048357
2015-11-27 04:17:04,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742581_1758, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:08,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742583_1760 src: /192.168.6.238:48158 dest: /192.168.6.249:50010
2015-11-27 04:17:09,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48158, dest: /192.168.6.249:50010, bytes: 2411844, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_270814248_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742583_1760, duration: 256112564
2015-11-27 04:17:09,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742583_1760, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:13,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742585_1762 src: /192.168.6.237:54085 dest: /192.168.6.249:50010
2015-11-27 04:17:13,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54085, dest: /192.168.6.249:50010, bytes: 2289374, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1224349465_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742585_1762, duration: 245059289
2015-11-27 04:17:13,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742585_1762, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:15,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742586_1763 src: /192.168.6.237:54086 dest: /192.168.6.249:50010
2015-11-27 04:17:16,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54086, dest: /192.168.6.249:50010, bytes: 2291717, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1988533966_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742586_1763, duration: 246368853
2015-11-27 04:17:16,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742586_1763, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:18,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742587_1764 src: /192.168.6.237:54087 dest: /192.168.6.249:50010
2015-11-27 04:17:18,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54087, dest: /192.168.6.249:50010, bytes: 2292447, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-909237142_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742587_1764, duration: 245609110
2015-11-27 04:17:18,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742587_1764, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:20,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742588_1765 src: /192.168.6.248:41063 dest: /192.168.6.249:50010
2015-11-27 04:17:20,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:41063, dest: /192.168.6.249:50010, bytes: 2408066, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_229439443_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742588_1765, duration: 256817029
2015-11-27 04:17:20,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742588_1765, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:17:22,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742589_1766 src: /192.168.6.237:54088 dest: /192.168.6.249:50010
2015-11-27 04:17:23,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54088, dest: /192.168.6.249:50010, bytes: 2292458, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1423866499_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742589_1766, duration: 246291444
2015-11-27 04:17:23,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742589_1766, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:25,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742590_1767 src: /192.168.6.237:54089 dest: /192.168.6.249:50010
2015-11-27 04:17:25,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54089, dest: /192.168.6.249:50010, bytes: 2296975, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1693992949_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742590_1767, duration: 254952338
2015-11-27 04:17:25,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742590_1767, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:27,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742591_1768 src: /192.168.6.238:48167 dest: /192.168.6.249:50010
2015-11-27 04:17:27,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48167, dest: /192.168.6.249:50010, bytes: 2277585, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_680410185_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742591_1768, duration: 245385273
2015-11-27 04:17:27,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742591_1768, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:29,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742592_1769 src: /192.168.6.248:41087 dest: /192.168.6.249:50010
2015-11-27 04:17:30,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:41087, dest: /192.168.6.249:50010, bytes: 2288012, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1175118199_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742592_1769, duration: 246757220
2015-11-27 04:17:30,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742592_1769, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:17:32,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742593_1770 src: /192.168.6.238:48168 dest: /192.168.6.249:50010
2015-11-27 04:17:32,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48168, dest: /192.168.6.249:50010, bytes: 2402830, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-581942997_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742593_1770, duration: 254991532
2015-11-27 04:17:32,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742593_1770, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:34,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742594_1771 src: /192.168.6.238:48169 dest: /192.168.6.249:50010
2015-11-27 04:17:35,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:48169, dest: /192.168.6.249:50010, bytes: 2298836, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_803795117_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742594_1771, duration: 246698420
2015-11-27 04:17:35,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742594_1771, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:37,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742595_1772 src: /192.168.6.237:54090 dest: /192.168.6.249:50010
2015-11-27 04:17:37,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.237:54090, dest: /192.168.6.249:50010, bytes: 2291904, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1729750434_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742595_1772, duration: 245408294
2015-11-27 04:17:37,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742595_1772, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-27 04:17:42,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742597_1774 src: /192.168.6.248:41102 dest: /192.168.6.249:50010
2015-11-27 04:17:42,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:41102, dest: /192.168.6.249:50010, bytes: 2289796, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1134970997_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742597_1774, duration: 249303192
2015-11-27 04:17:42,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742597_1774, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:17:44,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742598_1775 src: /192.168.6.248:41105 dest: /192.168.6.249:50010
2015-11-27 04:17:44,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:41105, dest: /192.168.6.249:50010, bytes: 2275376, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-30769806_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742598_1775, duration: 244680675
2015-11-27 04:17:44,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742598_1775, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-27 04:26:03,637 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-27 04:26:10,436 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-27 04:26:10,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-30 04:43:03,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-30 04:43:03,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-30 04:43:05,137 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-30 04:43:05,525 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-30 04:43:05,525 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-30 04:43:05,532 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-30 04:43:05,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-30 04:43:05,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-30 04:43:05,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-30 04:43:05,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-30 04:43:05,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-30 04:43:05,919 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-30 04:43:05,952 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-30 04:43:05,961 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-30 04:43:05,969 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-30 04:43:05,972 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-30 04:43:05,972 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-30 04:43:05,973 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-30 04:43:06,022 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 59816
2015-11-30 04:43:06,022 INFO org.mortbay.log: jetty-6.1.26
2015-11-30 04:43:06,618 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:59816
2015-11-30 04:43:06,741 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-30 04:43:06,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-30 04:43:06,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-30 04:43:06,942 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-30 04:43:06,978 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-30 04:43:07,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-30 04:43:07,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-30 04:43:07,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-30 04:43:07,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-30 04:43:07,248 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-30 04:43:07,252 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-30 04:43:07,630 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3043@rushikesh2
2015-11-30 04:43:07,748 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-30 04:43:07,748 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-30 04:43:07,749 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-30 04:43:07,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-30 04:43:07,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-30 04:43:07,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-30 04:43:07,916 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-30 04:43:07,917 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-30 04:43:07,917 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-30 04:43:07,977 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 60ms
2015-11-30 04:43:07,977 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 61ms
2015-11-30 04:43:07,978 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-30 04:43:08,043 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 66ms
2015-11-30 04:43:08,043 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 66ms
2015-11-30 04:43:08,338 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 669317721 ms.
2015-11-30 04:43:08,340 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448844585340 with interval 21600000
2015-11-30 04:43:08,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-30 04:43:08,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-30 04:43:08,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-30 04:43:08,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3638
2015-11-30 04:43:08,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-30 04:43:08,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3211df4510,  containing 1 storage report(s), of which we sent 1. The reports had 425 total blocks and used 1 RPC(s). This took 17 msec to generate and 96 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-30 04:43:08,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-30 04:43:34,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004 to 192.168.6.238:50010 
2015-11-30 04:43:34,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742174_1350 to 192.168.6.238:50010 
2015-11-30 04:43:36,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073741828_1004 (numBytes=4045946) to /192.168.6.238:50010
2015-11-30 04:43:40,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742165_1341 to 192.168.6.238:50010 
2015-11-30 04:44:20,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742174_1350 (numBytes=134217728) to /192.168.6.238:50010
2015-11-30 04:44:22,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742163_1339 to 192.168.6.238:50010 
2015-11-30 04:44:25,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742165_1341 (numBytes=134217728) to /192.168.6.238:50010
2015-11-30 04:44:28,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742175_1351 to 192.168.6.238:50010 
2015-11-30 04:44:37,192 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-30 04:44:41,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-30 04:44:41,296 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-30 04:44:41,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-11-30 04:45:52,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-11-30 04:45:53,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-11-30 04:45:53,609 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-11-30 04:45:53,672 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-11-30 04:45:53,672 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-11-30 04:45:53,678 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-11-30 04:45:53,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-11-30 04:45:53,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-11-30 04:45:53,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-11-30 04:45:53,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-11-30 04:45:53,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-11-30 04:45:53,795 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-11-30 04:45:53,803 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-11-30 04:45:53,808 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-11-30 04:45:53,813 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-11-30 04:45:53,815 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-11-30 04:45:53,815 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-11-30 04:45:53,815 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-11-30 04:45:53,825 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36386
2015-11-30 04:45:53,825 INFO org.mortbay.log: jetty-6.1.26
2015-11-30 04:45:53,979 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36386
2015-11-30 04:45:54,061 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-11-30 04:45:54,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-11-30 04:45:54,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-11-30 04:45:54,100 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-11-30 04:45:54,111 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-11-30 04:45:54,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-11-30 04:45:54,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-11-30 04:45:54,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-11-30 04:45:54,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-11-30 04:45:54,190 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-11-30 04:45:54,190 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-11-30 04:45:54,442 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3746@rushikesh2
2015-11-30 04:45:54,516 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-11-30 04:45:54,516 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-11-30 04:45:54,516 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-11-30 04:45:54,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-11-30 04:45:54,581 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-11-30 04:45:54,581 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-11-30 04:45:54,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-11-30 04:45:54,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-30 04:45:54,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-30 04:45:54,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35524251648
2015-11-30 04:45:54,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-11-30 04:45:54,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-11-30 04:45:54,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-11-30 04:45:54,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 43ms
2015-11-30 04:45:54,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 43ms
2015-11-30 04:45:54,867 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 669151192 ms.
2015-11-30 04:45:54,869 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448848925869 with interval 21600000
2015-11-30 04:45:54,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-11-30 04:45:54,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-11-30 04:45:54,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-11-30 04:45:54,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3641
2015-11-30 04:45:54,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-11-30 04:45:55,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x58d60a25f4,  containing 1 storage report(s), of which we sent 1. The reports had 425 total blocks and used 1 RPC(s). This took 19 msec to generate and 71 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-11-30 04:45:55,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-11-30 04:56:58,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742600_1777 src: /192.168.6.238:39342 dest: /192.168.6.249:50010
2015-11-30 04:56:58,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:39342, dest: /192.168.6.249:50010, bytes: 87071, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1479103996_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742600_1777, duration: 39556706
2015-11-30 04:56:58,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742600_1777, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-30 04:58:49,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742602_1779 src: /192.168.6.248:54030 dest: /192.168.6.249:50010
2015-11-30 04:58:49,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:54030, dest: /192.168.6.249:50010, bytes: 98305, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1479103996_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742602_1779, duration: 24657375
2015-11-30 04:58:49,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742602_1779, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-30 04:59:13,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742603_1780 src: /192.168.6.248:54169 dest: /192.168.6.249:50010
2015-11-30 04:59:14,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:54169, dest: /192.168.6.249:50010, bytes: 105074, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1479103996_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742603_1780, duration: 101123641
2015-11-30 04:59:14,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742603_1780, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-11-30 04:59:14,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742604_1781 src: /192.168.6.238:39387 dest: /192.168.6.249:50010
2015-11-30 04:59:14,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.238:39387, dest: /192.168.6.249:50010, bytes: 105074, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1479103996_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742604_1781, duration: 25069691
2015-11-30 04:59:14,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742604_1781, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-11-30 04:59:15,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742603_1780 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir3/blk_1073742603 for deletion
2015-11-30 04:59:15,188 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742603_1780 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir3/blk_1073742603
2015-11-30 05:04:42,184 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-11-30 05:04:46,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-11-30 05:04:46,675 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-11-30 05:04:46,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-12-01 04:19:27,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-12-01 04:19:27,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-01 04:19:28,219 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-01 04:19:28,281 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-01 04:19:28,281 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-01 04:19:28,287 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-12-01 04:19:28,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-12-01 04:19:28,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-01 04:19:28,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-01 04:19:28,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-01 04:19:28,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-01 04:19:28,519 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-01 04:19:28,527 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-01 04:19:28,533 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-01 04:19:28,538 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-01 04:19:28,540 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-01 04:19:28,540 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-01 04:19:28,540 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-01 04:19:28,551 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 51474
2015-12-01 04:19:28,551 INFO org.mortbay.log: jetty-6.1.26
2015-12-01 04:19:28,712 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:51474
2015-12-01 04:19:28,874 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-12-01 04:19:28,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-12-01 04:19:28,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-01 04:19:28,967 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-01 04:19:28,979 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-01 04:19:29,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-01 04:19:29,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-01 04:19:29,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-01 04:19:29,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-12-01 04:19:29,120 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-01 04:19:29,121 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-01 04:19:29,499 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 5895@rushikesh2
2015-12-01 04:19:29,579 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:19:29,579 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:19:29,580 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-01 04:19:29,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-12-01 04:19:29,664 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-12-01 04:19:29,664 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-01 04:19:29,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-01 04:19:29,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:19:29,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 04:19:29,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 94ms
2015-12-01 04:19:29,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 94ms
2015-12-01 04:19:29,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 04:19:29,857 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 71ms
2015-12-01 04:19:29,857 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 72ms
2015-12-01 04:19:30,146 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 584335913 ms.
2015-12-01 04:19:30,148 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448942643148 with interval 21600000
2015-12-01 04:19:30,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-12-01 04:19:30,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-12-01 04:19:30,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-01 04:19:30,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3711
2015-12-01 04:19:30,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-12-01 04:19:30,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4e09eb1e55a,  containing 1 storage report(s), of which we sent 1. The reports had 428 total blocks and used 1 RPC(s). This took 15 msec to generate and 70 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-12-01 04:19:30,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:20:01,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742447_1624 src: /192.168.6.237:34371 dest: /192.168.6.249:50010
2015-12-01 04:20:01,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742435_1612 src: /192.168.6.248:50488 dest: /192.168.6.249:50010
2015-12-01 04:20:01,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742432_1609 src: /192.168.6.237:34372 dest: /192.168.6.249:50010
2015-12-01 04:20:01,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742439_1616 src: /192.168.6.248:50489 dest: /192.168.6.249:50010
2015-12-01 04:20:02,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742439_1616 src: /192.168.6.248:50489 dest: /192.168.6.249:50010 of size 2295666
2015-12-01 04:20:02,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742447_1624 src: /192.168.6.237:34371 dest: /192.168.6.249:50010 of size 2292867
2015-12-01 04:20:02,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742435_1612 src: /192.168.6.248:50488 dest: /192.168.6.249:50010 of size 2284941
2015-12-01 04:20:02,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742432_1609 src: /192.168.6.237:34372 dest: /192.168.6.249:50010 of size 2273165
2015-12-01 04:20:07,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742421_1598 src: /192.168.6.248:50496 dest: /192.168.6.249:50010
2015-12-01 04:20:07,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742423_1600 src: /192.168.6.248:50497 dest: /192.168.6.249:50010
2015-12-01 04:20:07,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742416_1593 src: /192.168.6.237:34373 dest: /192.168.6.249:50010
2015-12-01 04:20:07,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742419_1596 src: /192.168.6.237:34374 dest: /192.168.6.249:50010
2015-12-01 04:20:08,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742421_1598 src: /192.168.6.248:50496 dest: /192.168.6.249:50010 of size 2273574
2015-12-01 04:20:08,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742419_1596 src: /192.168.6.237:34374 dest: /192.168.6.249:50010 of size 2275038
2015-12-01 04:20:08,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742416_1593 src: /192.168.6.237:34373 dest: /192.168.6.249:50010 of size 2268521
2015-12-01 04:20:08,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742423_1600 src: /192.168.6.248:50497 dest: /192.168.6.249:50010 of size 2386911
2015-12-01 04:20:13,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742574_1751 src: /192.168.6.248:50498 dest: /192.168.6.249:50010
2015-12-01 04:20:13,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742582_1759 src: /192.168.6.248:50499 dest: /192.168.6.249:50010
2015-12-01 04:20:13,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742566_1743 src: /192.168.6.237:34375 dest: /192.168.6.249:50010
2015-12-01 04:20:13,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742584_1761 src: /192.168.6.237:34376 dest: /192.168.6.249:50010
2015-12-01 04:20:14,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742582_1759 src: /192.168.6.248:50499 dest: /192.168.6.249:50010 of size 2297078
2015-12-01 04:20:14,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742574_1751 src: /192.168.6.248:50498 dest: /192.168.6.249:50010 of size 2290537
2015-12-01 04:20:14,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742584_1761 src: /192.168.6.237:34376 dest: /192.168.6.249:50010 of size 2287077
2015-12-01 04:20:14,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742566_1743 src: /192.168.6.237:34375 dest: /192.168.6.249:50010 of size 2284708
2015-12-01 04:20:19,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742550_1727 src: /192.168.6.248:50500 dest: /192.168.6.249:50010
2015-12-01 04:20:19,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742553_1730 src: /192.168.6.248:50501 dest: /192.168.6.249:50010
2015-12-01 04:20:19,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742548_1725 src: /192.168.6.237:34377 dest: /192.168.6.249:50010
2015-12-01 04:20:19,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742555_1732 src: /192.168.6.237:34378 dest: /192.168.6.249:50010
2015-12-01 04:20:20,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742553_1730 src: /192.168.6.248:50501 dest: /192.168.6.249:50010 of size 2411079
2015-12-01 04:20:20,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742550_1727 src: /192.168.6.248:50500 dest: /192.168.6.249:50010 of size 2287164
2015-12-01 04:20:20,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742548_1725 src: /192.168.6.237:34377 dest: /192.168.6.249:50010 of size 2409768
2015-12-01 04:20:20,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742555_1732 src: /192.168.6.237:34378 dest: /192.168.6.249:50010 of size 2295191
2015-12-01 04:20:22,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742517_1694 src: /192.168.6.248:50502 dest: /192.168.6.249:50010
2015-12-01 04:20:22,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742524_1701 src: /192.168.6.248:50503 dest: /192.168.6.249:50010
2015-12-01 04:20:22,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742497_1674 src: /192.168.6.237:34381 dest: /192.168.6.249:50010
2015-12-01 04:20:22,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742505_1682 src: /192.168.6.237:34382 dest: /192.168.6.249:50010
2015-12-01 04:20:23,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742497_1674 src: /192.168.6.237:34381 dest: /192.168.6.249:50010 of size 2296792
2015-12-01 04:20:23,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742517_1694 src: /192.168.6.248:50502 dest: /192.168.6.249:50010 of size 2294713
2015-12-01 04:20:23,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742505_1682 src: /192.168.6.237:34382 dest: /192.168.6.249:50010 of size 2288022
2015-12-01 04:20:23,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742524_1701 src: /192.168.6.248:50503 dest: /192.168.6.249:50010 of size 2287709
2015-12-01 04:20:28,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742484_1661 src: /192.168.6.248:50504 dest: /192.168.6.249:50010
2015-12-01 04:20:28,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742596_1773 src: /192.168.6.248:50505 dest: /192.168.6.249:50010
2015-12-01 04:20:28,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742483_1660 src: /192.168.6.237:34389 dest: /192.168.6.249:50010
2015-12-01 04:20:28,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742494_1671 src: /192.168.6.237:34390 dest: /192.168.6.249:50010
2015-12-01 04:20:29,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742484_1661 src: /192.168.6.248:50504 dest: /192.168.6.249:50010 of size 2290716
2015-12-01 04:20:29,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742596_1773 src: /192.168.6.248:50505 dest: /192.168.6.249:50010 of size 2288912
2015-12-01 04:20:29,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742446_1623 to 192.168.6.237:50010 
2015-12-01 04:20:29,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742445_1622 to 192.168.6.237:50010 
2015-12-01 04:20:29,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742483_1660 src: /192.168.6.237:34389 dest: /192.168.6.249:50010 of size 2417254
2015-12-01 04:20:29,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742494_1671 src: /192.168.6.237:34390 dest: /192.168.6.249:50010 of size 2288146
2015-12-01 04:20:29,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742445_1622 (numBytes=2292583) to /192.168.6.237:50010
2015-12-01 04:20:29,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742446_1623 (numBytes=2294051) to /192.168.6.237:50010
2015-12-01 04:20:32,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742434_1611 to 192.168.6.237:50010 
2015-12-01 04:20:32,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742462_1639 to 192.168.6.237:50010 
2015-12-01 04:20:32,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742434_1611 (numBytes=2289411) to /192.168.6.237:50010
2015-12-01 04:20:32,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742462_1639 (numBytes=2294380) to /192.168.6.237:50010
2015-12-01 04:20:35,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742404_1581 to 192.168.6.237:50010 
2015-12-01 04:20:35,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742406_1583 to 192.168.6.237:50010 
2015-12-01 04:20:35,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742404_1581 (numBytes=2273506) to /192.168.6.237:50010
2015-12-01 04:20:35,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742406_1583 (numBytes=2274079) to /192.168.6.237:50010
2015-12-01 04:20:38,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742422_1599 to 192.168.6.237:50010 
2015-12-01 04:20:38,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742393_1570 to 192.168.6.237:50010 
2015-12-01 04:20:38,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742422_1599 (numBytes=2272153) to /192.168.6.237:50010
2015-12-01 04:20:38,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742393_1570 (numBytes=2389070) to /192.168.6.237:50010
2015-12-01 04:20:41,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742565_1742 to 192.168.6.237:50010 
2015-12-01 04:20:41,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742570_1747 to 192.168.6.237:50010 
2015-12-01 04:20:41,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742565_1742 (numBytes=2291798) to /192.168.6.237:50010
2015-12-01 04:20:41,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742570_1747 (numBytes=2290641) to /192.168.6.237:50010
2015-12-01 04:20:44,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742577_1754 to 192.168.6.237:50010 
2015-12-01 04:20:44,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742591_1768 to 192.168.6.237:50010 
2015-12-01 04:20:44,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742577_1754 (numBytes=2291085) to /192.168.6.237:50010
2015-12-01 04:20:44,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742591_1768 (numBytes=2277585) to /192.168.6.237:50010
2015-12-01 04:20:47,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742501_1678 to 192.168.6.237:50010 
2015-12-01 04:20:47,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742496_1673 to 192.168.6.237:50010 
2015-12-01 04:20:47,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742501_1678 (numBytes=2281341) to /192.168.6.237:50010
2015-12-01 04:20:47,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742496_1673 (numBytes=2290339) to /192.168.6.237:50010
2015-12-01 04:20:50,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742527_1704 to 192.168.6.237:50010 
2015-12-01 04:20:50,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742521_1698 to 192.168.6.237:50010 
2015-12-01 04:20:50,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742527_1704 (numBytes=2286916) to /192.168.6.237:50010
2015-12-01 04:20:50,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742521_1698 (numBytes=2280566) to /192.168.6.237:50010
2015-12-01 04:20:53,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742485_1662 to 192.168.6.237:50010 
2015-12-01 04:20:53,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742482_1659 to 192.168.6.237:50010 
2015-12-01 04:20:53,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742485_1662 (numBytes=2295070) to /192.168.6.237:50010
2015-12-01 04:20:53,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742482_1659 (numBytes=2288728) to /192.168.6.237:50010
2015-12-01 04:20:56,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742488_1665 to 192.168.6.237:50010 
2015-12-01 04:20:56,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742597_1774 to 192.168.6.237:50010 
2015-12-01 04:20:56,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742488_1665 (numBytes=2290451) to /192.168.6.237:50010
2015-12-01 04:20:56,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742597_1774 (numBytes=2289796) to /192.168.6.237:50010
2015-12-01 04:21:01,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742458_1635 src: /192.168.6.248:50532 dest: /192.168.6.249:50010
2015-12-01 04:21:01,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742460_1637 src: /192.168.6.248:50533 dest: /192.168.6.249:50010
2015-12-01 04:21:01,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742459_1636 src: /192.168.6.237:34395 dest: /192.168.6.249:50010
2015-12-01 04:21:01,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742463_1640 src: /192.168.6.237:34396 dest: /192.168.6.249:50010
2015-12-01 04:21:02,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742458_1635 src: /192.168.6.248:50532 dest: /192.168.6.249:50010 of size 2412679
2015-12-01 04:21:02,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742460_1637 src: /192.168.6.248:50533 dest: /192.168.6.249:50010 of size 2289798
2015-12-01 04:21:02,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742459_1636 src: /192.168.6.237:34395 dest: /192.168.6.249:50010 of size 2293989
2015-12-01 04:21:02,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742463_1640 src: /192.168.6.237:34396 dest: /192.168.6.249:50010 of size 2411515
2015-12-01 04:21:07,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742390_1567 src: /192.168.6.248:50540 dest: /192.168.6.249:50010
2015-12-01 04:21:07,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742563_1740 src: /192.168.6.248:50541 dest: /192.168.6.249:50010
2015-12-01 04:21:07,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742392_1569 src: /192.168.6.237:34403 dest: /192.168.6.249:50010
2015-12-01 04:21:07,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742386_1563 src: /192.168.6.237:34404 dest: /192.168.6.249:50010
2015-12-01 04:21:08,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742563_1740 src: /192.168.6.248:50541 dest: /192.168.6.249:50010 of size 2406688
2015-12-01 04:21:08,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742390_1567 src: /192.168.6.248:50540 dest: /192.168.6.249:50010 of size 2266617
2015-12-01 04:21:08,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742392_1569 src: /192.168.6.237:34403 dest: /192.168.6.249:50010 of size 2272502
2015-12-01 04:21:08,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742386_1563 src: /192.168.6.237:34404 dest: /192.168.6.249:50010 of size 2255615
2015-12-01 04:21:14,087 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-12-01 04:21:16,656 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-01 04:21:16,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-12-01 04:21:50,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-12-01 04:21:50,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-01 04:21:50,640 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-01 04:21:50,705 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-01 04:21:50,705 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-01 04:21:50,710 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-12-01 04:21:50,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-12-01 04:21:50,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-01 04:21:50,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-01 04:21:50,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-01 04:21:50,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-01 04:21:50,828 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-01 04:21:50,836 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-01 04:21:50,841 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-01 04:21:50,846 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-01 04:21:50,848 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-01 04:21:50,848 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-01 04:21:50,848 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-01 04:21:50,858 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36259
2015-12-01 04:21:50,858 INFO org.mortbay.log: jetty-6.1.26
2015-12-01 04:21:51,011 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36259
2015-12-01 04:21:51,092 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-12-01 04:21:51,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-12-01 04:21:51,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-01 04:21:51,132 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-01 04:21:51,143 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-01 04:21:51,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-01 04:21:51,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-01 04:21:51,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-01 04:21:51,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-12-01 04:21:51,223 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-01 04:21:51,224 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-01 04:21:51,581 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 6514@rushikesh2
2015-12-01 04:21:51,662 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:21:51,663 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:21:51,663 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-01 04:21:51,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-12-01 04:21:51,745 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-12-01 04:21:51,745 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-01 04:21:51,777 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-01 04:21:51,777 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:21:51,778 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 04:21:51,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35599116108
2015-12-01 04:21:51,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 8ms
2015-12-01 04:21:51,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-12-01 04:21:51,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 04:21:51,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 45ms
2015-12-01 04:21:51,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 46ms
2015-12-01 04:21:52,000 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 584194059 ms.
2015-12-01 04:21:52,002 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448936112002 with interval 21600000
2015-12-01 04:21:52,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-12-01 04:21:52,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-12-01 04:21:52,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-01 04:21:52,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3714
2015-12-01 04:21:52,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-12-01 04:21:52,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x501a6f32126,  containing 1 storage report(s), of which we sent 1. The reports had 460 total blocks and used 1 RPC(s). This took 5 msec to generate and 74 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-12-01 04:21:52,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:22:24,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742424_1601 to 192.168.6.237:50010 
2015-12-01 04:22:24,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742431_1608 to 192.168.6.237:50010 
2015-12-01 04:22:25,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742424_1601 (numBytes=2272985) to /192.168.6.237:50010
2015-12-01 04:22:25,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742431_1608 (numBytes=2274516) to /192.168.6.237:50010
2015-12-01 04:22:27,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742552_1729 to 192.168.6.237:50010 
2015-12-01 04:22:27,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742556_1733 to 192.168.6.237:50010 
2015-12-01 04:22:27,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742552_1729 (numBytes=2295340) to /192.168.6.237:50010
2015-12-01 04:22:27,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742556_1733 (numBytes=2291457) to /192.168.6.237:50010
2015-12-01 04:22:33,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742430_1607 src: /192.168.6.248:50582 dest: /192.168.6.249:50010
2015-12-01 04:22:33,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742429_1606 src: /192.168.6.248:50581 dest: /192.168.6.249:50010
2015-12-01 04:22:33,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742410_1587 src: /192.168.6.237:34423 dest: /192.168.6.249:50010
2015-12-01 04:22:33,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742453_1630 src: /192.168.6.237:34422 dest: /192.168.6.249:50010
2015-12-01 04:22:33,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742429_1606 src: /192.168.6.248:50581 dest: /192.168.6.249:50010 of size 2270807
2015-12-01 04:22:33,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742430_1607 src: /192.168.6.248:50582 dest: /192.168.6.249:50010 of size 2276051
2015-12-01 04:22:34,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742410_1587 src: /192.168.6.237:34423 dest: /192.168.6.249:50010 of size 2272899
2015-12-01 04:22:34,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742453_1630 src: /192.168.6.237:34422 dest: /192.168.6.249:50010 of size 2409832
2015-12-01 04:22:36,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742399_1576 src: /192.168.6.248:50589 dest: /192.168.6.249:50010
2015-12-01 04:22:36,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742528_1705 src: /192.168.6.248:50590 dest: /192.168.6.249:50010
2015-12-01 04:22:36,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742396_1573 src: /192.168.6.237:34424 dest: /192.168.6.249:50010
2015-12-01 04:22:36,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742533_1710 src: /192.168.6.237:34425 dest: /192.168.6.249:50010
2015-12-01 04:22:36,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742399_1576 src: /192.168.6.248:50589 dest: /192.168.6.249:50010 of size 2271852
2015-12-01 04:22:36,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742533_1710 src: /192.168.6.237:34425 dest: /192.168.6.249:50010 of size 2409806
2015-12-01 04:22:36,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742396_1573 src: /192.168.6.237:34424 dest: /192.168.6.249:50010 of size 2267336
2015-12-01 04:22:37,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742528_1705 src: /192.168.6.248:50590 dest: /192.168.6.249:50010 of size 2411555
2015-12-01 04:22:39,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742545_1722 src: /192.168.6.248:50591 dest: /192.168.6.249:50010
2015-12-01 04:22:39,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742547_1724 src: /192.168.6.248:50592 dest: /192.168.6.249:50010
2015-12-01 04:22:39,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742542_1719 src: /192.168.6.237:34426 dest: /192.168.6.249:50010
2015-12-01 04:22:39,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742544_1721 src: /192.168.6.237:34427 dest: /192.168.6.249:50010
2015-12-01 04:22:39,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742545_1722 src: /192.168.6.248:50591 dest: /192.168.6.249:50010 of size 2293676
2015-12-01 04:22:39,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742542_1719 src: /192.168.6.237:34426 dest: /192.168.6.249:50010 of size 2290669
2015-12-01 04:22:39,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742547_1724 src: /192.168.6.248:50592 dest: /192.168.6.249:50010 of size 2284640
2015-12-01 04:22:39,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742544_1721 src: /192.168.6.237:34427 dest: /192.168.6.249:50010 of size 2291803
2015-12-01 04:22:42,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742525_1702 src: /192.168.6.248:50593 dest: /192.168.6.249:50010
2015-12-01 04:22:42,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742465_1642 src: /192.168.6.248:50594 dest: /192.168.6.249:50010
2015-12-01 04:22:42,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742522_1699 src: /192.168.6.237:34428 dest: /192.168.6.249:50010
2015-12-01 04:22:42,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742520_1697 src: /192.168.6.237:34429 dest: /192.168.6.249:50010
2015-12-01 04:22:42,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742465_1642 src: /192.168.6.248:50594 dest: /192.168.6.249:50010 of size 2294522
2015-12-01 04:22:42,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742525_1702 src: /192.168.6.248:50593 dest: /192.168.6.249:50010 of size 2291512
2015-12-01 04:22:42,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742520_1697 src: /192.168.6.237:34429 dest: /192.168.6.249:50010 of size 2284262
2015-12-01 04:22:43,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742522_1699 src: /192.168.6.237:34428 dest: /192.168.6.249:50010 of size 2287500
2015-12-01 04:22:45,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742479_1656 src: /192.168.6.237:34432 dest: /192.168.6.249:50010
2015-12-01 04:22:45,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742475_1652 src: /192.168.6.237:34433 dest: /192.168.6.249:50010
2015-12-01 04:22:45,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742479_1656 src: /192.168.6.237:34432 dest: /192.168.6.249:50010 of size 2291553
2015-12-01 04:22:45,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742475_1652 src: /192.168.6.237:34433 dest: /192.168.6.249:50010 of size 2287622
2015-12-01 04:22:48,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742508_1685 to 192.168.6.237:50010 
2015-12-01 04:22:48,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742506_1683 to 192.168.6.237:50010 
2015-12-01 04:22:48,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742506_1683 (numBytes=2285595) to /192.168.6.237:50010
2015-12-01 04:22:49,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742508_1685 (numBytes=2410555) to /192.168.6.237:50010
2015-12-01 04:22:54,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742535_1712 src: /192.168.6.248:50597 dest: /192.168.6.249:50010
2015-12-01 04:22:54,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742543_1720 src: /192.168.6.248:50598 dest: /192.168.6.249:50010
2015-12-01 04:22:54,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742383_1560 src: /192.168.6.237:34443 dest: /192.168.6.249:50010
2015-12-01 04:22:54,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742379_1556 src: /192.168.6.237:34444 dest: /192.168.6.249:50010
2015-12-01 04:22:54,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742543_1720 src: /192.168.6.248:50598 dest: /192.168.6.249:50010 of size 2405136
2015-12-01 04:22:54,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742383_1560 src: /192.168.6.237:34443 dest: /192.168.6.249:50010 of size 2244091
2015-12-01 04:22:55,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742379_1556 src: /192.168.6.237:34444 dest: /192.168.6.249:50010 of size 2239882
2015-12-01 04:22:55,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742535_1712 src: /192.168.6.248:50597 dest: /192.168.6.249:50010 of size 2294986
2015-12-01 04:22:57,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742467_1644 src: /192.168.6.237:34445 dest: /192.168.6.249:50010
2015-12-01 04:22:57,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742466_1643 src: /192.168.6.237:34446 dest: /192.168.6.249:50010
2015-12-01 04:22:57,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742467_1644 src: /192.168.6.237:34445 dest: /192.168.6.249:50010 of size 2287397
2015-12-01 04:22:57,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742466_1643 src: /192.168.6.237:34446 dest: /192.168.6.249:50010 of size 2288555
2015-12-01 04:23:00,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742471_1648 to 192.168.6.237:50010 
2015-12-01 04:23:00,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742495_1672 to 192.168.6.237:50010 
2015-12-01 04:23:01,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742471_1648 (numBytes=2294640) to /192.168.6.237:50010
2015-12-01 04:23:01,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742495_1672 (numBytes=2291028) to /192.168.6.237:50010
2015-12-01 04:23:03,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742499_1676 src: /192.168.6.237:34447 dest: /192.168.6.249:50010
2015-12-01 04:23:03,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742498_1675 src: /192.168.6.237:34448 dest: /192.168.6.249:50010
2015-12-01 04:23:03,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742499_1676 src: /192.168.6.237:34447 dest: /192.168.6.249:50010 of size 2288407
2015-12-01 04:23:03,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received BP-1750158012-192.168.6.248-1444037565733:blk_1073742498_1675 src: /192.168.6.237:34448 dest: /192.168.6.249:50010 of size 2414274
2015-12-01 04:23:09,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742594_1771 to 192.168.6.237:50010 
2015-12-01 04:23:09,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.6.249:50010, datanodeUuid=30ae543a-02e8-4984-b58e-6da4391dc3e5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0) Starting thread to transfer BP-1750158012-192.168.6.248-1444037565733:blk_1073742592_1769 to 192.168.6.237:50010 
2015-12-01 04:23:09,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742594_1771 (numBytes=2298836) to /192.168.6.237:50010
2015-12-01 04:23:09,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted BP-1750158012-192.168.6.248-1444037565733:blk_1073742592_1769 (numBytes=2288012) to /192.168.6.237:50010
2015-12-01 04:23:45,217 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-12-01 04:23:48,267 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-01 04:23:48,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-12-01 04:25:53,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-12-01 04:25:53,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-01 04:25:54,310 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-01 04:25:54,373 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-01 04:25:54,373 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-01 04:25:54,378 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-12-01 04:25:54,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-12-01 04:25:54,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-01 04:25:54,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-01 04:25:54,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-01 04:25:54,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-01 04:25:54,497 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-01 04:25:54,505 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-01 04:25:54,510 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-01 04:25:54,515 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-01 04:25:54,517 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-01 04:25:54,517 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-01 04:25:54,517 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-01 04:25:54,527 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39373
2015-12-01 04:25:54,527 INFO org.mortbay.log: jetty-6.1.26
2015-12-01 04:25:54,679 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39373
2015-12-01 04:25:54,761 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-12-01 04:25:54,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-12-01 04:25:54,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-01 04:25:54,800 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-01 04:25:54,811 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-01 04:25:54,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-01 04:25:54,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-01 04:25:54,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-01 04:25:54,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-12-01 04:25:54,890 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-01 04:25:54,891 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-01 04:25:55,217 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 7159@rushikesh2
2015-12-01 04:25:55,298 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:25:55,298 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:25:55,299 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-01 04:25:55,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-12-01 04:25:55,364 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-12-01 04:25:55,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-01 04:25:55,398 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-01 04:25:55,398 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:25:55,399 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 04:25:55,406 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35659535331
2015-12-01 04:25:55,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 9ms
2015-12-01 04:25:55,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 10ms
2015-12-01 04:25:55,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 04:25:55,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 52ms
2015-12-01 04:25:55,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 53ms
2015-12-01 04:25:55,660 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 583950399 ms.
2015-12-01 04:25:55,662 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448944313662 with interval 21600000
2015-12-01 04:25:55,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-12-01 04:25:55,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-12-01 04:25:55,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-01 04:25:55,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3717
2015-12-01 04:25:55,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-12-01 04:25:55,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x53a625c0c6d,  containing 1 storage report(s), of which we sent 1. The reports had 486 total blocks and used 1 RPC(s). This took 8 msec to generate and 75 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-12-01 04:25:55,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:35:48,898 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742378_1555 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742378 for deletion
2015-12-01 04:35:48,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1750158012-192.168.6.248-1444037565733 blk_1073742378_1555 file /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current/finalized/subdir0/subdir2/blk_1073742378
2015-12-01 04:41:13,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1750158012-192.168.6.248-1444037565733:blk_1073742605_1782 src: /192.168.6.248:50888 dest: /192.168.6.249:50010
2015-12-01 04:41:13,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.6.248:50888, dest: /192.168.6.249:50010, bytes: 3363967, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2052990752_1, offset: 0, srvID: 30ae543a-02e8-4984-b58e-6da4391dc3e5, blockid: BP-1750158012-192.168.6.248-1444037565733:blk_1073742605_1782, duration: 363898278
2015-12-01 04:41:13,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1750158012-192.168.6.248-1444037565733:blk_1073742605_1782, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2015-12-01 04:48:48,884 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-12-01 04:48:52,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: rushikesh1/192.168.6.248:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-01 04:48:53,736 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-01 04:48:53,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
2015-12-01 04:49:47,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = rushikesh2/192.168.6.249
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-12-01 04:49:47,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-01 04:49:48,306 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-01 04:49:48,369 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-01 04:49:48,369 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-01 04:49:48,374 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2015-12-01 04:49:48,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is rushikesh2
2015-12-01 04:49:48,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-01 04:49:48,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-01 04:49:48,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-01 04:49:48,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-01 04:49:48,492 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-01 04:49:48,500 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-01 04:49:48,505 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-01 04:49:48,510 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-01 04:49:48,512 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-01 04:49:48,512 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-01 04:49:48,512 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-01 04:49:48,522 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50916
2015-12-01 04:49:48,522 INFO org.mortbay.log: jetty-6.1.26
2015-12-01 04:49:48,675 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:50916
2015-12-01 04:49:48,756 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2015-12-01 04:49:48,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hduser
2015-12-01 04:49:48,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-01 04:49:48,796 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-01 04:49:48,806 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-01 04:49:48,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-01 04:49:48,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-01 04:49:48,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-01 04:49:48,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to rushikesh1/192.168.6.248:54310 starting to offer service
2015-12-01 04:49:48,886 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-01 04:49:48,886 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-01 04:49:49,236 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /app/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 10988@rushikesh2
2015-12-01 04:49:49,309 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:49:49,309 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:49:49,310 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-01 04:49:49,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1018713813;bpid=BP-1750158012-192.168.6.248-1444037565733;lv=-56;nsInfo=lv=-63;cid=CID-0bdb7046-0c42-4885-a155-0fa51af982fa;nsid=1018713813;c=0;bpid=BP-1750158012-192.168.6.248-1444037565733;dnuuid=30ae543a-02e8-4984-b58e-6da4391dc3e5
2015-12-01 04:49:49,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-f427aaf2-e296-4623-9eca-489900635169
2015-12-01 04:49:49,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /app/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-01 04:49:49,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-01 04:49:49,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:49:49,410 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 04:49:49,416 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /app/hadoop/tmp/dfs/data/current/BP-1750158012-192.168.6.248-1444037565733/current: 35659788288
2015-12-01 04:49:49,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1750158012-192.168.6.248-1444037565733 on /app/hadoop/tmp/dfs/data/current: 7ms
2015-12-01 04:49:49,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1750158012-192.168.6.248-1444037565733: 9ms
2015-12-01 04:49:49,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current...
2015-12-01 04:49:49,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1750158012-192.168.6.248-1444037565733 on volume /app/hadoop/tmp/dfs/data/current: 49ms
2015-12-01 04:49:49,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 49ms
2015-12-01 04:49:49,667 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/app/hadoop/tmp/dfs/data, DS-f427aaf2-e296-4623-9eca-489900635169): no suitable block pools found to scan.  Waiting 582516392 ms.
2015-12-01 04:49:49,669 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1448927651669 with interval 21600000
2015-12-01 04:49:49,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 beginning handshake with NN
2015-12-01 04:49:49,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid null) service to rushikesh1/192.168.6.248:54310 successfully registered with NN
2015-12-01 04:49:49,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode rushikesh1/192.168.6.248:54310 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-01 04:49:49,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310 trying to claim ACTIVE state with txid=3727
2015-12-01 04:49:49,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1750158012-192.168.6.248-1444037565733 (Datanode Uuid 30ae543a-02e8-4984-b58e-6da4391dc3e5) service to rushikesh1/192.168.6.248:54310
2015-12-01 04:49:49,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x688433da437,  containing 1 storage report(s), of which we sent 1. The reports had 486 total blocks and used 1 RPC(s). This took 6 msec to generate and 88 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2015-12-01 04:49:49,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1750158012-192.168.6.248-1444037565733
2015-12-01 04:50:45,880 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "rushikesh2/192.168.6.249"; destination host is: "rushikesh1":54310; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:553)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:823)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1079)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:974)
2015-12-01 04:50:49,071 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-01 04:50:49,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at rushikesh2/192.168.6.249
************************************************************/
